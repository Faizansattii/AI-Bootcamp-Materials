{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Week 10 - Day 2: CNN Architectures\n",
    "## From Pooling to Famous Networks in PyTorch\n",
    "\n",
    "### Today's Learning Goals:\n",
    "- âœ… Master pooling layers (Max and Average)\n",
    "- âœ… Understand complete CNN building blocks\n",
    "- âœ… Build CNNs from scratch in PyTorch\n",
    "- âœ… Implement VGG-style architecture\n",
    "- âœ… Understand ResNet skip connections\n",
    "- âœ… Train a CNN on Fashion-MNIST\n",
    "- âœ… Visualize learned filters and feature maps\n",
    "- âœ… Compare different architectures\n",
    "\n",
    "---\n",
    "\n",
    "**Let's build real CNNs that actually work!** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Part 1: Setup and Data Loading\n",
    "\n",
    "Let's import libraries and load Fashion-MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Training data\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded successfully!\")\n",
    "print(f\"ðŸ“Š Training samples: {len(train_dataset):,}\")\n",
    "print(f\"ðŸ“Š Test samples: {len(test_dataset):,}\")\n",
    "print(f\"ðŸ“Š Number of classes: {len(class_names)}\")\n",
    "print(f\"ðŸ“Š Image shape: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    img, label = train_dataset[i]\n",
    "    img = img.squeeze().numpy()\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f\"{class_names[label]}\", fontsize=10, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Fashion-MNIST Sample Images\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Fashion-MNIST: 10 categories of clothing and accessories!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŠ Part 2: Understanding Pooling Layers\n",
    "\n",
    "Let's visualize how pooling reduces spatial dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample feature map\n",
    "sample_feature = torch.randn(1, 1, 8, 8)\n",
    "\n",
    "# Apply max pooling (2x2, stride=2)\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "max_pooled = max_pool(sample_feature)\n",
    "\n",
    "# Apply average pooling (2x2, stride=2)\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "avg_pooled = avg_pool(sample_feature)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original\n",
    "im1 = axes[0].imshow(sample_feature[0, 0].numpy(), cmap='viridis')\n",
    "axes[0].set_title(f\"Original Feature Map\\n{sample_feature.shape[2]}Ã—{sample_feature.shape[3]}\", \n",
    "                  fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "axes[0].grid(True, color='white', linewidth=1)\n",
    "\n",
    "# Max pooled\n",
    "im2 = axes[1].imshow(max_pooled[0, 0].numpy(), cmap='viridis')\n",
    "axes[1].set_title(f\"Max Pooled (2Ã—2)\\n{max_pooled.shape[2]}Ã—{max_pooled.shape[3]}\", \n",
    "                  fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "axes[1].grid(True, color='white', linewidth=1)\n",
    "\n",
    "# Average pooled\n",
    "im3 = axes[2].imshow(avg_pooled[0, 0].numpy(), cmap='viridis')\n",
    "axes[2].set_title(f\"Average Pooled (2Ã—2)\\n{avg_pooled.shape[2]}Ã—{avg_pooled.shape[3]}\", \n",
    "                  fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "axes[2].grid(True, color='white', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŒŠ Pooling Effects:\")\n",
    "print(f\"â€¢ Original shape: {sample_feature.shape}\")\n",
    "print(f\"â€¢ After pooling: {max_pooled.shape}\")\n",
    "print(f\"â€¢ Spatial size reduced: 8Ã—8 â†’ 4Ã—4 (75% reduction!)\")\n",
    "print(f\"\\nðŸ’¡ Max pooling keeps strongest activations!\")\n",
    "print(f\"ðŸ’¡ Average pooling smooths the output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Part 3: Building Your First Complete CNN\n",
    "\n",
    "Let's build a simple CNN with all the components!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 28Ã—28 â†’ 28Ã—28\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 14Ã—14 â†’ 14Ã—14\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Halves the size\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv Block 1: Conv â†’ ReLU â†’ Pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28Ã—28 â†’ 14Ã—14\n",
    "        \n",
    "        # Conv Block 2: Conv â†’ ReLU â†’ Pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 14Ã—14 â†’ 7Ã—7\n",
    "        \n",
    "        # Flatten for FC layers\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten\n",
    "        \n",
    "        # FC layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"ðŸ—ï¸ Simple CNN Architecture:\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Total parameters: {total_params:,}\")\n",
    "print(f\"ðŸ“Š Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "print(f\"\\nâœ… Input shape: {dummy_input.shape}\")\n",
    "print(f\"âœ… Output shape: {dummy_output.shape}\")\n",
    "print(f\"\\nðŸ’¡ Output has 10 values (one for each class)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš‚ Part 4: Training and Evaluation Functions\n",
    "\n",
    "Let's create reusable training functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "print(\"âœ… Training functions defined!\")\n",
    "print(\"\\nðŸš‚ Ready to train models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Part 5: Train the Simple CNN\n",
    "\n",
    "Let's train our first CNN and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "# Training history\n",
    "train_losses, train_accs = [], []\n",
    "test_losses, test_accs = [], []\n",
    "\n",
    "print(\"ðŸš‚ Training Simple CNN...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"\\nðŸŽ¯ Final Test Accuracy: {test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(train_losses, 'b-o', label='Train Loss', linewidth=2)\n",
    "ax1.plot(test_losses, 'r-o', label='Test Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(train_accs, 'b-o', label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(test_accs, 'r-o', label='Test Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Training Progress:\")\n",
    "print(f\"â€¢ Started at: {train_accs[0]:.2f}% (train), {test_accs[0]:.2f}% (test)\")\n",
    "print(f\"â€¢ Ended at: {train_accs[-1]:.2f}% (train), {test_accs[-1]:.2f}% (test)\")\n",
    "print(f\"â€¢ Improvement: +{train_accs[-1]-train_accs[0]:.2f}% (train), +{test_accs[-1]-test_accs[0]:.2f}% (test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Part 6: Building VGG-Style Architecture\n",
    "\n",
    "Let's implement a deeper CNN inspired by VGG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGStyleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGStyleCNN, self).__init__()\n",
    "        \n",
    "        # VGG-style blocks: Conv-Conv-Pool pattern\n",
    "        # Block 1: 1 â†’ 64 channels\n",
    "        self.conv1_1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Block 2: 64 â†’ 128 channels\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1: Conv-ReLU-Conv-ReLU-Pool\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.pool(x)  # 28Ã—28 â†’ 14Ã—14\n",
    "        \n",
    "        # Block 2: Conv-ReLU-Conv-ReLU-Pool\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = self.pool(x)  # 14Ã—14 â†’ 7Ã—7\n",
    "        \n",
    "        # Flatten and FC layers\n",
    "        x = x.view(-1, 128 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create VGG-style model\n",
    "vgg_model = VGGStyleCNN().to(device)\n",
    "\n",
    "# Count parameters\n",
    "vgg_params = sum(p.numel() for p in vgg_model.parameters())\n",
    "\n",
    "print(\"ðŸ“ VGG-Style CNN Architecture:\")\n",
    "print(\"=\"*60)\n",
    "print(vgg_model)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Total parameters: {vgg_params:,}\")\n",
    "print(f\"ðŸ“Š Simple CNN had: {total_params:,} parameters\")\n",
    "print(f\"ðŸ“Š VGG-style is {vgg_params/total_params:.2f}x larger!\")\n",
    "\n",
    "print(\"\\nðŸ’¡ VGG pattern: Two Conv layers before each Pool!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Part 7: ResNet Skip Connections\n",
    "\n",
    "Let's implement residual blocks with skip connections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic ResNet building block with skip connection\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Main path: Conv-BN-ReLU-Conv-BN\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection (shortcut)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            # If dimensions change, adjust skip connection\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Main path\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Add skip connection\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test residual block\n",
    "test_block = ResidualBlock(64, 64).to(device)\n",
    "test_input = torch.randn(1, 64, 28, 28).to(device)\n",
    "test_output = test_block(test_input)\n",
    "\n",
    "print(\"ðŸš€ Residual Block Created!\")\n",
    "print(f\"\\nâœ… Input shape: {test_input.shape}\")\n",
    "print(f\"âœ… Output shape: {test_output.shape}\")\n",
    "print(f\"\\nðŸ’¡ Skip connection adds input directly to output!\")\n",
    "print(f\"ðŸ’¡ This helps gradients flow backward easily!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Part 8: Visualizing Learned Filters\n",
    "\n",
    "Let's see what our trained CNN learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first conv layer weights\n",
    "first_layer_weights = model.conv1.weight.data.cpu()\n",
    "\n",
    "# Visualize 16 filters\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(16):\n",
    "    filter_img = first_layer_weights[i, 0].numpy()\n",
    "    axes[i].imshow(filter_img, cmap='gray')\n",
    "    axes[i].set_title(f\"Filter {i+1}\", fontsize=9)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Learned Filters from First Convolutional Layer\", \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¨ What the filters learned:\")\n",
    "print(\"â€¢ Dark/light patterns for edge detection\")\n",
    "print(\"â€¢ Different orientations (vertical, horizontal, diagonal)\")\n",
    "print(\"â€¢ Some detect textures and gradients\")\n",
    "print(\"\\nðŸ’¡ These were learned automatically during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ºï¸ Part 9: Visualizing Feature Maps\n",
    "\n",
    "Let's see how the CNN processes an image layer by layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test image\n",
    "test_img, test_label = test_dataset[0]\n",
    "test_img_batch = test_img.unsqueeze(0).to(device)\n",
    "\n",
    "# Hook to capture feature maps\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.conv2.register_forward_hook(get_activation('conv2'))\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(test_img_batch)\n",
    "    prediction = output.argmax(dim=1).item()\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Original image\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "ax1.imshow(test_img.squeeze().numpy(), cmap='gray')\n",
    "ax1.set_title(f\"Input Image: {class_names[test_label]}\\nPredicted: {class_names[prediction]}\",\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Conv1 feature maps (show 8 of 32)\n",
    "conv1_features = activations['conv1'][0].cpu().numpy()\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(3, 8, 8 + i + 1)\n",
    "    ax.imshow(conv1_features[i], cmap='viridis')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Conv1\\n32 maps', fontsize=9, fontweight='bold')\n",
    "    ax.set_title(f\"Map {i+1}\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Conv2 feature maps (show 8 of 64)\n",
    "conv2_features = activations['conv2'][0].cpu().numpy()\n",
    "for i in range(8):\n",
    "    ax = plt.subplot(3, 8, 16 + i + 1)\n",
    "    ax.imshow(conv2_features[i], cmap='plasma')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Conv2\\n64 maps', fontsize=9, fontweight='bold')\n",
    "    ax.set_title(f\"Map {i+1}\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Feature Maps Across CNN Layers\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ—ºï¸ Feature Map Progression:\")\n",
    "print(f\"â€¢ Input: {test_img.shape}\")\n",
    "print(f\"â€¢ After Conv1: {activations['conv1'].shape}\")\n",
    "print(f\"â€¢ After Conv2: {activations['conv2'].shape}\")\n",
    "print(\"\\nðŸ’¡ Notice: More channels, smaller spatial size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Challenge - Build Your Own CNN\n",
    "\n",
    "**Your Task:** Create a custom CNN with:\n",
    "1. At least 3 convolutional layers\n",
    "2. Use both Max Pooling and Batch Normalization\n",
    "3. 2-3 fully connected layers\n",
    "4. Achieve >88% test accuracy on Fashion-MNIST\n",
    "\n",
    "**Requirements:**\n",
    "- Define the model class\n",
    "- Train for 5 epochs\n",
    "- Plot training curves\n",
    "- Report final accuracy\n",
    "\n",
    "**Hints:**\n",
    "- Try 32 â†’ 64 â†’ 128 channel progression\n",
    "- Use nn.BatchNorm2d after conv layers\n",
    "- Learning rate: 0.001 with Adam optimizer\n",
    "\n",
    "**Bonus:**\n",
    "- Implement dropout\n",
    "- Try data augmentation\n",
    "- Visualize your model's feature maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build and train your custom CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary & Key Takeaways\n",
    "\n",
    "### ðŸŽ“ What We Learned Today:\n",
    "\n",
    "**1. Pooling Layers:**\n",
    "- Max pooling: Keeps strongest features\n",
    "- Average pooling: Smooths output\n",
    "- Reduces spatial dimensions (computational efficiency)\n",
    "- Makes features translation-invariant\n",
    "\n",
    "**2. Complete CNN Blocks:**\n",
    "- Standard pattern: Conv â†’ ReLU â†’ Pool\n",
    "- Can repeat: Conv â†’ Conv â†’ ReLU â†’ Pool (VGG style)\n",
    "- End with FC layers for classification\n",
    "\n",
    "**3. Famous Architectures:**\n",
    "- AlexNet (2012): Deep learning breakthrough\n",
    "- VGG (2014): Simplicity with 3Ã—3 filters\n",
    "- ResNet (2015): Skip connections for very deep networks\n",
    "\n",
    "**4. PyTorch Implementation:**\n",
    "- nn.Conv2d for convolution\n",
    "- nn.MaxPool2d for pooling\n",
    "- nn.Linear for fully connected\n",
    "- F.relu for activation\n",
    "\n",
    "**5. Training Pipeline:**\n",
    "- Load data with DataLoader\n",
    "- Define model, loss, optimizer\n",
    "- Training loop: forward, loss, backward, update\n",
    "- Evaluate on test set\n",
    "\n",
    "**6. Visualization:**\n",
    "- Learned filters show edge detectors\n",
    "- Feature maps reveal hierarchical learning\n",
    "- Early layers: edges, later layers: complex patterns\n",
    "\n",
    "### ðŸŽ¯ Key Architecture Patterns:\n",
    "\n",
    "**Simple CNN:**\n",
    "```\n",
    "Input â†’ Conv â†’ ReLU â†’ Pool â†’ Conv â†’ ReLU â†’ Pool â†’ FC â†’ Output\n",
    "```\n",
    "\n",
    "**VGG-Style:**\n",
    "```\n",
    "Input â†’ [Conv â†’ Conv â†’ Pool]Ã—N â†’ FC â†’ Output\n",
    "Pattern: Double channels, halve dimensions\n",
    "```\n",
    "\n",
    "**ResNet-Style:**\n",
    "```\n",
    "Input â†’ [Conv â†’ Conv + Skip Connection]Ã—N â†’ FC â†’ Output\n",
    "Skip connection: Add input to output\n",
    "```\n",
    "\n",
    "### ðŸ’¡ Pro Tips:\n",
    "\n",
    "1. **Start simple** - Add complexity gradually\n",
    "2. **Use 3Ã—3 kernels** - Most effective size\n",
    "3. **Double channels** after each pooling\n",
    "4. **Batch normalization** helps training\n",
    "5. **Dropout** prevents overfitting\n",
    "6. **Adam optimizer** good default choice\n",
    "7. **Learning rate 0.001** typical starting point\n",
    "8. **Data augmentation** improves generalization\n",
    "\n",
    "### ðŸš€ What's Next (Day 3):\n",
    "\n",
    "**Tomorrow we'll cover:**\n",
    "- Regularization techniques\n",
    "- Dropout and batch normalization in detail\n",
    "- Overfitting prevention\n",
    "- TensorBoard for training visualization\n",
    "- Advanced training tricks\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work! You can now build and train real CNNs! ðŸŽ‰**\n",
    "\n",
    "Remember: Modern CNNs are just clever combinations of these building blocks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
