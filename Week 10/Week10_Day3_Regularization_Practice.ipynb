{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Week 10 - Day 3: Regularization and Training Techniques\n",
    "## Preventing Overfitting and Professional CNN Training\n",
    "\n",
    "### Today's Learning Goals:\n",
    "- âœ… Understand overfitting and how to detect it\n",
    "- âœ… Implement dropout for regularization\n",
    "- âœ… Use batch normalization effectively\n",
    "- âœ… Apply data augmentation techniques\n",
    "- âœ… Implement early stopping\n",
    "- âœ… Visualize training with TensorBoard\n",
    "- âœ… Compare models with/without regularization\n",
    "- âœ… Build a production-ready CNN\n",
    "\n",
    "---\n",
    "\n",
    "**Let's master professional CNN training!** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Part 1: Setup and Imports\n",
    "\n",
    "Let's set up our environment with all necessary libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Part 2: Load Data with Augmentation\n",
    "\n",
    "Let's load Fashion-MNIST with data augmentation for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms WITH augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance to flip\n",
    "    transforms.RandomRotation(degrees=10),     # Rotate Â±10 degrees\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Slight shift\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Test transforms WITHOUT augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"âœ… Training samples: {len(train_dataset):,}\")\n",
    "print(f\"âœ… Test samples: {len(test_dataset):,}\")\n",
    "print(f\"\\nðŸŽ¨ Data augmentation applied to training set!\")\n",
    "print(\"   â€¢ Random horizontal flip\")\n",
    "print(\"   â€¢ Random rotation (Â±10Â°)\")\n",
    "print(\"   â€¢ Random translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "# Get one image\n",
    "original_img = train_dataset.dataset.data[0].numpy()\n",
    "\n",
    "# Show augmented versions\n",
    "for i in range(10):\n",
    "    augmented, label = train_dataset[0]\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(augmented.squeeze().numpy(), cmap='gray')\n",
    "    ax.set_title(f\"Augmented {i+1}\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Same Image with Different Augmentations\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Each epoch, images are augmented differently!\")\n",
    "print(\"   This effectively multiplies your training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Part 3: Models - With and Without Regularization\n",
    "\n",
    "Let's build two models to compare regularization effects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithoutRegularization(nn.Module):\n",
    "    \"\"\"Simple CNN without any regularization\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNWithoutRegularization, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28â†’14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 14â†’7\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 7â†’3\n",
    "        \n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNWithRegularization(nn.Module):\n",
    "    \"\"\"CNN with dropout and batch normalization\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNWithRegularization, self).__init__()\n",
    "        \n",
    "        # Conv layers with batch norm\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # FC layers with dropout\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv blocks with BatchNorm\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # FC layers with Dropout\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Dropout before output\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create both models\n",
    "model_no_reg = CNNWithoutRegularization().to(device)\n",
    "model_with_reg = CNNWithRegularization().to(device)\n",
    "\n",
    "print(\"ðŸ—ï¸ Models Created:\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"Model WITHOUT Regularization:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model_no_reg.parameters()):,}\")\n",
    "print(\"  â€¢ No Dropout\")\n",
    "print(\"  â€¢ No Batch Normalization\")\n",
    "print()\n",
    "print(\"Model WITH Regularization:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model_with_reg.parameters()):,}\")\n",
    "print(\"  â€¢ Dropout (p=0.5) in FC layers\")\n",
    "print(\"  â€¢ Batch Normalization after each Conv\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš‚ Part 4: Training Functions with TensorBoard\n",
    "\n",
    "Let's create training functions that log to TensorBoard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, writer=None, epoch=0):\n",
    "    \"\"\"\n",
    "    Train for one epoch with TensorBoard logging\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc='Training', leave=False)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Log to TensorBoard every 100 batches\n",
    "        if writer and batch_idx % 100 == 0:\n",
    "            step = epoch * len(train_loader) + batch_idx\n",
    "            writer.add_scalar('Batch/Loss', loss.item(), step)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, num_epochs, model_name):\n",
    "    \"\"\"\n",
    "    Complete training loop with TensorBoard logging\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    writer = SummaryWriter(f'runs/{model_name}')\n",
    "    \n",
    "    # History\n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs = [], []\n",
    "    \n",
    "    print(f\"\\nðŸš‚ Training {model_name}...\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, writer, epoch\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Save history\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/test', test_acc, epoch)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.2f}%\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    writer.close()\n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "    \n",
    "    return train_losses, train_accs, test_losses, test_accs\n",
    "\n",
    "print(\"âœ… Training functions defined!\")\n",
    "print(\"âœ… TensorBoard logging enabled!\")\n",
    "print(\"\\nðŸ’¡ Run this in terminal to view TensorBoard:\")\n",
    "print(\"   tensorboard --logdir=runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Part 5: Train Model WITHOUT Regularization\n",
    "\n",
    "Let's train the baseline model to see overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model without regularization\n",
    "history_no_reg = train_model(\n",
    "    model_no_reg, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    num_epochs=10,\n",
    "    model_name='no_regularization'\n",
    ")\n",
    "\n",
    "train_losses_no_reg, train_accs_no_reg, test_losses_no_reg, test_accs_no_reg = history_no_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ›¡ï¸ Part 6: Train Model WITH Regularization\n",
    "\n",
    "Now let's train the regularized model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with regularization\n",
    "history_with_reg = train_model(\n",
    "    model_with_reg, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    num_epochs=10,\n",
    "    model_name='with_regularization'\n",
    ")\n",
    "\n",
    "train_losses_reg, train_accs_reg, test_losses_reg, test_accs_reg = history_with_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Part 7: Compare Results\n",
    "\n",
    "Let's visualize the difference regularization makes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss - No Regularization\n",
    "axes[0, 0].plot(train_losses_no_reg, 'b-o', label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(test_losses_no_reg, 'r-o', label='Test Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('WITHOUT Regularization - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss - With Regularization\n",
    "axes[0, 1].plot(train_losses_reg, 'b-o', label='Train Loss', linewidth=2)\n",
    "axes[0, 1].plot(test_losses_reg, 'r-o', label='Test Loss', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('WITH Regularization - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy - No Regularization\n",
    "axes[1, 0].plot(train_accs_no_reg, 'b-o', label='Train Acc', linewidth=2)\n",
    "axes[1, 0].plot(test_accs_no_reg, 'r-o', label='Test Acc', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('WITHOUT Regularization - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy - With Regularization\n",
    "axes[1, 1].plot(train_accs_reg, 'b-o', label='Train Acc', linewidth=2)\n",
    "axes[1, 1].plot(test_accs_reg, 'r-o', label='Test Acc', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('WITH Regularization - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nðŸ“Š Final Results Comparison:\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"WITHOUT Regularization:\")\n",
    "print(f\"  Train Accuracy: {train_accs_no_reg[-1]:.2f}%\")\n",
    "print(f\"  Test Accuracy:  {test_accs_no_reg[-1]:.2f}%\")\n",
    "print(f\"  Gap: {train_accs_no_reg[-1] - test_accs_no_reg[-1]:.2f}% (overfitting!)\")\n",
    "print()\n",
    "print(\"WITH Regularization:\")\n",
    "print(f\"  Train Accuracy: {train_accs_reg[-1]:.2f}%\")\n",
    "print(f\"  Test Accuracy:  {test_accs_reg[-1]:.2f}%\")\n",
    "print(f\"  Gap: {train_accs_reg[-1] - test_accs_reg[-1]:.2f}% (better!)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nâœ… Regularization reduced overfitting!\")\n",
    "print(\"ðŸ’¡ Notice: Smaller gap between train and test accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Challenge - Build Your Production CNN\n",
    "\n",
    "**Your Task:** Create a production-ready CNN with:\n",
    "1. Batch normalization after every conv layer\n",
    "2. Dropout (0.3-0.5) in FC layers\n",
    "3. Data augmentation (already done)\n",
    "4. Weight decay in optimizer\n",
    "5. Achieve >90% test accuracy\n",
    "\n",
    "**Bonus:**\n",
    "- Implement early stopping\n",
    "- Try different architectures\n",
    "- Visualize with TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build your production-ready CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary & Key Takeaways\n",
    "\n",
    "### ðŸŽ“ What We Learned:\n",
    "\n",
    "**1. Overfitting Detection:**\n",
    "- Large gap between train and test accuracy\n",
    "- Train loss decreasing, test loss increasing\n",
    "- Model memorizing instead of learning\n",
    "\n",
    "**2. Regularization Techniques:**\n",
    "- **Dropout:** Random neuron deactivation (0.3-0.5)\n",
    "- **Batch Normalization:** Normalize activations\n",
    "- **Data Augmentation:** Create variations\n",
    "- **Weight Decay:** Penalize large weights (1e-4)\n",
    "- **Early Stopping:** Stop before overfitting\n",
    "\n",
    "**3. Best Practices:**\n",
    "- Always use batch normalization\n",
    "- Add dropout in FC layers\n",
    "- Apply data augmentation\n",
    "- Monitor train vs test metrics\n",
    "- Save best model checkpoints\n",
    "\n",
    "**4. TensorBoard:**\n",
    "- Real-time training visualization\n",
    "- Compare multiple experiments\n",
    "- Track loss, accuracy, weights\n",
    "- Professional ML workflow\n",
    "\n",
    "### ðŸ’¡ Pro Tips:\n",
    "\n",
    "1. Start simple, add regularization gradually\n",
    "2. BatchNorm after Conv, Dropout before output\n",
    "3. Use weight_decay=1e-4 in optimizer\n",
    "4. Monitor validation set closely\n",
    "5. Save checkpoints every epoch\n",
    "6. Visualize everything with TensorBoard\n",
    "\n",
    "### ðŸŽ¯ When to Use What:\n",
    "\n",
    "**Small dataset (< 10K samples):**\n",
    "- Heavy data augmentation\n",
    "- Higher dropout (0.5)\n",
    "- Smaller network\n",
    "\n",
    "**Large dataset (> 100K samples):**\n",
    "- Light augmentation\n",
    "- Lower dropout (0.3)\n",
    "- Deeper network OK\n",
    "\n",
    "**Signs of overfitting:**\n",
    "- Add more regularization\n",
    "- Reduce model complexity\n",
    "- Get more data\n",
    "\n",
    "**Signs of underfitting:**\n",
    "- Increase model capacity\n",
    "- Train longer\n",
    "- Reduce regularization\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You now know professional CNN training! ðŸŽ‰**\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply to real projects\n",
    "- Experiment with architectures\n",
    "- Try transfer learning\n",
    "- Deploy your models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
