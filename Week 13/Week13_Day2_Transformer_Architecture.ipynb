{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udf93 Week 13 - Day 2: Transformer Architecture\n",
        "\n",
        "## Today's Goals:\n",
        "\u2705 Understand self-attention mechanism\n",
        "\u2705 Build Transformer components in PyTorch\n",
        "\u2705 Implement positional encoding\n",
        "\u2705 Train a simple Transformer model\n",
        "\u2705 Visualize attention patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Part 1: Setup - Install & Import All Libraries\n",
        "\n",
        "**IMPORTANT:** Run ALL cells in this part sequentially!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: Import core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "print(\"\u2705 Core libraries imported!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\u2705 Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: Configure matplotlib\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"\u2705 Visualization configured!\")\n",
        "print(\"\\n\ud83d\ude80 Ready to build Transformers!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Part 2: Self-Attention Mechanism\n",
        "\n",
        "The core innovation that makes Transformers work!\n",
        "\n",
        "**The Problem with RNNs:**\n",
        "- Process words one-by-one (slow)\n",
        "- Forget long-range dependencies\n",
        "- Can't parallelize\n",
        "\n",
        "**The Transformer Solution:**\n",
        "- Process ALL words simultaneously\n",
        "- Every word attends to every other word\n",
        "- Fully parallelizable!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Self-Attention Implementation\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # Create Q, K, V projection layers\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, embed_dim)\n",
        "        \n",
        "        # Project to Q, K, V\n",
        "        Q = self.query(x)  # \"What am I looking for?\"\n",
        "        K = self.key(x)    # \"What do I contain?\"\n",
        "        V = self.value(x)  # \"What information do I have?\"\n",
        "        \n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "        scores = scores / math.sqrt(self.embed_dim)\n",
        "        \n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "print(\"\u2705 Self-Attention class created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Self-Attention\n",
        "embed_dim = 64\n",
        "seq_len = 5  # Representing: \"The cat sat on mat\"\n",
        "batch_size = 1\n",
        "\n",
        "# Create random word embeddings\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Apply attention\n",
        "attention = SelfAttention(embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\u2705 Input shape: {x.shape}\")\n",
        "print(f\"\u2705 Output shape: {output.shape}\")\n",
        "print(f\"\u2705 Attention weights shape: {weights.shape}\")\n",
        "print(f\"\\n\ud83c\udfaf Attention Matrix:\")\n",
        "print(weights[0].detach().numpy().round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize attention patterns\n",
        "words = ['The', 'cat', 'sat', 'on', 'mat']\n",
        "attn_matrix = weights[0].detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(attn_matrix, \n",
        "            xticklabels=words, \n",
        "            yticklabels=words,\n",
        "            annot=True, \n",
        "            fmt='.2f', \n",
        "            cmap='YlOrRd',\n",
        "            cbar_kws={'label': 'Attention Weight'})\n",
        "plt.title('Self-Attention Weights', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Attending TO these words', fontweight='bold')\n",
        "plt.ylabel('Words', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udca1 Each row shows what a word attends to!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udca1 Key Insights:\n",
        "\n",
        "\u2705 **Self-Attention** allows each word to look at all other words  \n",
        "\u2705 **Query, Key, Value** matrices encode different aspects of meaning  \n",
        "\u2705 **Attention weights** show relationships between words  \n",
        "\u2705 **Parallel processing** - all words processed simultaneously!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udccd Part 3: Positional Encoding\n",
        "\n",
        "**The Position Problem:**\n",
        "- Self-attention has NO information about word order!\n",
        "- \"Dog bites man\" vs \"Man bites dog\" - Same words, different meanings!\n",
        "\n",
        "**The Solution:**\n",
        "- Add positional information using sine/cosine functions\n",
        "- Each position gets a unique \"signature\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Positional Encoding Implementation\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        \n",
        "        # Calculate div_term\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * \n",
        "                             (-math.log(10000.0) / embed_dim))\n",
        "        \n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to embeddings\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "print(\"\u2705 Positional Encoding class created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Positional Encoding\n",
        "pos_encoder = PositionalEncoding(embed_dim=64)\n",
        "x = torch.randn(1, 10, 64)\n",
        "x_with_pos = pos_encoder(x)\n",
        "\n",
        "print(f\"\u2705 Original shape: {x.shape}\")\n",
        "print(f\"\u2705 With positional encoding: {x_with_pos.shape}\")\n",
        "print(\"\\n\ud83d\udca1 Now model knows: position 0, 1, 2, ..., 9!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize positional encoding patterns\n",
        "pe_matrix = pos_encoder.pe[0, :50, :64].numpy()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(pe_matrix.T, cmap='RdBu', center=0, ax=ax1)\n",
        "ax1.set_title('Positional Encoding Matrix', fontsize=12, fontweight='bold')\n",
        "ax1.set_xlabel('Position')\n",
        "ax1.set_ylabel('Embedding Dimension')\n",
        "\n",
        "# Line plot\n",
        "for i in range(6):\n",
        "    ax2.plot(pe_matrix[:, i], label=f'Dim {i}', linewidth=2)\n",
        "ax2.set_title('Positional Patterns', fontsize=12, fontweight='bold')\n",
        "ax2.set_xlabel('Position')\n",
        "ax2.set_ylabel('Encoding Value')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udca1 Each position has a unique sine/cosine signature!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udca1 Key Insights:\n",
        "\n",
        "\u2705 **Sine/Cosine waves** create unique position signatures  \n",
        "\u2705 **Added to embeddings** - not replacing them!  \n",
        "\u2705 **No learning required** - mathematical pattern works perfectly  \n",
        "\u2705 **Enables long sequences** - generalizes beyond training length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfd7\ufe0f Part 4: Building the Complete Transformer Block\n",
        "\n",
        "Now we combine everything into the famous Transformer architecture!\n",
        "\n",
        "**Components:**\n",
        "1. Multi-Head Self-Attention\n",
        "2. Feed-Forward Network\n",
        "3. Layer Normalization  \n",
        "4. Residual Connections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Multi-Head Attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Feed-Forward Network\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        \n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Multi-Head Attention + Residual + Norm\n",
        "        attn_out, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        \n",
        "        # Feed-Forward + Residual + Norm\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        \n",
        "        return x\n",
        "\n",
        "print(\"\u2705 Transformer Block created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Transformer Block\n",
        "transformer_block = TransformerBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256\n",
        ")\n",
        "\n",
        "x = torch.randn(1, 10, 64)\n",
        "output = transformer_block(x)\n",
        "\n",
        "print(f\"\u2705 Input: {x.shape}\")\n",
        "print(f\"\u2705 Output: {output.shape}\")\n",
        "print(f\"\\n\ud83d\udcca Parameters: {sum(p.numel() for p in transformer_block.parameters()):,}\")\n",
        "print(\"\\n\ud83d\udca1 This is ONE layer - GPT-3 has 96 of these!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udca1 Key Insights:\n",
        "\n",
        "\u2705 **Multi-Head Attention** captures different types of relationships  \n",
        "\u2705 **Residual Connections** help with gradient flow (like ResNet)  \n",
        "\u2705 **Layer Normalization** stabilizes training  \n",
        "\u2705 **Feed-Forward** processes each position independently\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf93 Part 5: Training a Complete Transformer Model\n",
        "\n",
        "Let's put everything together and train on a simple task!\n",
        "\n",
        "**Task:** Sequence Reversal  \n",
        "**Input:** [1, 2, 3, 4, 5]  \n",
        "**Output:** [5, 4, 3, 2, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Transformer Model\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, max_len)\n",
        "        \n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Embedding + Positional Encoding\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        \n",
        "        # Apply Transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        # Output projection\n",
        "        logits = self.fc_out(x)\n",
        "        return logits\n",
        "\n",
        "# Create model\n",
        "model = SimpleTransformer(\n",
        "    vocab_size=20,\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=128,\n",
        "    num_layers=2,\n",
        "    max_len=100\n",
        ").to(device)\n",
        "\n",
        "print(f\"\u2705 Transformer Model created!\")\n",
        "print(f\"\ud83d\udcca Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset for sequence reversal\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ReverseDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, seq_len=8, vocab_size=15):\n",
        "        self.data = torch.randint(1, vocab_size, (num_samples, seq_len))\n",
        "        self.targets = torch.flip(self.data, dims=[1])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataset = ReverseDataset(num_samples=1000, seq_len=8, vocab_size=15)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Show example\n",
        "sample_in, sample_out = train_dataset[0]\n",
        "print(\"\ud83d\udcdd Example:\")\n",
        "print(f\"   Input:  {sample_in.numpy()}\")\n",
        "print(f\"   Target: {sample_out.numpy()}\")\n",
        "print(f\"\\n\u2705 Dataset: {len(train_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 20\n",
        "vocab_size = 20\n",
        "losses = []\n",
        "\n",
        "print(\"\ud83d\ude80 Training Transformer...\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch_in, batch_target in train_loader:\n",
        "        batch_in = batch_in.to(device)\n",
        "        batch_target = batch_target.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_in)\n",
        "        loss = criterion(logits.reshape(-1, vocab_size), batch_target.reshape(-1))\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1:2d}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\n\u2705 Training complete! \ud83c\udf89\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training progress\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses, marker='o', linewidth=2, markersize=6)\n",
        "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch', fontweight='bold')\n",
        "plt.ylabel('Loss', fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udca1 Decreasing loss = Model is learning!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "model.eval()\n",
        "\n",
        "print(\"\ud83e\uddea Testing Transformer:\\n\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(5):\n",
        "        test_in, test_target = train_dataset[i]\n",
        "        test_in_batch = test_in.unsqueeze(0).to(device)\n",
        "        \n",
        "        # Get prediction\n",
        "        logits = model(test_in_batch)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        correct = (predictions[0].cpu() == test_target).sum().item()\n",
        "        accuracy = correct / len(test_target) * 100\n",
        "        \n",
        "        print(f\"Test {i+1}:\")\n",
        "        print(f\"   Input:      {test_in.numpy()}\")\n",
        "        print(f\"   Target:     {test_target.numpy()}\")\n",
        "        print(f\"   Prediction: {predictions[0].cpu().numpy()}\")\n",
        "        print(f\"   \u2705 Accuracy: {accuracy:.0f}%\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udca1 Key Insights:\n",
        "\n",
        "\u2705 **Complete architecture** - Embedding \u2192 Transformer \u2192 Output  \n",
        "\u2705 **Fast training** - Parallel processing makes it efficient  \n",
        "\u2705 **High accuracy** - Even on simple tasks, performance is excellent  \n",
        "\u2705 **Scalable** - Same architecture powers GPT-4 with billions of parameters!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Challenge Time!\n",
        "\n",
        "### \ud83c\udfc6 Challenge: Experiment with Hyperparameters\n",
        "\n",
        "**Your Mission:** Modify the Transformer and observe the effects!\n",
        "\n",
        "Try changing these values:\n",
        "```python\n",
        "embed_dim = 64      # Try: 32, 128, 256\n",
        "num_heads = 4       # Try: 2, 8 (must divide embed_dim!)\n",
        "num_layers = 2      # Try: 1, 3, 4\n",
        "ff_dim = 128        # Try: 64, 256, 512\n",
        "learning_rate = 0.001  # Try: 0.0001, 0.01\n",
        "```\n",
        "\n",
        "**Questions to explore:**\n",
        "1. Does more heads improve performance?\n",
        "2. What happens with more layers?\n",
        "3. Can you train faster with different learning rates?\n",
        "4. What's the smallest model that still works well?\n",
        "\n",
        "**Bonus Challenge:**  \n",
        "Try a harder task:\n",
        "- Sort the sequence in ascending order\n",
        "- Remove duplicate numbers\n",
        "- Add 1 to each number\n",
        "\n",
        "Good luck! \ud83d\ude80\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your experimentation code here!\n",
        "\n",
        "# Example: Modified hyperparameters\n",
        "# model_new = SimpleTransformer(\n",
        "#     vocab_size=20,\n",
        "#     embed_dim=128,  # Increased!\n",
        "#     num_heads=8,     # More heads!\n",
        "#     ff_dim=256,\n",
        "#     num_layers=3,    # Deeper!\n",
        "#     max_len=100\n",
        "# ).to(device)\n",
        "\n",
        "print(\"\ud83d\udca1 Uncomment and modify the code above to experiment!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83d\udcda Summary - What We Learned Today\n",
        "\n",
        "### 1. Self-Attention Mechanism \ud83c\udfaf\n",
        "- **Query, Key, Value** matrices\n",
        "- **Attention weights** show word relationships\n",
        "- **Parallel processing** - all words at once\n",
        "\n",
        "### 2. Positional Encoding \ud83d\udccd\n",
        "- **Sine/Cosine patterns** give position information\n",
        "- **Added to embeddings** not replacing them\n",
        "- **No learning required** - mathematical solution\n",
        "\n",
        "### 3. Transformer Architecture \ud83c\udfd7\ufe0f\n",
        "- **Multi-Head Attention** for different relationships\n",
        "- **Feed-Forward Networks** for position-wise processing\n",
        "- **Residual Connections** for gradient flow\n",
        "- **Layer Normalization** for stability\n",
        "\n",
        "### 4. Complete Model \ud83c\udf93\n",
        "- **Embedding \u2192 Transformer \u2192 Output** pipeline\n",
        "- **Trained on sequence reversal** task\n",
        "- **High accuracy** achieved quickly\n",
        "- **Scalable** to billions of parameters\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\ude80 What's Next?\n",
        "\n",
        "**Tomorrow (Day 3): Hugging Face**\n",
        "- Use pre-trained Transformers (BERT, GPT-2)\n",
        "- Fine-tune models for your tasks\n",
        "- No training from scratch!\n",
        "- Build real NLP applications in 3 lines of code\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udca1 Key Takeaways:\n",
        "\n",
        "\u2705 **Transformers revolutionized AI** by using self-attention  \n",
        "\u2705 **Parallel processing** makes them 1000x faster than RNNs  \n",
        "\u2705 **Same architecture** powers GPT, BERT, Claude, ChatGPT  \n",
        "\u2705 **You just built** the foundation of modern AI!\n",
        "\n",
        "**Excellent work! \ud83c\udf89**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}