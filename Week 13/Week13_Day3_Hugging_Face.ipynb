{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udf93 Week 13 - Day 3: Hugging Face and Pre-trained Models\n",
        "\n",
        "## Today's Goals:\n",
        "\u2705 Master Hugging Face pipelines for instant NLP\n",
        "\u2705 Load and use pre-trained models (BERT, GPT-2)\n",
        "\u2705 Fine-tune models for custom tasks\n",
        "\u2705 Build production-ready NLP applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Part 1: Setup - Install & Import All Libraries\n",
        "\n",
        "**IMPORTANT:** Run ALL cells in this part sequentially!\n",
        "\n",
        "**Why Hugging Face?**\n",
        "- \ud83d\ude80 100,000+ pre-trained models  \n",
        "- \ud83d\udcb0 Free to use (trained by Google, Meta, OpenAI)\n",
        "- \u26a1 3 lines of code for most tasks\n",
        "- \ud83c\udfaf No need to train from scratch!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: Install Hugging Face libraries\n",
        "!pip install -q transformers datasets torch evaluate accelerate\n",
        "\n",
        "print(\"\u2705 Hugging Face installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2: Import core libraries\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\u2705 Libraries imported!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: Check device and setup\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"\u2705 Using: {'GPU' if device == 0 else 'CPU'}\")\n",
        "print(\"\\n\ud83d\ude80 Ready to use Hugging Face!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Part 2: Hugging Face Pipelines - Instant NLP!\n",
        "\n",
        "**What are Pipelines?**  \n",
        "Pre-built functions that combine tokenizer + model for common tasks!\n",
        "\n",
        "**Available Pipelines:**\n",
        "- \ud83c\udfad Sentiment Analysis\n",
        "- \u2753 Question Answering\n",
        "- \ud83d\udcdd Text Generation\n",
        "- \ud83d\udcf0 Summarization\n",
        "- \ud83d\udd0d Named Entity Recognition (NER)\n",
        "- \ud83c\udfaf Zero-Shot Classification\n",
        "\n",
        "Let's try them all!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Sentiment Analysis (Positive/Negative detector)\n",
        "sentiment = pipeline(\"sentiment-analysis\", device=device)\n",
        "\n",
        "texts = [\n",
        "    \"I love this product! Amazing quality!\",\n",
        "    \"Terrible experience, very disappointed.\",\n",
        "    \"It's okay, nothing special.\",\n",
        "    \"Best purchase ever! Highly recommend!\"\n",
        "]\n",
        "\n",
        "print(\"\ud83c\udfad Sentiment Analysis Results:\\n\")\n",
        "for text in texts:\n",
        "    result = sentiment(text)[0]\n",
        "    print(f\"'{text}'\")\n",
        "    print(f\"   \u2192 {result['label']}: {result['score']:.0%}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Question Answering\n",
        "qa = pipeline(\"question-answering\", device=device)\n",
        "\n",
        "context = \"\"\"\n",
        "Transformers were introduced in 2017 by Google researchers. They use \n",
        "self-attention instead of recurrent layers, enabling parallel processing. \n",
        "Major models include BERT (2018), GPT-2 (2019), and GPT-3 (2020). These models \n",
        "power ChatGPT, Claude, and other AI systems.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"When were Transformers introduced?\",\n",
        "    \"Who introduced Transformers?\",\n",
        "    \"Name a major Transformer model.\"\n",
        "]\n",
        "\n",
        "print(\"\u2753 Question Answering:\\n\")\n",
        "for q in questions:\n",
        "    result = qa(question=q, context=context)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {result['answer']} ({result['score']:.0%})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3: Text Generation (like ChatGPT!)\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n",
        "\n",
        "prompts = [\n",
        "    \"Artificial intelligence will\",\n",
        "    \"The future of technology is\"\n",
        "]\n",
        "\n",
        "print(\"\ud83d\udcdd Text Generation:\\n\")\n",
        "for prompt in prompts:\n",
        "    result = generator(prompt, max_length=40, num_return_sequences=1)[0]\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: {result['generated_text']}\\n\")\n",
        "    print(\"-\" * 70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4: Summarization\n",
        "summarizer = pipeline(\"summarization\", device=device)\n",
        "\n",
        "article = \"\"\"\n",
        "Artificial intelligence has made remarkable progress in recent years. \n",
        "Transformer models revolutionized natural language processing. Companies like \n",
        "OpenAI, Google, and Anthropic developed powerful models such as GPT-4, PaLM, \n",
        "and Claude. These models can perform tasks including translation, summarization, \n",
        "question answering, and code generation. Applications span healthcare, education, \n",
        "customer service, and creative writing.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"\ud83d\udcf0 Original: {len(article.split())} words\\n\")\n",
        "\n",
        "summary = summarizer(article, max_length=50, min_length=25)[0]\n",
        "print(\"\ud83d\udcdd Summary:\")\n",
        "print(summary['summary_text'])\n",
        "print(f\"\\n\u2705 Summary: {len(summary['summary_text'].split())} words\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 5: Named Entity Recognition (Extract names, places, organizations)\n",
        "ner = pipeline(\"ner\", grouped_entities=True, device=device)\n",
        "\n",
        "text = \"\"\"\n",
        "Apple Inc. was founded by Steve Jobs in Cupertino, California. Tim Cook became \n",
        "CEO in 2011. The company is valued at over $3 trillion.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\ud83d\udd0d Named Entity Recognition:\\n\")\n",
        "entities = ner(text)\n",
        "\n",
        "for entity in entities:\n",
        "    print(f\"{entity['word']:<20} \u2192 {entity['entity_group']:<10} ({entity['score']:.0%})\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 PER=Person | ORG=Organization | LOC=Location\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udca1 Key Insights:\n",
        "\n",
        "\u2705 **One-line solutions** for complex NLP tasks  \n",
        "\u2705 **Pre-trained models** - no training needed!  \n",
        "\u2705 **Multiple tasks** - sentiment, QA, generation, NER  \n",
        "\u2705 **Production-ready** - used by real companies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Part 3: Manual Model Loading & Tokenization\n",
        "\n",
        "Understanding what happens inside pipelines!\n",
        "\n",
        "**Every model needs TWO components:**\n",
        "1. **Tokenizer:** Text \u2192 Numbers\n",
        "2. **Model:** Numbers \u2192 Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load BERT tokenizer and model manually\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "print(f\"\u2705 Loaded: {model_name}\")\n",
        "print(f\"\ud83d\udcca Parameters: {model.num_parameters():,}\")\n",
        "print(f\"\ud83d\udd24 Vocabulary: {tokenizer.vocab_size:,} words\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How tokenization works\n",
        "text = \"Hugging Face makes NLP easy!\"\n",
        "\n",
        "# Step 1: Split into tokens\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(f\"\ud83d\udd24 Tokens: {tokens}\")\n",
        "\n",
        "# Step 2: Convert to IDs\n",
        "ids = tokenizer.encode(text)\n",
        "print(f\"\ud83d\udd22 Token IDs: {ids}\")\n",
        "\n",
        "# Step 3: Full encoding (for models)\n",
        "encoded = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "print(f\"\\n\ud83d\udce6 Encoded input:\")\n",
        "print(f\"   Input IDs shape: {encoded['input_ids'].shape}\")\n",
        "print(f\"   Attention mask: {encoded['attention_mask'].shape}\")\n",
        "\n",
        "# Step 4: Decode back to text\n",
        "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
        "print(f\"\\n\ud83d\udd04 Decoded: {decoded}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions manually\n",
        "model.eval()\n",
        "\n",
        "texts = [\n",
        "    \"This movie is amazing!\",\n",
        "    \"I hated every minute.\"\n",
        "]\n",
        "\n",
        "print(\"\ud83c\udfaf Manual Predictions:\\n\")\n",
        "\n",
        "for text in texts:\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "        probs = torch.softmax(logits, dim=1)[0]\n",
        "    \n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"   {'\u2705 Positive' if pred == 1 else '\u274c Negative'} ({probs[pred]:.0%})\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udca1 Key Insights:\n",
        "\n",
        "\u2705 **Tokenizer** converts text to numbers that models understand  \n",
        "\u2705 **Special tokens** [CLS], [SEP] mark sentence boundaries  \n",
        "\u2705 **Attention masks** tell model which tokens are real vs padding  \n",
        "\u2705 **Manual control** useful for custom applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf93 Part 4: Fine-Tuning on Custom Data\n",
        "\n",
        "**The Power of Fine-Tuning:**\n",
        "- \ud83c\udfaf Adapt pre-trained models to YOUR task\n",
        "- \u26a1 Train in minutes, not weeks\n",
        "- \ud83d\udcca Achieve 90%+ accuracy with small datasets\n",
        "\n",
        "**Today's Task:** Fine-tune BERT on movie review sentiment!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load IMDB movie review dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "print(\"\u2705 Dataset loaded!\")\n",
        "print(f\"   Training: {len(dataset['train']):,} reviews\")\n",
        "print(f\"   Test: {len(dataset['test']):,} reviews\")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\n\ud83d\udcdd Sample reviews:\\n\")\n",
        "for i in range(2):\n",
        "    example = dataset['train'][i]\n",
        "    label = \"Positive\" if example['label'] == 1 else \"Negative\"\n",
        "    print(f\"Review: {example['text'][:100]}...\")\n",
        "    print(f\"Label: {label}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create smaller subset for faster training\n",
        "small_train = dataset['train'].shuffle(seed=42).select(range(1000))\n",
        "small_test = dataset['test'].shuffle(seed=42).select(range(200))\n",
        "\n",
        "print(f\"\u2705 Training subset: {len(small_train)} samples\")\n",
        "print(f\"\u2705 Test subset: {len(small_test)} samples\")\n",
        "print(\"\\n\ud83d\udca1 Using small dataset for demo - scale up for production!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", \n",
        "                     truncation=True, max_length=256)\n",
        "\n",
        "print(\"\ud83d\udd24 Tokenizing datasets...\")\n",
        "tokenized_train = small_train.map(tokenize_function, batched=True)\n",
        "tokenized_test = small_test.map(tokenize_function, batched=True)\n",
        "\n",
        "print(\"\u2705 Tokenization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load fresh model for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2  # Binary: positive/negative\n",
        ")\n",
        "\n",
        "print(\"\u2705 Model loaded for fine-tuning!\")\n",
        "print(f\"\ud83d\udcca Parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "print(\"\u2705 Training configured!\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define evaluation metrics\n",
        "import evaluate\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(\"\u2705 Metrics configured (Accuracy)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Trainer and fine-tune!\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\ud83d\ude80 Starting fine-tuning...\")\n",
        "print(\"\u23f3 This will take 3-5 minutes\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\u2705 Fine-tuning complete! \ud83c\udf89\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the fine-tuned model\n",
        "results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Final Results:\")\n",
        "print(f\"   Accuracy: {results['eval_accuracy']:.1%}\")\n",
        "print(f\"   Loss: {results['eval_loss']:.4f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Comparison:\")\n",
        "print(f\"   Random guessing: 50.0%\")\n",
        "print(f\"   Our fine-tuned model: {results['eval_accuracy']:.1%}\")\n",
        "print(f\"   Improvement: {(results['eval_accuracy'] - 0.5) / 0.5 * 100:.0f}% better!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on custom movie reviews\n",
        "test_reviews = [\n",
        "    \"This movie was absolutely incredible! Best film I've seen in years.\",\n",
        "    \"Waste of time and money. Terrible acting.\",\n",
        "    \"It was okay, nothing special.\",\n",
        "    \"Mind-blowing cinematography! A masterpiece!\"\n",
        "]\n",
        "\n",
        "print(\"\ud83c\udfaf Testing on Custom Reviews:\\n\")\n",
        "\n",
        "model.eval()\n",
        "for review in test_reviews:\n",
        "    inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "        probs = torch.softmax(logits, dim=1)[0]\n",
        "    \n",
        "    print(f\"Review: '{review}'\")\n",
        "    print(f\"   {'\u2705 Positive' if pred == 1 else '\u274c Negative'} ({probs[pred]:.0%})\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./my-sentiment-model\")\n",
        "tokenizer.save_pretrained(\"./my-sentiment-model\")\n",
        "\n",
        "print(\"\u2705 Model saved to './my-sentiment-model'\")\n",
        "print(\"\\n\ud83d\udca1 You can now:\")\n",
        "print(\"   1. Load it anytime with AutoModelForSequenceClassification.from_pretrained()\")\n",
        "print(\"   2. Share it on Hugging Face Hub\")\n",
        "print(\"   3. Deploy in production applications!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udca1 Key Insights:\n",
        "\n",
        "\u2705 **Fine-tuning is fast** - 3 epochs in minutes vs weeks of training  \n",
        "\u2705 **Small datasets work** - 1000 samples gave 90%+ accuracy  \n",
        "\u2705 **Transfer learning magic** - pre-trained knowledge + your data  \n",
        "\u2705 **Production ready** - save and deploy anywhere\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf10 Part 5: Exploring the Hugging Face Hub\n",
        "\n",
        "**The Hub = GitHub for ML Models**\n",
        "- \ud83c\udfaf 100,000+ models\n",
        "- \ud83c\udd93 Free to use\n",
        "- \ud83d\udc65 Community-driven\n",
        "- \ud83d\udcca Models for every task\n",
        "\n",
        "Let's explore!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for models programmatically\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "print(\"\ud83d\udd0d Top Sentiment Analysis Models:\\n\")\n",
        "models = list_models(filter=\"text-classification\", sort=\"downloads\", limit=5)\n",
        "\n",
        "for i, model in enumerate(models, 1):\n",
        "    print(f\"{i}. {model.modelId}\")\n",
        "    print(f\"   Downloads: {model.downloads:,}\")\n",
        "    print(f\"   Likes: {model.likes}\\n\")\n",
        "\n",
        "print(\"\ud83d\udca1 Visit https://huggingface.co/models to explore more!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udca1 Key Insights:\n",
        "\n",
        "\u2705 **Massive library** - 100,000+ pre-trained models  \n",
        "\u2705 **Every task covered** - classification, generation, QA, etc.  \n",
        "\u2705 **Community driven** - anyone can share models  \n",
        "\u2705 **Easy to use** - load with 1 line of code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Challenge Time!\n",
        "\n",
        "### \ud83c\udfc6 Challenge: Build a Multi-Task NLP Analyzer\n",
        "\n",
        "**Your Mission:** Create a function that analyzes text and returns:\n",
        "1. Sentiment (positive/negative)\n",
        "2. Named entities (people, places, organizations)\n",
        "3. Summary (if text is long enough)\n",
        "\n",
        "**Starter Code:**\n",
        "```python\n",
        "def analyze_text(text):\n",
        "    # TODO: Use pipelines to analyze text\n",
        "    # Return dictionary with all results\n",
        "    results = {\n",
        "        'sentiment': None,\n",
        "        'entities': None,\n",
        "        'summary': None\n",
        "    }\n",
        "    return results\n",
        "\n",
        "# Test it\n",
        "sample = \"\"\"\n",
        "Elon Musk announced that Tesla will open a new factory in Austin, Texas. \n",
        "The company expects to create 5,000 jobs. This is great news for the economy.\n",
        "\"\"\"\n",
        "\n",
        "results = analyze_text(sample)\n",
        "print(results)\n",
        "```\n",
        "\n",
        "**Expected Output:**\n",
        "```\n",
        "{\n",
        "    'sentiment': 'POSITIVE (95%)',\n",
        "    'entities': ['Elon Musk', 'Tesla', 'Austin', 'Texas'],\n",
        "    'summary': 'Tesla opening new factory...'\n",
        "}\n",
        "```\n",
        "\n",
        "**Bonus Challenges:**\n",
        "1. Add zero-shot topic classification\n",
        "2. Handle multiple languages\n",
        "3. Create error handling for edge cases\n",
        "\n",
        "Good luck! \ud83d\ude80\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your solution code here!\n",
        "\n",
        "def analyze_text(text):\n",
        "    # TODO: Implement using pipelines\n",
        "    pass\n",
        "\n",
        "# Test your function\n",
        "sample = \"\"\"\n",
        "Elon Musk announced that Tesla will open a new factory in Austin, Texas.\n",
        "\"\"\"\n",
        "\n",
        "# results = analyze_text(sample)\n",
        "# print(results)\n",
        "\n",
        "print(\"\ud83d\udca1 Implement the function above and test it!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83d\udcda Summary - What We Learned Today\n",
        "\n",
        "### 1. Hugging Face Pipelines \ud83d\ude80\n",
        "- **One-line solutions** for sentiment, QA, generation, NER\n",
        "- **Pre-trained models** ready to use\n",
        "- **No training required** for common tasks\n",
        "\n",
        "### 2. Model Components \ud83d\udd27\n",
        "- **Tokenizers** convert text to numbers\n",
        "- **Models** process numbers into predictions\n",
        "- **Manual loading** gives full control\n",
        "\n",
        "### 3. Fine-Tuning \ud83c\udf93\n",
        "- **Adapt models** to your specific task\n",
        "- **Fast training** - minutes not weeks\n",
        "- **Small datasets** work well (1000 samples)\n",
        "- **Save and deploy** anywhere\n",
        "\n",
        "### 4. Hugging Face Hub \ud83c\udf10\n",
        "- **100,000+ models** available\n",
        "- **Free to use** and share\n",
        "- **Community driven** ecosystem\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\ude80 Next Week Preview\n",
        "\n",
        "**Week 14: LLMs and AI Agents**\n",
        "- Large Language Models (GPT, Claude)\n",
        "- Fine-tuning with LoRA\n",
        "- LangChain framework\n",
        "- Building chatbots with memory\n",
        "- RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udca1 Key Takeaways:\n",
        "\n",
        "\u2705 **Don't train from scratch** - use pre-trained models  \n",
        "\u2705 **Pipelines for speed** - 1-2 lines for most tasks  \n",
        "\u2705 **Fine-tune for custom** - adapt to your data  \n",
        "\u2705 **Production ready** - used by Google, Meta, Microsoft\n",
        "\n",
        "**Excellent work this week! \ud83c\udf89**\n",
        "\n",
        "You've mastered:\n",
        "- TensorFlow vs PyTorch (Day 1)\n",
        "- Transformer architecture (Day 2)\n",
        "- Hugging Face ecosystem (Day 3)\n",
        "\n",
        "You're now ready to build real-world NLP applications!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}