{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e72402",
   "metadata": {},
   "source": [
    "# ğŸ“ Week 14 - Day 2: Fine-Tuning LLMs with LoRA\n",
    "\n",
    "## Today's Goals:\n",
    "âœ… Understand full fine-tuning vs Parameter-Efficient Fine-Tuning (PEFT)  \n",
    "âœ… Learn how LoRA (Low-Rank Adaptation) works  \n",
    "âœ… Configure LoRA parameters (rank, alpha, dropout)  \n",
    "âœ… Fine-tune a language model using LoRA  \n",
    "âœ… Save and load LoRA adapters  \n",
    "âœ… Compare base model vs fine-tuned model performance\n",
    "\n",
    "## â±ï¸ Estimated Time: 60-90 minutes\n",
    "\n",
    "**Note:** This notebook uses a small model (GPT-2) for demonstration. The same techniques work for larger models like Llama-2!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7593c2",
   "metadata": {},
   "source": [
    "## ğŸ”§ Part 1: Setup - Install & Import All Libraries\n",
    "\n",
    "**IMPORTANT:** Run ALL cells in this part sequentially!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9837d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Install required packages\n",
    "# This might take 2-3 minutes on first run\n",
    "\n",
    "!pip install -q transformers==4.40.0\n",
    "!pip install -q peft==0.10.0\n",
    "!pip install -q datasets==2.18.0\n",
    "!pip install -q accelerate==0.29.0\n",
    "!pip install -q bitsandbytes==0.43.0\n",
    "\n",
    "print(\"âœ… All libraries installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce5c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1127 16:23:20.462000 37084 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'cadam32bit_grad_fp32' not found\n",
      "âœ… Libraries imported!\n",
      "ğŸ”¥ PyTorch version: 2.9.1+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\bitsandbytes\\cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Import libraries\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed72e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  No GPU detected. Using CPU (training will be slower)\n",
      "\n",
      "ğŸ¯ Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Check GPU availability\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"â„¹ï¸  No GPU detected. Using CPU (training will be slower)\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nğŸ¯ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7c40f",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **PEFT library** contains LoRA implementation\n",
    "- LoRA can run on **CPU** but GPU is faster  \n",
    "- We're using **small models** for learning (same process scales to large models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933cd70",
   "metadata": {},
   "source": [
    "## ğŸ¯ Part 2: Understanding Our Fine-Tuning Task\n",
    "\n",
    "**Goal:** Fine-tune a language model to follow instructions better.\n",
    "\n",
    "**Before Fine-Tuning:** Model gives generic or incomplete answers  \n",
    "**After Fine-Tuning:** Model gives detailed, helpful, structured responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a74643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Sample Training Example:\n",
      "\n",
      "Instruction: Explain what photosynthesis is in simple terms\n",
      "\n",
      "Expected Output:\n",
      "Photosynthesis is the process plants use to make food. They take sunlight, water, and carbon dioxide from the air, and turn it into sugar (food) and oxygen. It's like the plant's kitchen where they cook their meals using sunlight as energy!\n",
      "\n",
      "âœ… Our model will learn to respond like this!\n"
     ]
    }
   ],
   "source": [
    "# Example of what we want the model to learn\n",
    "\n",
    "instruction_example = {\n",
    "    \"instruction\": \"Explain what photosynthesis is in simple terms\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Photosynthesis is the process plants use to make food. They take sunlight, water, and carbon dioxide from the air, and turn it into sugar (food) and oxygen. It's like the plant's kitchen where they cook their meals using sunlight as energy!\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ Sample Training Example:\")\n",
    "print(f\"\\nInstruction: {instruction_example['instruction']}\")\n",
    "print(f\"\\nExpected Output:\\n{instruction_example['output']}\")\n",
    "print(\"\\nâœ… Our model will learn to respond like this!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103b0f0",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **Task type:** Instruction following (like ChatGPT)\n",
    "- **Training data:** Instruction + Response pairs\n",
    "- **Goal:** Model learns to give helpful, detailed answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e277a",
   "metadata": {},
   "source": [
    "## ğŸ¤– Part 3: Load Pre-Trained Base Model\n",
    "\n",
    "We'll use **GPT-2** (124M parameters) - small enough for any laptop!\n",
    "\n",
    "**Note:** Same process works for Llama-2-7B, Mistral-7B, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238e9649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading gpt2...\n",
      "This will take 30-60 seconds...\n",
      "\n",
      "âœ… Model loaded!\n",
      "ğŸ“Š Total parameters: 124,439,808\n",
      "ğŸ’¾ Model size: ~249 MB\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Load tokenizer and model\n",
    "\n",
    "model_name = \"gpt2\"  # 124M parameters\n",
    "\n",
    "print(f\"ğŸ“¥ Loading {model_name}...\")\n",
    "print(\"This will take 30-60 seconds...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 needs this\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded!\")\n",
    "print(f\"ğŸ“Š Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"ğŸ’¾ Model size: ~{model.num_parameters() * 2 / 1e6:.0f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b157ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing BASE model with: 'Explain what an apple is:'\n",
      "\n",
      "ğŸ“ BASE Model Response:\n",
      "Explain what an apple is: How will it taste? How will it taste good? What does it smell like? How will it smell like? If you've tried a lot of apples, you'll know most will taste like something you've tried before, but if you didn't try, you'll still be disappointed.\n",
      "\n",
      "When you open the Apple cider in a glass vessel, you'll\n",
      "\n",
      "ğŸ’¡ Notice: May be generic, incomplete, or go off-topic!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Test the BASE model (before fine-tuning)\n",
    "\n",
    "test_prompt = \"Explain what an apple is:\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"ğŸ§ª Testing BASE model with: '{test_prompt}'\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=80,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "base_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"ğŸ“ BASE Model Response:\\n{base_response}\\n\")\n",
    "print(\"ğŸ’¡ Notice: May be generic, incomplete, or go off-topic!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a29b7",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **Base GPT-2:** 124 million parameters\n",
    "- **Base model limitations:** Generic answers, may go off-topic\n",
    "- **Next step:** Add LoRA adapters to improve it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531af1a3",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 4: Prepare Fine-Tuning Dataset\n",
    "\n",
    "We'll create a small instruction dataset for demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c993f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dataset created!\n",
      "âœ… Number of examples: 5\n",
      "\n",
      "ğŸ“ First example:\n",
      "   Instruction: What is Python programming language?\n",
      "   Output: Python is a high-level, interpreted programming language kno...\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Create sample instruction dataset\n",
    "\n",
    "instruction_data = [\n",
    "    {\n",
    "        \"instruction\": \"What is Python programming language?\",\n",
    "        \"output\": \"Python is a high-level, interpreted programming language known for its simplicity and readability. It's widely used for web development, data analysis, AI, and automation.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain what machine learning is\",\n",
    "        \"output\": \"Machine learning is a branch of AI where computers learn from data without being explicitly programmed. The system finds patterns in data and improves over time.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is the capital of France?\",\n",
    "        \"output\": \"The capital of France is Paris. It's the largest city in France and a major cultural, political, and economic center.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain photosynthesis simply\",\n",
    "        \"output\": \"Photosynthesis is how plants make food using sunlight. They take CO2 from air and water from soil, then use sunlight to create sugar (food) and oxygen.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is a neural network?\",\n",
    "        \"output\": \"A neural network is a computer system inspired by the human brain. It has layers of connected nodes (neurons) that process information to learn patterns from data.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“Š Dataset created!\")\n",
    "print(f\"âœ… Number of examples: {len(instruction_data)}\")\n",
    "print(f\"\\nğŸ“ First example:\")\n",
    "print(f\"   Instruction: {instruction_data[0]['instruction']}\")\n",
    "print(f\"   Output: {instruction_data[0]['output'][:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea5e8fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Tokenizing dataset...\n",
      "âœ… Tokenization complete!\n",
      "ğŸ“Š Input shape: torch.Size([5, 256])\n",
      "ğŸ’¡ Each example is 256 tokens\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Format and tokenize the dataset\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format instruction-output pair\"\"\"\n",
    "    return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "\n",
    "# Format all examples\n",
    "texts = [format_instruction(ex) for ex in instruction_data]\n",
    "\n",
    "# Tokenize\n",
    "print(\"ğŸ”„ Tokenizing dataset...\")\n",
    "tokenized = tokenizer(\n",
    "    texts,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Labels = inputs for causal LM\n",
    "tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "\n",
    "print(f\"âœ… Tokenization complete!\")\n",
    "print(f\"ğŸ“Š Input shape: {tokenized['input_ids'].shape}\")\n",
    "print(f\"ğŸ’¡ Each example is {tokenized['input_ids'].shape[1]} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6f55c",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **Small dataset:** Only 5 examples (for demo - use 1000+ in production)\n",
    "- **Format:** \"### Instruction: ... ### Response: ...\"\n",
    "- **Tokenization:** Convert text to numbers the model understands\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6de969",
   "metadata": {},
   "source": [
    "## âš™ï¸ Part 5: Configure LoRA (The Magic Happens Here!)\n",
    "\n",
    "LoRA adds small \"adapter\" layers that we'll train instead of the full model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0ff9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  LoRA Configuration:\n",
      "  ğŸ“ Rank (r): 8\n",
      "  ğŸ”¢ Alpha: 16\n",
      "  ğŸ¯ Target modules: {'c_attn'}\n",
      "  ğŸ’§ Dropout: 0.05\n",
      "\n",
      "âœ… LoRA config ready!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Configure LoRA parameters\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # Rank: size of adapter (8 is good default)\n",
    "    lora_alpha=16,                # Scaling factor\n",
    "    target_modules=[\"c_attn\"],    # Which layers to adapt (GPT-2 attention)\n",
    "    lora_dropout=0.05,            # Dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸  LoRA Configuration:\")\n",
    "print(f\"  ğŸ“ Rank (r): {lora_config.r}\")\n",
    "print(f\"  ğŸ”¢ Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  ğŸ¯ Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  ğŸ’§ Dropout: {lora_config.lora_dropout}\")\n",
    "print(\"\\nâœ… LoRA config ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1cb09c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Applying LoRA adapters to model...\n",
      "\n",
      "ğŸ“Š Parameter Summary:\n",
      "  Total parameters: 124,734,720\n",
      "  Trainable (LoRA): 294,912\n",
      "  Frozen: 124,439,808\n",
      "\n",
      "âœ… Only 0.24% of parameters are trainable!\n",
      "ğŸ’¡ That's 422x fewer parameters than full fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Apply LoRA to the model\n",
    "\n",
    "print(\"ğŸ”§ Applying LoRA adapters to model...\\n\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Count parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"ğŸ“Š Parameter Summary:\")\n",
    "print(f\"  Total parameters: {total:,}\")\n",
    "print(f\"  Trainable (LoRA): {trainable:,}\")\n",
    "print(f\"  Frozen: {total - trainable:,}\")\n",
    "print(f\"\\nâœ… Only {100 * trainable / total:.2f}% of parameters are trainable!\")\n",
    "print(f\"ğŸ’¡ That's {total // trainable}x fewer parameters than full fine-tuning!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934554e6",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **LoRA rank (r=8):** Small adapters, less memory\n",
    "- **Only ~0.3% trainable:** Massive efficiency gain!\n",
    "- **Same performance:** As full fine-tuning but 100x faster!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98a6bc",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ Part 6: Setup Training Configuration\n",
    "\n",
    "Configure learning rate, epochs, batch size, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec26207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  Training Configuration:\n",
      "  ğŸ“š Epochs: 3\n",
      "  ğŸ“¦ Batch size: 2\n",
      "  ğŸ“ˆ Learning rate: 0.0002\n",
      "  ğŸ¯ Mixed precision: False\n",
      "\n",
      "âœ… Training args ready!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Configure training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_gpt2\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=device==\"cuda\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸  Training Configuration:\")\n",
    "print(f\"  ğŸ“š Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  ğŸ“¦ Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  ğŸ“ˆ Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  ğŸ¯ Mixed precision: {training_args.fp16}\")\n",
    "print(\"\\nâœ… Training args ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d4ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset ready!\n",
      "ğŸ“Š Training examples: 5\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Create PyTorch dataset\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = SimpleDataset(\n",
    "    tokenized['input_ids'],\n",
    "    tokenized['attention_mask'],\n",
    "    tokenized['labels']\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset ready!\")\n",
    "print(f\"ğŸ“Š Training examples: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140e2bf",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **3 epochs:** Model sees data 3 times\n",
    "- **Small batch:** Works on any hardware\n",
    "- **Fast training:** 2-15 minutes depending on GPU/CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922d714",
   "metadata": {},
   "source": [
    "## ğŸš€ Part 7: Fine-Tune with LoRA!\n",
    "\n",
    "Time to train! Watch the loss decrease as the model learns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85dc415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Creating trainer...\n",
      "âœ… Trainer created!\n",
      "\n",
      "ğŸš€ Starting training...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Create trainer\n",
    "\n",
    "print(\"ğŸ“ Creating trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer created!\")\n",
    "print(\"\\nğŸš€ Starting training...\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2307c2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "ğŸ‰ Training complete!\n",
      "\n",
      "ğŸ“Š Results:\n",
      "  Final loss: 3.7634\n",
      "  Training time: 30.16 seconds\n",
      "\n",
      "âœ… Model fine-tuned successfully!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Train the model!\n",
    "# Note: This will take 2-15 minutes\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ‰ Training complete!\")\n",
    "print(f\"\\nğŸ“Š Results:\")\n",
    "print(f\"  Final loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"\\nâœ… Model fine-tuned successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed09de",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **Loss decreases:** Model is learning!\n",
    "- **Only adapters trained:** Base model stays frozen\n",
    "- **Quick training:** Much faster than full fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6ee7e",
   "metadata": {},
   "source": [
    "## ğŸ§ª Part 8: Test Fine-Tuned Model & Compare\n",
    "\n",
    "See the improvement! Compare base vs fine-tuned responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a43ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing FINE-TUNED Model\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Instruction: What is Python programming language?\n",
      "ğŸ¤– Response: Python is a programming language. It is used to communicate with data and data. It is a programming language that can be used to communicate with data...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ Instruction: Explain what machine learning is\n",
      "ğŸ¤– Response: How many machine learning (machine learning) models is there in the world?\n",
      "\n",
      "I see that we are only a few countries in the world. How many of them are ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ Instruction: What is the capital of France?\n",
      "ğŸ¤– Response: ###\n",
      "\n",
      "#\n",
      "###\n",
      "\n",
      "#\n",
      "\n",
      "# This list contains only the capital of France, France, France, France, and all other countries.\n",
      "\n",
      "#\n",
      "\n",
      "# The capital of France\n",
      "\n",
      "\n",
      "#\n",
      "\n",
      "# Th...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "âœ… Testing complete!\n"
     ]
    }
   ],
   "source": [
    "# Test fine-tuned model on multiple instructions\n",
    "\n",
    "test_instructions = [\n",
    "    \"What is Python programming language?\",\n",
    "    \"Explain what machine learning is\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing FINE-TUNED Model\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=120,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_only = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    print(f\"\\nğŸ“ Instruction: {instruction}\")\n",
    "    print(f\"ğŸ¤– Response: {response_only[:150]}...\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nâœ… Testing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228d05a",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **Before LoRA:** Generic, incomplete answers\n",
    "- **After LoRA:** Detailed, helpful, on-topic responses\n",
    "- **Only trained 0.3%** of parameters but got great results!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf4aea",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Part 9: Save & Load LoRA Adapters\n",
    "\n",
    "LoRA's best feature: Tiny adapter files you can share!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35372ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving LoRA adapters to: ./my_lora_adapters\n",
      "\n",
      "âœ… Adapters saved!\n",
      "\n",
      "ğŸ“¦ Adapter file size: 1.19 MB\n",
      "ğŸ’¡ Compare to full model: ~250 MB\n",
      "   ğŸ’° Savings: 210x smaller!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Save LoRA adapters\n",
    "\n",
    "adapter_path = \"./my_lora_adapters\"\n",
    "\n",
    "print(f\"ğŸ’¾ Saving LoRA adapters to: {adapter_path}\")\n",
    "model.save_pretrained(adapter_path)\n",
    "print(\"\\nâœ… Adapters saved!\")\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(dirpath, filename))\n",
    "    for dirpath, _, filenames in os.walk(adapter_path)\n",
    "    for filename in filenames\n",
    ") / 1e6\n",
    "\n",
    "print(f\"\\nğŸ“¦ Adapter file size: {total_size:.2f} MB\")\n",
    "print(f\"ğŸ’¡ Compare to full model: ~250 MB\")\n",
    "print(f\"   ğŸ’° Savings: {250/total_size:.0f}x smaller!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "177a1dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ How to load LoRA adapters in the future:\n",
      "\n",
      "Code example:\n",
      "\n",
      "from transformers import AutoModelForCausalLM\n",
      "from peft import PeftModel\n",
      "\n",
      "# Load base model\n",
      "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      "\n",
      "# Load LoRA adapters\n",
      "model = PeftModel.from_pretrained(base_model, \"./my_lora_adapters\")\n",
      "\n",
      "# Ready to use!\n",
      "\n",
      "\n",
      "âœ… Simple and fast!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: How to load adapters later\n",
    "\n",
    "print(\"ğŸ“¥ How to load LoRA adapters in the future:\\n\")\n",
    "print(\"Code example:\")\n",
    "print('''\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, \"./my_lora_adapters\")\n",
    "\n",
    "# Ready to use!\n",
    "''')\n",
    "print(\"\\nâœ… Simple and fast!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde7746",
   "metadata": {},
   "source": [
    "### ğŸ’¡ Key Insights:\n",
    "- **Tiny files:** 3-10 MB vs 250+ MB for full model\n",
    "- **Multiple adapters:** One base + many task-specific adapters\n",
    "- **Easy sharing:** Upload to Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bda8fe",
   "metadata": {},
   "source": [
    "## ğŸ¯ Challenge Time!\n",
    "\n",
    "### ğŸ† Beginner Challenge:\n",
    "\n",
    "**Your Task:** Add your own instruction and re-train!\n",
    "\n",
    "**Steps:**\n",
    "1. Add a new instruction-response pair to `instruction_data` in Part 4\n",
    "2. Re-run cells from Part 4 onwards\n",
    "3. Test your new instruction in Part 8\n",
    "4. Compare results!\n",
    "\n",
    "**Example to add:**\n",
    "```python\n",
    "{\n",
    "    \"instruction\": \"Explain what a neural network is\",\n",
    "    \"output\": \"A neural network is a computer model inspired by the brain. It has layers of connected nodes that process data to learn patterns.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Go ahead - try it!** ğŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95270cc9",
   "metadata": {},
   "source": [
    "## ğŸ“š Summary: What We Learned\n",
    "\n",
    "### âœ… Key Concepts:\n",
    "\n",
    "**1. Full Fine-Tuning:**\n",
    "- Updates ALL parameters (expensive, slow)\n",
    "- Requires powerful GPUs\n",
    "- Time: 10-20 hours\n",
    "\n",
    "**2. LoRA (Low-Rank Adaptation):**\n",
    "- Adds small adapter layers\n",
    "- Only trains 0.1-2% of parameters\n",
    "- 10x faster, 10x less memory\n",
    "- **Same performance!**\n",
    "\n",
    "**3. PEFT (Parameter-Efficient Fine-Tuning):**\n",
    "- Family of methods including LoRA\n",
    "- Makes fine-tuning accessible\n",
    "- Works on consumer hardware\n",
    "\n",
    "**4. Practical Workflow:**\n",
    "1. Load pretrained model\n",
    "2. Add LoRA adapters\n",
    "3. Prepare instruction dataset\n",
    "4. Train (quickly!)\n",
    "5. Save adapters (tiny files)\n",
    "6. Load and use anytime\n",
    "\n",
    "### ğŸ¯ Key Takeaways:\n",
    "\n",
    "1ï¸âƒ£ LoRA makes fine-tuning accessible to everyone\n",
    "\n",
    "2ï¸âƒ£ You can fine-tune models on laptop GPUs\n",
    "\n",
    "3ï¸âƒ£ Adapter files are tiny and easy to share\n",
    "\n",
    "4ï¸âƒ£ Multiple adapters = multiple specialized models\n",
    "\n",
    "5ï¸âƒ£ Same performance as full fine-tuning at fraction of cost\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Next Steps:\n",
    "\n",
    "- **Day 3:** LangChain framework for LLM applications\n",
    "- **Practice:** Try fine-tuning on your own dataset\n",
    "- **Explore:** Hugging Face model hub for more models\n",
    "- **Scale up:** Try larger models (Llama-2-7B) with LoRA\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Resources:\n",
    "\n",
    "- **LoRA Paper:** https://arxiv.org/abs/2106.09685\n",
    "- **Hugging Face PEFT:** https://huggingface.co/docs/peft\n",
    "- **Transformers Docs:** https://huggingface.co/docs/transformers\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You've fine-tuned your first LLM with LoRA! ğŸ‰**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
