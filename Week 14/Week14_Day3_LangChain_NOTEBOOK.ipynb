{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65579d0f",
   "metadata": {},
   "source": [
    "# üéì Week 14 - Day 3: LangChain I - Building with LLMs\n",
    "\n",
    "## Today's Goals:\n",
    "‚úÖ Understand what LangChain is and why it's useful  \n",
    "‚úÖ Work with Prompt Templates  \n",
    "‚úÖ Build and use Chains  \n",
    "‚úÖ Create conversational agents with memory  \n",
    "‚úÖ Use Runnables (modern LangChain API)\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time: 90 minutes\n",
    "\n",
    "**Note:** We'll use OpenAI models. You'll need an API key (free tier available).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc76ebb",
   "metadata": {},
   "source": [
    "## üîß Part 1: Setup - Install & Import All Libraries\n",
    "\n",
    "**IMPORTANT:** Run ALL cells in this part sequentially!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca428be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Zigron\\AppData\\Local\\Temp\\pip-uninstall-iky5on51'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "sip 6.12.0 requires packaging>=24.2, but you have packaging 23.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain libraries installed!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Install required packages\n",
    "\n",
    "!pip install -q langchain==0.1.20\n",
    "!pip install -q langchain-openai==0.1.7\n",
    "!pip install -q langchain-community==0.0.38\n",
    "\n",
    "print(\"‚úÖ LangChain libraries installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8372bacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Import libraries\n",
    "\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69397c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API configured!\n",
      "üí° Replace 'your-api-key-here' with your actual API key!\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Setup API key\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# Get free key: https://platform.openai.com/api-keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "print(\"‚úÖ API configured!\")\n",
    "print(\"üí° Replace 'your-api-key-here' with your actual API key!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db45eb",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **LangChain** = Framework for LLM applications\n",
    "- **langchain-openai** for OpenAI models\n",
    "- Need **API key** to use OpenAI models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09d184",
   "metadata": {},
   "source": [
    "## ü§ñ Part 2: Your First LangChain LLM Call\n",
    "\n",
    "Start simple - make a direct LLM call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334601c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized!\n",
      "Model: gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Initialize the LLM\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized!\")\n",
    "print(f\"Model: {llm.model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a71af9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api*****here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# STEP 2: Make a simple call\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExplain LangChain in one sentence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mü§ñ Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:158\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    149\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     **kwargs: Any,\n\u001b[32m    154\u001b[39m ) -> BaseMessage:\n\u001b[32m    155\u001b[39m     config = ensure_config(config)\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    157\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    168\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:560\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    554\u001b[39m     prompts: List[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m     **kwargs: Any,\n\u001b[32m    558\u001b[39m ) -> LLMResult:\n\u001b[32m    559\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:421\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    420\u001b[39m             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    422\u001b[39m flattened_outputs = [\n\u001b[32m    423\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    425\u001b[39m ]\n\u001b[32m    426\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    410\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m         )\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:632\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    636\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:522\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    520\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    521\u001b[39m params = {**params, **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api*****here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# STEP 2: Make a simple call\n",
    "\n",
    "response = llm.invoke(\"Explain LangChain in one sentence\")\n",
    "\n",
    "print(\"ü§ñ Response:\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e05e2a",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **ChatOpenAI** wraps OpenAI API\n",
    "- **invoke()** sends prompt, gets response\n",
    "- **temperature** controls creativity (0-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6573abd",
   "metadata": {},
   "source": [
    "## üìù Part 3: Prompt Templates - Reusable Prompts\n",
    "\n",
    "Templates make prompts reusable with variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create a prompt template\n",
    "\n",
    "template = \"Tell me a {adjective} joke about {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"topic\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt template created!\")\n",
    "print(f\"Template: {template}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Use template with different inputs\n",
    "\n",
    "# Example 1\n",
    "p1 = prompt.format(adjective=\"funny\", topic=\"programming\")\n",
    "print(\"Example 1:\", p1)\n",
    "\n",
    "# Example 2\n",
    "p2 = prompt.format(adjective=\"short\", topic=\"AI\")\n",
    "print(\"Example 2:\", p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Combine template with LLM\n",
    "\n",
    "formatted = prompt.format(adjective=\"clever\", topic=\"Python\")\n",
    "response = llm.invoke(formatted)\n",
    "\n",
    "print(\"ü§ñ Joke:\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a94ef",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Templates** = Reusable prompts\n",
    "- **Variables** in {braces}\n",
    "- **Cleaner code** - separate logic from data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6f79f",
   "metadata": {},
   "source": [
    "## ‚õìÔ∏è Part 4: Chains - Connecting Components\n",
    "\n",
    "Chains combine prompts + LLMs into workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67276e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create an LLMChain\n",
    "\n",
    "template = \"You are a helpful assistant. Answer: {question}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(\"‚úÖ LLMChain created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Run the chain\n",
    "\n",
    "response = chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "\n",
    "print(\"ü§ñ Response:\")\n",
    "print(response[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29320688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Use chain multiple times\n",
    "\n",
    "questions = [\n",
    "    \"What is a prompt template?\",\n",
    "    \"What are chains?\",\n",
    "    \"What is an agent?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response = chain.invoke({\"question\": q})\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {response['text'][:80]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a893f",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **LLMChain** = Prompt + LLM\n",
    "- **Reusable** workflow\n",
    "- Call with different inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb060e",
   "metadata": {},
   "source": [
    "## üîó Part 5: Sequential Chains - Multi-Step Workflows\n",
    "\n",
    "Chain multiple LLM calls together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147768ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create first chain\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a story idea about {topic}. 1-2 sentences.\"\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"idea\")\n",
    "\n",
    "print(\"‚úÖ Chain 1: Story idea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a881c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Create second chain\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"idea\"],\n",
    "    template=\"Expand this idea: {idea}. Give 3 plot points.\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"outline\")\n",
    "\n",
    "print(\"‚úÖ Chain 2: Outline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9105d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Combine into sequential chain\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "full_chain = SimpleSequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sequential chain!\")\n",
    "print(\"Flow: Topic ‚Üí Idea ‚Üí Outline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5729962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Run sequential chain\n",
    "\n",
    "result = full_chain.invoke(\"space exploration\")\n",
    "\n",
    "print(\"üìù Final Outline:\")\n",
    "print(result[\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae8f61",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Sequential** = Multiple steps\n",
    "- Output1 ‚Üí Input2\n",
    "- **Multi-step** tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc17354",
   "metadata": {},
   "source": [
    "## üí¨ Part 6: Conversational Chains - Chatbots with Memory\n",
    "\n",
    "Build chatbot that remembers conversation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ba0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create memory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Memory initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2dabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Create conversational chain\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversational chain!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Have a conversation\n",
    "\n",
    "# Message 1\n",
    "r1 = conversation.predict(input=\"Hi! I'm Alex, I love Python.\")\n",
    "print(f\"Human: Hi! I'm Alex, I love Python.\")\n",
    "print(f\"AI: {r1[:100]}...\")\n",
    "print()\n",
    "\n",
    "# Message 2\n",
    "r2 = conversation.predict(input=\"What's my name?\")\n",
    "print(f\"Human: What's my name?\")\n",
    "print(f\"AI: {r2}\")\n",
    "print()\n",
    "\n",
    "# Message 3\n",
    "r3 = conversation.predict(input=\"What language did I mention?\")\n",
    "print(f\"Human: What language did I mention?\")\n",
    "print(f\"AI: {r3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161e55c",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Memory** stores history\n",
    "- Chain **remembers** context\n",
    "- Perfect for **chatbots**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891556c",
   "metadata": {},
   "source": [
    "## üèÉ Part 7: Runnables - Modern LangChain API\n",
    "\n",
    "The new, cleaner way to work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ff935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create chain with LCEL\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a fact about {topic}\"\n",
    ")\n",
    "\n",
    "# Modern chain with pipe\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"‚úÖ Runnable chain!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575747e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Use invoke()\n",
    "\n",
    "result = chain.invoke({\"topic\": \"AI\"})\n",
    "\n",
    "print(\"ü§ñ Result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Use batch()\n",
    "\n",
    "topics = [\n",
    "    {\"topic\": \"ML\"},\n",
    "    {\"topic\": \"DL\"},\n",
    "    {\"topic\": \"NLP\"}\n",
    "]\n",
    "\n",
    "results = chain.batch(topics)\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. {r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca278f",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Runnables** = Modern API\n",
    "- **Pipe |** chains components\n",
    "- **Cleaner, Pythonic**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d780842",
   "metadata": {},
   "source": [
    "## ü§ñ Part 8: Simple Conversational Agent\n",
    "\n",
    "Create smart agent with context!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133984da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create agent with system prompt\n",
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "print(\"‚úÖ Agent chain created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Create conversation loop\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def chat(message):\n",
    "    response = chain.invoke({\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": message\n",
    "    })\n",
    "    \n",
    "    # Add to history\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "    chat_history.append(response)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "print(\"‚úÖ Chat function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32aee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Test the agent\n",
    "\n",
    "print(\"üí¨ Conversation:\\n\")\n",
    "\n",
    "# Turn 1\n",
    "r1 = chat(\"Hi! I'm learning about LangChain.\")\n",
    "print(f\"You: Hi! I'm learning about LangChain.\")\n",
    "print(f\"AI: {r1}\\n\")\n",
    "\n",
    "# Turn 2\n",
    "r2 = chat(\"What was I just talking about?\")\n",
    "print(f\"You: What was I just talking about?\")\n",
    "print(f\"AI: {r2}\\n\")\n",
    "\n",
    "# Turn 3\n",
    "r3 = chat(\"Give me 3 tips for learning it.\")\n",
    "print(f\"You: Give me 3 tips for learning it.\")\n",
    "print(f\"AI: {r3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322b7d9",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **System prompts** set behavior\n",
    "- **MessagesPlaceholder** for history\n",
    "- **Manual memory** management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e512fe",
   "metadata": {},
   "source": [
    "## üéØ Challenge Time!\n",
    "\n",
    "### üèÜ Beginner Challenge:\n",
    "\n",
    "**Task:** Build a translator chatbot!\n",
    "\n",
    "**Requirements:**\n",
    "1. User can ask to translate sentences\n",
    "2. Bot remembers what language they're translating to\n",
    "3. Use ConversationChain with memory\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"I want to translate to Spanish\"\n",
    "Bot: \"Great! I'll translate to Spanish for you.\"\n",
    "\n",
    "User: \"Hello, how are you?\"\n",
    "Bot: \"Hola, ¬øc√≥mo est√°s?\"\n",
    "```\n",
    "\n",
    "**Hint:** Use the ConversationChain from Part 6!\n",
    "\n",
    "**Try it yourself!** üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade94890",
   "metadata": {},
   "source": [
    "## üìö Summary: What We Learned\n",
    "\n",
    "### ‚úÖ Key Concepts:\n",
    "\n",
    "**1. LangChain:**\n",
    "- Framework for LLM applications\n",
    "- Pre-built components\n",
    "- Production-ready patterns\n",
    "\n",
    "**2. Prompt Templates:**\n",
    "- Reusable prompts with variables\n",
    "- Clean separation of logic and data\n",
    "- Easy to maintain\n",
    "\n",
    "**3. Chains:**\n",
    "- Connect components into workflows\n",
    "- LLMChain = basic building block\n",
    "- Sequential chains for multi-step\n",
    "\n",
    "**4. Memory:**\n",
    "- Store conversation history\n",
    "- Enable contextual responses\n",
    "- Perfect for chatbots\n",
    "\n",
    "**5. Runnables:**\n",
    "- Modern LangChain API\n",
    "- Pipe operator |\n",
    "- invoke(), batch(), stream()\n",
    "\n",
    "**6. Agents:**\n",
    "- LLMs that make decisions\n",
    "- Use tools and maintain context\n",
    "- Next level of capability\n",
    "\n",
    "### üéØ Key Takeaways:\n",
    "\n",
    "1Ô∏è‚É£ LangChain simplifies LLM app development\n",
    "\n",
    "2Ô∏è‚É£ Templates make prompts reusable\n",
    "\n",
    "3Ô∏è‚É£ Chains connect components into workflows\n",
    "\n",
    "4Ô∏è‚É£ Memory enables natural conversations\n",
    "\n",
    "5Ô∏è‚É£ Runnables provide clean, modern API\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "- **Day 4:** LangChain II + LangSmith + LangGraph\n",
    "- **Practice:** Build your own chatbot\n",
    "- **Explore:** LangChain documentation\n",
    "- **Experiment:** Try different chain combinations\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Resources:\n",
    "\n",
    "- **LangChain Docs:** https://python.langchain.com/docs/\n",
    "- **LangChain GitHub:** https://github.com/langchain-ai/langchain\n",
    "- **Tutorials:** https://python.langchain.com/docs/tutorials/\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You've learned LangChain basics! üéâ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
