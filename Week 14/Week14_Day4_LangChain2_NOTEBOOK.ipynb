{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff58a3c",
   "metadata": {},
   "source": [
    "# üéì Week 14 - Day 4: LangChain II - Advanced Features\n",
    "\n",
    "## Today's Goals:\n",
    "‚úÖ Create structured outputs with Pydantic  \n",
    "‚úÖ Implement advanced memory patterns  \n",
    "‚úÖ Use LangSmith for debugging and monitoring  \n",
    "‚úÖ Build production-ready LLM applications  \n",
    "‚úÖ Optimize chain performance\n",
    "\n",
    "## ‚è±Ô∏è Estimated Time: 90 minutes\n",
    "\n",
    "**Note:** We'll use OpenAI and LangSmith (both have free tiers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219dcce1",
   "metadata": {},
   "source": [
    "## üîß Part 1: Setup - Install & Import All Libraries\n",
    "\n",
    "**IMPORTANT:** Run ALL cells in this part sequentially!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce11cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\~ydantic_core'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries installed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.20 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.1.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Install required packages\n",
    "\n",
    "!pip install -q langchain==0.1.20\n",
    "!pip install -q langchain-openai==0.1.7\n",
    "!pip install -q pydantic==2.6.4\n",
    "!pip install -q langsmith==0.1.0\n",
    "\n",
    "print(\"‚úÖ All libraries installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702ea2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Import libraries\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryMemory\n",
    ")\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c37c37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys configured!\n",
      "üí° Replace API keys with your actual keys!\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Setup API keys\n",
    "\n",
    "# OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# LangSmith API key (optional - for tracing)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-key\"  # Get from smith.langchain.com\n",
    "\n",
    "print(\"‚úÖ API keys configured!\")\n",
    "print(\"üí° Replace API keys with your actual keys!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b08e5",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Pydantic** for data validation\n",
    "- **LangSmith** for debugging (optional but recommended)\n",
    "- All features work without LangSmith too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf42d0",
   "metadata": {},
   "source": [
    "## üìù Part 2: The Problem with Unstructured Outputs\n",
    "\n",
    "See why we need structured outputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6697913",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api*****here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m)\n\u001b[32m      5\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mExtract the person\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms name and age from: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mJohn is 25 years old\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Return as JSON.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mü§ñ LLM Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:158\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    149\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     **kwargs: Any,\n\u001b[32m    154\u001b[39m ) -> BaseMessage:\n\u001b[32m    155\u001b[39m     config = ensure_config(config)\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    157\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    168\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:560\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    554\u001b[39m     prompts: List[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m     **kwargs: Any,\n\u001b[32m    558\u001b[39m ) -> LLMResult:\n\u001b[32m    559\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:421\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    420\u001b[39m             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    422\u001b[39m flattened_outputs = [\n\u001b[32m    423\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    425\u001b[39m ]\n\u001b[32m    426\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    410\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m         )\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:632\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    636\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:522\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    520\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    521\u001b[39m params = {**params, **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-api*****here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# STEP 1: Make a normal LLM call\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "prompt = \"Extract the person's name and age from: 'John is 25 years old'. Return as JSON.\"\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(\"ü§ñ LLM Response:\")\n",
    "print(response.content)\n",
    "print(\"\\n‚ö†Ô∏è  Issues:\")\n",
    "print(\"1. Response is a string, not structured data\")\n",
    "print(\"2. May include extra text\")\n",
    "print(\"3. Format can vary\")\n",
    "print(\"4. Hard to parse reliably\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25ed65",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Raw LLM output** = String (unpredictable)\n",
    "- Need to **parse manually** (error-prone)\n",
    "- **Not production-ready**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694d0b2",
   "metadata": {},
   "source": [
    "## ‚úÖ Part 3: Structured Outputs with Pydantic\n",
    "\n",
    "Define exact output format!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Define output schema with Pydantic\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Person information\"\"\"\n",
    "    name: str = Field(description=\"The person's full name\")\n",
    "    age: int = Field(description=\"The person's age in years\")\n",
    "\n",
    "print(\"‚úÖ Person schema created!\")\n",
    "print(f\"Fields: {Person.__fields__.keys()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Create parser and prompt\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Person)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract person information. {format_instructions}\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# Add format instructions\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "print(\"‚úÖ Parser and prompt ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Create chain with structured output\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Test it\n",
    "result = chain.invoke({\"text\": \"Sarah is 30 years old and lives in NYC\"})\n",
    "\n",
    "print(\"‚úÖ Structured Result:\")\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Name: {result['name']}\")\n",
    "print(f\"Age: {result['age']}\")\n",
    "print(\"\\nüí° Clean, validated Python dict!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c137e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Try multiple examples\n",
    "\n",
    "texts = [\n",
    "    \"Mike is 45 years old\",\n",
    "    \"Emma turned 22 yesterday\",\n",
    "    \"The doctor, Alex Chen, is 38\"\n",
    "]\n",
    "\n",
    "print(\"üìä Batch Processing:\\n\")\n",
    "for text in texts:\n",
    "    result = chain.invoke({\"text\": text})\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  ‚Üí Name: {result['name']}, Age: {result['age']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eef147",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Pydantic schema** defines exact structure\n",
    "- **Parser** enforces the schema\n",
    "- **Reliable** output every time\n",
    "- **Production-ready** validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0932692",
   "metadata": {},
   "source": [
    "## üìã Part 4: Complex Structured Outputs\n",
    "\n",
    "Extract multiple entities!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb985e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Define complex schema\n",
    "\n",
    "class Task(BaseModel):\n",
    "    \"\"\"A single task\"\"\"\n",
    "    title: str = Field(description=\"Task title\")\n",
    "    priority: str = Field(description=\"Priority: high, medium, or low\")\n",
    "    completed: bool = Field(description=\"Whether task is done\")\n",
    "\n",
    "class TaskList(BaseModel):\n",
    "    \"\"\"List of tasks\"\"\"\n",
    "    tasks: List[Task] = Field(description=\"List of tasks\")\n",
    "\n",
    "print(\"‚úÖ Complex schema created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Create parser for complex schema\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=TaskList)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract tasks from the text. {format_instructions}\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(\"‚úÖ Complex chain ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Test complex extraction\n",
    "\n",
    "text = \"\"\"\n",
    "I need to:\n",
    "1. Finish the project report (high priority, not done yet)\n",
    "2. Reply to emails (medium priority, already done)\n",
    "3. Buy groceries (low priority, not done)\n",
    "\"\"\"\n",
    "\n",
    "result = chain.invoke({\"text\": text})\n",
    "\n",
    "print(\"üìã Extracted Tasks:\\n\")\n",
    "for i, task in enumerate(result['tasks'], 1):\n",
    "    status = \"‚úÖ\" if task['completed'] else \"‚¨ú\"\n",
    "    print(f\"{i}. {status} [{task['priority'].upper()}] {task['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee3073",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Nested structures** work perfectly\n",
    "- **Lists of objects** handled automatically\n",
    "- **Complex extraction** made simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdedb86",
   "metadata": {},
   "source": [
    "## ü™ü Part 5: Window Memory - Recent Context Only\n",
    "\n",
    "Keep only last N messages for efficiency!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create window memory (keep last 2 exchanges)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=2,  # Keep last 2 exchanges (4 messages)\n",
    "    return_messages=True,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Window memory created!\")\n",
    "print(f\"Window size: {memory.k} exchanges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a944d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Simulate conversation\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"üí¨ Conversation:\\n\")\n",
    "\n",
    "# Message 1\n",
    "r1 = conversation.predict(input=\"Hi! My favorite color is blue.\")\n",
    "print(\"You: Hi! My favorite color is blue.\")\n",
    "print(f\"AI: {r1[:60]}...\\n\")\n",
    "\n",
    "# Message 2\n",
    "r2 = conversation.predict(input=\"I also love pizza.\")\n",
    "print(\"You: I also love pizza.\")\n",
    "print(f\"AI: {r2[:60]}...\\n\")\n",
    "\n",
    "# Message 3\n",
    "r3 = conversation.predict(input=\"And I have a dog named Max.\")\n",
    "print(\"You: And I have a dog named Max.\")\n",
    "print(f\"AI: {r3[:60]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ff265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Test memory (only remembers recent messages)\n",
    "\n",
    "# This should remember dog (recent)\n",
    "r4 = conversation.predict(input=\"What's my dog's name?\")\n",
    "print(\"You: What's my dog's name?\")\n",
    "print(f\"AI: {r4}\\n\")\n",
    "\n",
    "# This might NOT remember color (too old - outside window)\n",
    "r5 = conversation.predict(input=\"What's my favorite color?\")\n",
    "print(\"You: What's my favorite color?\")\n",
    "print(f\"AI: {r5}\")\n",
    "print(\"\\nüí° Window memory forgot old messages!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bfbd00",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Window memory** = Only last K exchanges\n",
    "- **Efficient** for long conversations\n",
    "- **Recent context** matters most\n",
    "- Trade-off: **Forgets old info**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08efe9",
   "metadata": {},
   "source": [
    "## üìù Part 6: Summary Memory - Compress Old Messages\n",
    "\n",
    "Summarize old messages to save tokens!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create summary memory\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Summary memory created!\")\n",
    "print(\"Old messages will be summarized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0461d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Use summary memory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=True  # See summaries\n",
    ")\n",
    "\n",
    "print(\"üí¨ Conversation with Summary:\\n\")\n",
    "\n",
    "# Have a longer conversation\n",
    "messages = [\n",
    "    \"I'm planning a trip to Japan next month.\",\n",
    "    \"I want to visit Tokyo, Kyoto, and Osaka.\",\n",
    "    \"My budget is around $3000.\",\n",
    "    \"I love trying new foods.\",\n",
    "    \"What should I pack?\"\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "    response = conversation.predict(input=msg)\n",
    "    print(f\"You: {msg}\")\n",
    "    print(f\"AI: {response[:80]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c20b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Check the summary\n",
    "\n",
    "print(\"üìù Conversation Summary:\")\n",
    "print(summary_memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7fddfa",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Summary memory** compresses old messages\n",
    "- **Saves tokens** in long conversations\n",
    "- **Retains key information**\n",
    "- Best for **cost optimization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5241bc",
   "metadata": {},
   "source": [
    "## üîç Part 7: LangSmith - Debugging & Tracing\n",
    "\n",
    "See what happens inside your chains!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Enable LangSmith tracing\n",
    "\n",
    "# If you set the env vars in Part 1, tracing is already on!\n",
    "# Otherwise:\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your-key\"\n",
    "\n",
    "print(\"‚úÖ LangSmith tracing enabled!\")\n",
    "print(\"\\nüí° All LLM calls will be traced\")\n",
    "print(\"View at: https://smith.langchain.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Create a chain with multiple steps\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Multi-step chain\n",
    "joke_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "explain_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain why this joke is funny: {joke}\"\n",
    ")\n",
    "\n",
    "# Chain: topic ‚Üí joke ‚Üí explanation\n",
    "joke_chain = joke_prompt | llm\n",
    "explain_chain = explain_prompt | llm\n",
    "\n",
    "full_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | {\"joke\": joke_chain}\n",
    "    | explain_chain\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-step chain created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f8e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Run chain (will be traced in LangSmith)\n",
    "\n",
    "result = full_chain.invoke(\"programming\")\n",
    "\n",
    "print(\"ü§ñ Result:\")\n",
    "print(result.content)\n",
    "\n",
    "print(\"\\n‚úÖ Check LangSmith dashboard to see:\")\n",
    "print(\"  ‚Ä¢ Each step of the chain\")\n",
    "print(\"  ‚Ä¢ Input/output at each stage\")\n",
    "print(\"  ‚Ä¢ Token counts\")\n",
    "print(\"  ‚Ä¢ Latency\")\n",
    "print(\"\\nLink: https://smith.langchain.com\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee23f1",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **LangSmith** traces every step\n",
    "- **Debug** complex chains easily\n",
    "- **Monitor** production apps\n",
    "- **Free tier** available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d68f36",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Part 8: Production-Ready Chain with Everything\n",
    "\n",
    "Combine structured outputs, memory, and tracing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Define structured output for chatbot\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    \"\"\"Chatbot response with metadata\"\"\"\n",
    "    message: str = Field(description=\"Response message\")\n",
    "    sentiment: str = Field(description=\"Sentiment: positive, neutral, or negative\")\n",
    "    topics: List[str] = Field(description=\"Main topics discussed\")\n",
    "\n",
    "print(\"‚úÖ Response schema defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Create production chain\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=ChatResponse)\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant. \n",
    "Respond to the user and analyze the conversation.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Recent conversation:\n",
    "{chat_history}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Use window memory\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3,\n",
    "    return_messages=True,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Production chain ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Create chat function\n",
    "\n",
    "def chat(user_input):\n",
    "    \"\"\"Chat with structured output and memory\"\"\"\n",
    "    \n",
    "    # Get chat history\n",
    "    history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "    \n",
    "    # Run chain\n",
    "    chain = prompt | llm | parser\n",
    "    result = chain.invoke({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": history\n",
    "    })\n",
    "    \n",
    "    # Save to memory\n",
    "    memory.save_context(\n",
    "        {\"input\": user_input},\n",
    "        {\"output\": result[\"message\"]}\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Chat function created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Test production chatbot\n",
    "\n",
    "print(\"üí¨ Testing Production Chatbot:\\n\")\n",
    "\n",
    "messages = [\n",
    "    \"I just got accepted to my dream university!\",\n",
    "    \"But I'm worried about the tuition costs.\",\n",
    "    \"What should I do?\"\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "    response = chat(msg)\n",
    "    \n",
    "    print(f\"You: {msg}\")\n",
    "    print(f\"AI: {response['message']}\")\n",
    "    print(f\"üìä Sentiment: {response['sentiment']}\")\n",
    "    print(f\"üè∑Ô∏è  Topics: {', '.join(response['topics'])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d15c5",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "- **Structured output** for analysis\n",
    "- **Memory** for context\n",
    "- **LangSmith** traces everything\n",
    "- **Production-ready** pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec8037",
   "metadata": {},
   "source": [
    "## üéØ Challenge Time!\n",
    "\n",
    "### üèÜ Beginner Challenge:\n",
    "\n",
    "**Task:** Build a smart email classifier!\n",
    "\n",
    "**Requirements:**\n",
    "1. Define Pydantic schema:\n",
    "   - category: str (work, personal, spam)\n",
    "   - priority: str (high, medium, low)\n",
    "   - summary: str (brief summary)\n",
    "   - action_needed: bool\n",
    "\n",
    "2. Create chain with structured output\n",
    "\n",
    "3. Test with sample emails\n",
    "\n",
    "4. Add window memory (bonus)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Email: \"Meeting tomorrow at 2pm about project deadline\"\n",
    "Output: {\n",
    "  category: \"work\",\n",
    "  priority: \"high\",\n",
    "  summary: \"Project deadline meeting\",\n",
    "  action_needed: true\n",
    "}\n",
    "```\n",
    "\n",
    "**Try it yourself!** üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b3fd3",
   "metadata": {},
   "source": [
    "## üìö Summary: What We Learned\n",
    "\n",
    "### ‚úÖ Key Concepts:\n",
    "\n",
    "**1. Structured Outputs:**\n",
    "- Pydantic schemas define exact format\n",
    "- JsonOutputParser enforces structure\n",
    "- Reliable, type-safe data\n",
    "- Production-ready\n",
    "\n",
    "**2. Pydantic Benefits:**\n",
    "- Field validation\n",
    "- Type checking\n",
    "- Nested structures\n",
    "- List handling\n",
    "\n",
    "**3. Advanced Memory:**\n",
    "- WindowMemory: Recent context only\n",
    "- SummaryMemory: Compress old messages\n",
    "- Choose based on use case\n",
    "- Optimize token usage\n",
    "\n",
    "**4. LangSmith:**\n",
    "- Trace every LLM call\n",
    "- Debug complex chains\n",
    "- Monitor production\n",
    "- Optimize performance\n",
    "\n",
    "**5. Production Patterns:**\n",
    "- Combine all features\n",
    "- Structured + Memory + Tracing\n",
    "- Reliable, maintainable apps\n",
    "- Ready to deploy\n",
    "\n",
    "### üéØ Key Takeaways:\n",
    "\n",
    "1Ô∏è‚É£ Structured outputs solve the unpredictability problem\n",
    "\n",
    "2Ô∏è‚É£ Different memory types for different needs\n",
    "\n",
    "3Ô∏è‚É£ LangSmith is essential for production\n",
    "\n",
    "4Ô∏è‚É£ These patterns make apps reliable\n",
    "\n",
    "5Ô∏è‚É£ You're ready to build production LLM apps!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "- **Practice:** Build your own structured applications\n",
    "- **Experiment:** Try different memory types\n",
    "- **Explore:** LangSmith dashboard features\n",
    "- **Build:** Production-ready chatbots\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Resources:\n",
    "\n",
    "- **Pydantic Docs:** https://docs.pydantic.dev/\n",
    "- **LangChain Memory:** https://python.langchain.com/docs/modules/memory/\n",
    "- **LangSmith:** https://smith.langchain.com\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You've learned advanced LangChain features! üéâ**\n",
    "\n",
    "**You're now ready to build production-ready LLM applications!** üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
