{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Week 15 - Day 1: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## Today's Goals:\n",
    "‚úÖ Understand RAG concepts and architecture\n",
    "\n",
    "‚úÖ Learn about embeddings and vector stores\n",
    "\n",
    "‚úÖ Build a complete RAG system with LangChain\n",
    "\n",
    "‚úÖ Query your own documents using natural language\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Part 1: Setup - Install & Import All Libraries\n",
    "\n",
    "**IMPORTANT:** Run ALL cells in this part sequentially!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing packages... (this may take 1-2 minutes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.20 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 1.1.0 which is incompatible.\n",
      "langchain 0.1.20 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.4.49 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 1.1.0 which is incompatible.\n",
      "langchain-community 0.0.38 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.4.49 which is incompatible.\n",
      "langchain-huggingface 0.0.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 1.1.0 which is incompatible.\n",
      "langchain-openai 0.1.7 requires langchain-core<0.3,>=0.1.46, but you have langchain-core 1.1.0 which is incompatible.\n",
      "langchain-text-splitters 0.0.2 requires langchain-core<0.3,>=0.1.28, but you have langchain-core 1.1.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Install required packages\n",
    "print(\"üì¶ Installing packages... (this may take 1-2 minutes)\\n\")\n",
    "\n",
    "!pip install -q langchain langchain-community langchain-huggingface\n",
    "!pip install -q faiss-cpu sentence-transformers\n",
    "!pip install -q langchain-groq python-dotenv\n",
    "!pip install -q chromadb tiktoken\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# LangChain Core\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter, CharacterTextSplitter\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Document Loaders\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextLoader, DirectoryLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain\\schema\\__init__.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseDocumentTransformer, Document\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangChainException, OutputParserException\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMemory\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     AIMessage,\n\u001b[32m     10\u001b[39m     BaseMessage,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     messages_to_dict,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m message_to_dict\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core.memory'"
     ]
    }
   ],
   "source": [
    "# STEP 2: Import ALL libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain Core\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Document Loaders\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "# Embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Vector Stores\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "\n",
    "# LLM\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Chains\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Set up API Key for Groq (Free LLM API)\n",
    "# Get your free API key from: https://console.groq.com/\n",
    "\n",
    "# Option 1: Set directly (for learning - don't do this in production!)\n",
    "GROQ_API_KEY = \"your-groq-api-key-here\"  # Replace with your actual key\n",
    "\n",
    "# Option 2: Use environment variable (recommended)\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "print(\"‚úÖ API key configured!\")\n",
    "print(\"\\nüí° Get a FREE Groq API key at: https://console.groq.com/\")\n",
    "print(\"üöÄ Ready to build RAG!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Part 2: Understanding the RAG Pipeline\n",
    "\n",
    "Before we code, let's understand what we're building!\n",
    "\n",
    "### üéØ What is RAG?\n",
    "\n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "It's a technique that makes LLMs smarter by:\n",
    "1. **Retrieving** relevant information from your documents\n",
    "2. **Augmenting** the LLM's prompt with that information\n",
    "3. **Generating** an answer using both its knowledge AND your data\n",
    "\n",
    "### üîÑ The RAG Pipeline:\n",
    "\n",
    "```\n",
    "üìÑ Documents ‚Üí ‚úÇÔ∏è Chunks ‚Üí üî¢ Embeddings ‚Üí üíæ Vector Store\n",
    "                                                    ‚Üì\n",
    "üí¨ Query ‚Üí üî¢ Embed Query ‚Üí üîç Search ‚Üí üì• Get Top-K ‚Üí ü§ñ LLM ‚Üí ‚úÖ Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÑ Part 3: Creating Sample Documents\n",
    "\n",
    "Let's create some sample documents to work with. In a real scenario, you'd load PDFs, Word docs, or text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents about a fictional company\n",
    "# In real projects, you'd load actual files!\n",
    "\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Employee Handbook - Chapter 1: Company Overview\n",
    "        \n",
    "        TechCorp was founded in 2015 by Sarah Chen and Michael Rodriguez. \n",
    "        Our headquarters is located in San Francisco, California. \n",
    "        We have over 500 employees across 3 offices: San Francisco, New York, and London.\n",
    "        \n",
    "        Our mission is to make AI accessible to everyone through innovative products.\n",
    "        Our core values are: Innovation, Integrity, Inclusivity, and Impact.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"employee_handbook.pdf\", \"chapter\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Employee Handbook - Chapter 2: Leave Policy\n",
    "        \n",
    "        Annual Leave: All employees receive 20 days of paid annual leave per year.\n",
    "        Sick Leave: Employees can take up to 10 days of paid sick leave annually.\n",
    "        Parental Leave: New parents receive 16 weeks of paid parental leave.\n",
    "        \n",
    "        To request leave, submit a request through the HR portal at least 2 weeks in advance.\n",
    "        Emergency leave can be requested by emailing hr@techcorp.com.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"employee_handbook.pdf\", \"chapter\": 2}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Employee Handbook - Chapter 3: Remote Work Policy\n",
    "        \n",
    "        TechCorp supports hybrid work arrangements. Employees can work remotely \n",
    "        up to 3 days per week. Core hours are 10 AM to 4 PM in your local timezone.\n",
    "        \n",
    "        To set up remote work:\n",
    "        1. Get approval from your manager\n",
    "        2. Ensure you have reliable internet (minimum 50 Mbps)\n",
    "        3. Set up your home office following our ergonomics guide\n",
    "        4. Install the company VPN for secure access\n",
    "        \n",
    "        Remote employees must attend in-person meetings when required.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"employee_handbook.pdf\", \"chapter\": 3}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp IT Support Guide - Password Reset\n",
    "        \n",
    "        To reset your password:\n",
    "        1. Go to portal.techcorp.com/reset\n",
    "        2. Enter your employee ID and registered email\n",
    "        3. Click 'Send Reset Link'\n",
    "        4. Check your email for the reset link (valid for 24 hours)\n",
    "        5. Create a new password following our security requirements:\n",
    "           - Minimum 12 characters\n",
    "           - At least one uppercase letter\n",
    "           - At least one number\n",
    "           - At least one special character\n",
    "        \n",
    "        If you can't access your email, contact IT support at it-help@techcorp.com\n",
    "        or call the helpdesk at extension 5555.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"it_guide.pdf\", \"topic\": \"password\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Benefits Summary - Health Insurance\n",
    "        \n",
    "        All full-time employees are eligible for comprehensive health insurance.\n",
    "        \n",
    "        Plans offered:\n",
    "        - Basic Plan: $0 monthly premium, $2000 deductible\n",
    "        - Standard Plan: $50 monthly premium, $1000 deductible\n",
    "        - Premium Plan: $150 monthly premium, $500 deductible\n",
    "        \n",
    "        Dental and vision coverage is included in all plans.\n",
    "        Family coverage is available at additional cost.\n",
    "        \n",
    "        Enrollment period is January 1-31 each year.\n",
    "        New employees can enroll within 30 days of their start date.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"benefits_guide.pdf\", \"topic\": \"health\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"üìö Created {len(sample_documents)} sample documents!\")\n",
    "print(\"\\nüìÑ Documents created:\")\n",
    "for i, doc in enumerate(sample_documents, 1):\n",
    "    print(f\"   {i}. {doc.metadata.get('source', 'unknown')} - {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÇÔ∏è Part 4: Text Splitting (Chunking)\n",
    "\n",
    "Documents need to be split into smaller chunks for effective retrieval.\n",
    "\n",
    "### Why Chunking?\n",
    "- LLMs have context limits\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Better for finding specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum characters per chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks (preserves context)\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split priorities\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "chunks = text_splitter.split_documents(sample_documents)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Split {len(sample_documents)} documents into {len(chunks)} chunks!\")\n",
    "print(\"\\nüìä Chunk Statistics:\")\n",
    "print(f\"   Average chunk size: {sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\n",
    "print(f\"   Smallest chunk: {min(len(c.page_content) for c in chunks)} characters\")\n",
    "print(f\"   Largest chunk: {max(len(c.page_content) for c in chunks)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a few chunks\n",
    "print(\"üìã Sample Chunks:\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"\\nüîπ Chunk {i}:\")\n",
    "    print(f\"   Source: {chunk.metadata.get('source', 'unknown')}\")\n",
    "    print(f\"   Length: {len(chunk.page_content)} chars\")\n",
    "    print(f\"   Content: {chunk.page_content[:150]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "\n",
    "‚úÖ **RecursiveCharacterTextSplitter** is smart - it tries to keep paragraphs together\n",
    "\n",
    "‚úÖ **Chunk overlap** ensures context isn't lost at boundaries\n",
    "\n",
    "‚úÖ **Metadata is preserved** - we can track which document each chunk came from\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Part 5: Creating Embeddings\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning.\n",
    "\n",
    "Similar meanings = Similar vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding model\n",
    "# We use a free, open-source model from HuggingFace\n",
    "print(\"‚è≥ Loading embedding model... (first time may take 1-2 minutes)\\n\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Small, fast, and effective!\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what an embedding looks like!\n",
    "sample_text = \"How do I reset my password?\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "print(f\"üìù Text: '{sample_text}'\")\n",
    "print(f\"\\nüî¢ Embedding (first 10 values):\")\n",
    "print(f\"   {sample_embedding[:10]}\")\n",
    "print(f\"\\nüìä Embedding dimensions: {len(sample_embedding)}\")\n",
    "print(\"\\nüí° This 384-dimensional vector captures the 'meaning' of the text!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate semantic similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Three sentences to compare\n",
    "sentences = [\n",
    "    \"How do I reset my password?\",\n",
    "    \"I forgot my login credentials\",  # Similar meaning!\n",
    "    \"What is the weather today?\"       # Different meaning!\n",
    "]\n",
    "\n",
    "# Get embeddings for all\n",
    "embeddings_list = [embeddings.embed_query(s) for s in sentences]\n",
    "\n",
    "print(\"üîç Semantic Similarity Demo:\\n\")\n",
    "print(f\"Sentence 1: '{sentences[0]}'\")\n",
    "print(f\"Sentence 2: '{sentences[1]}'\")\n",
    "print(f\"Sentence 3: '{sentences[2]}'\")\n",
    "\n",
    "sim_1_2 = cosine_similarity(embeddings_list[0], embeddings_list[1])\n",
    "sim_1_3 = cosine_similarity(embeddings_list[0], embeddings_list[2])\n",
    "\n",
    "print(f\"\\nüìä Similarity Scores:\")\n",
    "print(f\"   Sentence 1 ‚Üî Sentence 2: {sim_1_2:.4f} (Similar meaning!)\")\n",
    "print(f\"   Sentence 1 ‚Üî Sentence 3: {sim_1_3:.4f} (Different meaning!)\")\n",
    "print(\"\\n‚úÖ Higher score = More similar meaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "\n",
    "‚úÖ **Embeddings capture meaning**, not just keywords\n",
    "\n",
    "‚úÖ **\"Password\" and \"credentials\"** are recognized as similar concepts\n",
    "\n",
    "‚úÖ **Cosine similarity** measures how aligned two vectors are (0 to 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Part 6: Creating a Vector Store\n",
    "\n",
    "A vector store is a specialized database for storing and searching embeddings.\n",
    "\n",
    "We'll use **FAISS** - Facebook's fast similarity search library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS vector store from our chunks\n",
    "print(\"‚è≥ Creating vector store...\\n\")\n",
    "\n",
    "# This automatically:\n",
    "# 1. Converts all chunks to embeddings\n",
    "# 2. Stores them in FAISS index\n",
    "# 3. Keeps track of metadata\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store created successfully!\")\n",
    "print(f\"üìä Indexed {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the retrieval!\n",
    "query = \"How many days of annual leave do I get?\"\n",
    "\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Search for similar documents\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"\\nüì• Top 3 Retrieved Chunks:\\n\")\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nüîπ Result {i} (Similarity: {1-score:.4f}):\")  # FAISS returns distance, not similarity\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insights:\n",
    "\n",
    "‚úÖ **FAISS.from_documents()** handles embedding creation automatically\n",
    "\n",
    "‚úÖ **similarity_search_with_score()** returns both documents AND similarity scores\n",
    "\n",
    "‚úÖ The most relevant chunks about \"leave policy\" are retrieved!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Part 7: Building the Complete RAG Chain\n",
    "\n",
    "Now let's connect everything: Vector Store + LLM = RAG!\n",
    "\n",
    "We'll use **Groq** for fast, free LLM inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Fast and capable!\n",
    "    temperature=0,  # Deterministic outputs\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized!\")\n",
    "print(\"üìä Model: Llama 3.1 8B (via Groq)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom prompt template for RAG\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant for TechCorp employees. \n",
    "Answer the question based ONLY on the following context.\n",
    "If you don't know the answer based on the context, say \"I don't have that information in my knowledge base.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt template created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever from our vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most similar chunks\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever created!\")\n",
    "print(\"üìä Will retrieve top 3 most relevant chunks for each query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" = put all retrieved docs into prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,  # Also return the source chunks!\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Chain built successfully!\")\n",
    "print(\"\\nüéâ Your RAG system is ready to answer questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí¨ Part 8: Testing Your RAG System!\n",
    "\n",
    "Let's ask some questions and see RAG in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    \"\"\"Ask a question and display the answer with sources\"\"\"\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get the answer\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"\\n‚úÖ Answer:\\n{result['result']}\")\n",
    "    \n",
    "    print(f\"\\nüìö Sources Used:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"   {i}. {doc.metadata.get('source', 'unknown')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Leave Policy\n",
    "result1 = ask_question(\"How many days of annual leave do employees get?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Password Reset\n",
    "result2 = ask_question(\"How do I reset my password?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Remote Work\n",
    "result3 = ask_question(\"Can I work from home? What are the requirements?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4: Health Insurance\n",
    "result4 = ask_question(\"What health insurance plans are available?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5: Test with question NOT in documents\n",
    "result5 = ask_question(\"What is the company's stock price?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Observations:\n",
    "\n",
    "‚úÖ **Accurate answers** - The system pulls information directly from the documents\n",
    "\n",
    "‚úÖ **Source attribution** - We can see which documents were used\n",
    "\n",
    "‚úÖ **Handles unknown queries** - When information isn't in the documents, it says so!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 9: Mini Challenge\n",
    "\n",
    "### üèÜ Challenge Tasks:\n",
    "\n",
    "**Your Mission:**\n",
    "1. Create your own document about a topic you're interested in\n",
    "2. Add it to the vector store\n",
    "3. Ask questions about your new document!\n",
    "\n",
    "**Hints:**\n",
    "```python\n",
    "# Create a new document\n",
    "my_doc = Document(\n",
    "    page_content=\"Your content here...\",\n",
    "    metadata={\"source\": \"my_document.txt\"}\n",
    ")\n",
    "\n",
    "# Add to vector store\n",
    "vectorstore.add_documents([my_doc])\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Your RAG system should now answer questions about your new content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Try adding your own document and asking questions about it\n",
    "\n",
    "# Step 1: Create your document\n",
    "# my_doc = Document(...)\n",
    "\n",
    "# Step 2: Add to vector store\n",
    "# vectorstore.add_documents([my_doc])\n",
    "\n",
    "# Step 3: Ask questions!\n",
    "# ask_question(\"Your question about your document\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Summary - What We Learned Today\n",
    "\n",
    "### 1. RAG Fundamentals üéØ\n",
    "- RAG = Retrieval-Augmented Generation\n",
    "- Combines search with LLM generation\n",
    "- Grounds LLM answers in your actual data\n",
    "\n",
    "### 2. Document Processing ‚úÇÔ∏è\n",
    "- Split documents into manageable chunks\n",
    "- Use overlap to preserve context\n",
    "- Keep metadata for source tracking\n",
    "\n",
    "### 3. Embeddings üî¢\n",
    "- Convert text to numerical vectors\n",
    "- Similar meanings = similar vectors\n",
    "- Enable semantic search (not just keyword matching)\n",
    "\n",
    "### 4. Vector Stores üíæ\n",
    "- FAISS for fast similarity search\n",
    "- Index documents for quick retrieval\n",
    "- Return most relevant chunks\n",
    "\n",
    "### 5. RAG Chain üîó\n",
    "- Connect retriever + LLM\n",
    "- Custom prompts guide the LLM\n",
    "- Return answers with sources\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "‚úÖ **RAG makes LLMs more accurate** by grounding answers in your data\n",
    "\n",
    "‚úÖ **Chunk size matters** - experiment to find what works best\n",
    "\n",
    "‚úÖ **Good prompts are crucial** - tell the LLM to use ONLY the context\n",
    "\n",
    "‚úÖ **Source attribution** builds trust in AI answers\n",
    "\n",
    "‚úÖ **LangChain simplifies** the entire RAG pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips\n",
    "\n",
    "1. **Start with quality data** - Clean, well-structured documents work best\n",
    "2. **Tune chunk size** - Try 500-1000 characters to start\n",
    "3. **Use metadata** - Track sources for debugging and citations\n",
    "4. **Test retrieval first** - Make sure the right chunks are being found\n",
    "5. **Iterate on prompts** - The prompt template greatly affects output quality\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps - Tomorrow!\n",
    "\n",
    "**Day 2: Advanced RAG**\n",
    "- Hybrid retrieval (keyword + semantic)\n",
    "- Re-ranking strategies\n",
    "- Handling large document collections\n",
    "- Production-ready optimizations\n",
    "\n",
    "**Get ready to take your RAG skills to the next level! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've built your first RAG system!\n",
    "\n",
    "You now know how to:\n",
    "- ‚úÖ Process documents into chunks\n",
    "- ‚úÖ Create embeddings for semantic search\n",
    "- ‚úÖ Store and search vectors with FAISS\n",
    "- ‚úÖ Build a complete RAG chain with LangChain\n",
    "- ‚úÖ Answer questions from your own documents!\n",
    "\n",
    "**Keep practicing and see you tomorrow! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
