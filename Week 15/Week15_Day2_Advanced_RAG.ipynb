{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Week 15 - Day 2: Advanced RAG Techniques\n",
    "\n",
    "## Today's Goals:\n",
    "‚úÖ Implement **Hybrid Search** (BM25 + Semantic)\n",
    "\n",
    "‚úÖ Add **Re-ranking** for improved precision\n",
    "\n",
    "‚úÖ Use **Multi-Query Retrieval** for better coverage\n",
    "\n",
    "‚úÖ Build **Parent Document Retriever** for hierarchical context\n",
    "\n",
    "‚úÖ Compare basic vs advanced RAG performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Part 1: Setup - Install & Import All Libraries\n",
    "\n",
    "**IMPORTANT:** Run ALL cells in this part before continuing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing packages... (this may take 2-3 minutes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-groq 1.1.0 requires langchain-core<2.0.0,>=1.1.0, but you have langchain-core 0.1.53 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.20 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 1.1.0 which is incompatible.\n",
      "langchain 0.1.20 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.4.49 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 1.1.0 which is incompatible.\n",
      "langchain-community 0.0.38 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.4.49 which is incompatible.\n",
      "langchain-huggingface 0.0.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 1.1.0 which is incompatible.\n",
      "langchain-openai 0.1.7 requires langchain-core<0.3,>=0.1.46, but you have langchain-core 1.1.0 which is incompatible.\n",
      "langchain-text-splitters 0.0.2 requires langchain-core<0.3,>=0.1.28, but you have langchain-core 1.1.0 which is incompatible.\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Install required packages (including new ones for advanced RAG)\n",
    "print(\"üì¶ Installing packages... (this may take 2-3 minutes)\\n\")\n",
    "\n",
    "!pip install -q langchain langchain-community langchain-huggingface\n",
    "!pip install -q faiss-cpu sentence-transformers\n",
    "!pip install -q langchain-groq python-dotenv\n",
    "!pip install -q rank-bm25  # For BM25 keyword search\n",
    "!pip install -q chromadb tiktoken\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Core\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Document Loaders\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages\\langchain\\schema\\__init__.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseDocumentTransformer, Document\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LangChainException, OutputParserException\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMemory\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     AIMessage,\n\u001b[32m     10\u001b[39m     BaseMessage,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     messages_to_dict,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m message_to_dict\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core.memory'"
     ]
    }
   ],
   "source": [
    "# STEP 2: Import ALL libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Document Loaders\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Vector Stores\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Retrievers - NEW for Day 2!\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Re-ranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# LLM\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Chains\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# BM25 for keyword search\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Set up API Key\n",
    "GROQ_API_KEY = \"your-groq-api-key-here\"  # Replace with your actual key\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "print(\"‚úÖ API key configured!\")\n",
    "print(\"üí° Get a FREE Groq API key at: https://console.groq.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÑ Part 2: Create Sample Documents\n",
    "\n",
    "We'll use the same TechCorp documents from Day 1, plus some additional ones to test advanced techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Employee Handbook - Chapter 1: Company Overview\n",
    "        \n",
    "        TechCorp was founded in 2015 by Sarah Chen and Michael Rodriguez. \n",
    "        Our headquarters is located in San Francisco, California. \n",
    "        We have over 500 employees across 3 offices: San Francisco, New York, and London.\n",
    "        \n",
    "        Our mission is to make AI accessible to everyone through innovative products.\n",
    "        Our core values are: Innovation, Integrity, Inclusivity, and Impact.\n",
    "        The company ID for TechCorp in the registry is TC-2015-SF.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"employee_handbook.pdf\", \"chapter\": 1, \"topic\": \"company\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Employee Handbook - Chapter 2: Leave Policy\n",
    "        \n",
    "        Annual Leave: All employees receive 20 days of paid annual leave per year.\n",
    "        Sick Leave: Employees can take up to 10 days of paid sick leave annually.\n",
    "        Parental Leave: New parents receive 16 weeks of paid parental leave.\n",
    "        \n",
    "        To request leave, submit a request through the HR portal at least 2 weeks in advance.\n",
    "        Emergency leave can be requested by emailing hr@techcorp.com.\n",
    "        Leave policy reference number: LP-2023-V2.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"employee_handbook.pdf\", \"chapter\": 2, \"topic\": \"leave\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Employee Handbook - Chapter 3: Remote Work Policy\n",
    "        \n",
    "        TechCorp supports hybrid work arrangements. Employees can work remotely \n",
    "        up to 3 days per week. Core hours are 10 AM to 4 PM in your local timezone.\n",
    "        \n",
    "        To set up remote work:\n",
    "        1. Get approval from your manager\n",
    "        2. Ensure you have reliable internet (minimum 50 Mbps)\n",
    "        3. Set up your home office following our ergonomics guide\n",
    "        4. Install the company VPN for secure access\n",
    "        \n",
    "        Remote work policy code: RW-POL-2024.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"employee_handbook.pdf\", \"chapter\": 3, \"topic\": \"remote\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp IT Support Guide - Password Reset\n",
    "        \n",
    "        To reset your password:\n",
    "        1. Go to portal.techcorp.com/reset\n",
    "        2. Enter your employee ID and registered email\n",
    "        3. Click 'Send Reset Link'\n",
    "        4. Check your email for the reset link (valid for 24 hours)\n",
    "        5. Create a new password following our security requirements\n",
    "        \n",
    "        Password requirements: Minimum 12 characters, uppercase, number, special char.\n",
    "        IT ticket category: PWD-RESET-001\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"it_guide.pdf\", \"topic\": \"password\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Benefits Summary - Health Insurance\n",
    "        \n",
    "        All full-time employees are eligible for comprehensive health insurance.\n",
    "        \n",
    "        Plans offered:\n",
    "        - Basic Plan (Plan ID: HI-BASIC-01): $0 monthly premium, $2000 deductible\n",
    "        - Standard Plan (Plan ID: HI-STD-02): $50 monthly premium, $1000 deductible\n",
    "        - Premium Plan (Plan ID: HI-PREM-03): $150 monthly premium, $500 deductible\n",
    "        \n",
    "        Dental and vision coverage is included in all plans.\n",
    "        Enrollment period is January 1-31 each year.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"benefits_guide.pdf\", \"topic\": \"health\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Expense Policy - Travel Reimbursement\n",
    "        \n",
    "        Business travel expenses are reimbursable following these guidelines:\n",
    "        - Flights: Economy class for trips under 6 hours, business class for longer\n",
    "        - Hotels: Up to $200/night in major cities, $150/night elsewhere\n",
    "        - Meals: Up to $75/day per diem\n",
    "        - Ground transport: Actual costs with receipts\n",
    "        \n",
    "        Submit expenses within 30 days through Expensify.\n",
    "        Policy reference: EXP-TRV-2024-V1\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"expense_policy.pdf\", \"topic\": \"travel\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        TechCorp Product Documentation - API Reference\n",
    "        \n",
    "        TechCorp AI API v2.0 Documentation\n",
    "        \n",
    "        Base URL: https://api.techcorp.com/v2/\n",
    "        Authentication: Bearer token in Authorization header\n",
    "        Rate limits: 1000 requests/minute for standard tier\n",
    "        \n",
    "        Endpoints:\n",
    "        - POST /analyze: Submit text for analysis\n",
    "        - GET /results/{id}: Retrieve analysis results\n",
    "        - DELETE /jobs/{id}: Cancel a running job\n",
    "        \n",
    "        Error code API-429: Rate limit exceeded\n",
    "        Error code API-401: Invalid authentication\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"api_docs.pdf\", \"topic\": \"api\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"üìö Created {len(sample_documents)} sample documents!\")\n",
    "print(\"\\nüìÑ Documents:\")\n",
    "for i, doc in enumerate(sample_documents, 1):\n",
    "    print(f\"   {i}. {doc.metadata.get('source')} - {doc.metadata.get('topic', 'general')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter and chunk documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(sample_documents)\n",
    "print(f\"‚úÇÔ∏è Split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings model\n",
    "print(\"‚è≥ Loading embedding model...\\n\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store (basic retriever)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "basic_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "print(\"‚úÖ Basic vector store created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÄ Part 3: Hybrid Search (BM25 + Semantic)\n",
    "\n",
    "Hybrid search combines:\n",
    "- **BM25 (keyword search)**: Great for exact matches, IDs, specific terms\n",
    "- **Semantic search**: Great for meaning, synonyms, context\n",
    "\n",
    "Together = Best of both worlds! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BM25 Retriever for keyword search\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 4  # Return top 4 results\n",
    "\n",
    "print(\"‚úÖ BM25 Retriever created!\")\n",
    "print(\"üìä Uses keyword matching (TF-IDF based)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ensemble Retriever (Hybrid Search!)\n",
    "# This combines BM25 and Vector search with equal weights\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, basic_retriever],\n",
    "    weights=[0.5, 0.5]  # Equal weight to keyword and semantic\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Hybrid (Ensemble) Retriever created!\")\n",
    "print(\"üìä Combines: 50% BM25 + 50% Semantic Search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test: Query with a specific ID (BM25 will help!)\n",
    "query = \"What is policy LP-2023-V2 about?\"\n",
    "\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare: Semantic only vs Hybrid\n",
    "print(\"\\nüìä Semantic Search Results:\")\n",
    "semantic_results = basic_retriever.invoke(query)\n",
    "for i, doc in enumerate(semantic_results[:2], 1):\n",
    "    print(f\"   {i}. {doc.metadata.get('source')} - {doc.page_content[:80]}...\")\n",
    "\n",
    "print(\"\\nüîÄ Hybrid Search Results:\")\n",
    "hybrid_results = hybrid_retriever.invoke(query)\n",
    "for i, doc in enumerate(hybrid_results[:2], 1):\n",
    "    print(f\"   {i}. {doc.metadata.get('source')} - {doc.page_content[:80]}...\")\n",
    "\n",
    "print(\"\\nüí° Notice: Hybrid search finds the exact policy ID better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight:\n",
    "\n",
    "When the query contains specific identifiers (like \"LP-2023-V2\"), BM25 excels at finding exact matches.\n",
    "Semantic search might miss these because embeddings don't capture arbitrary IDs well.\n",
    "\n",
    "**Hybrid = Best coverage!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 4: Re-ranking for Better Precision\n",
    "\n",
    "**Problem**: Initial retrieval returns candidates, but the order might not be optimal.\n",
    "\n",
    "**Solution**: Use a **Cross-Encoder** to re-score and re-order the results.\n",
    "\n",
    "Cross-encoders are more accurate than bi-encoders because they see query AND document together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Cross-Encoder for re-ranking\n",
    "print(\"‚è≥ Loading Cross-Encoder re-ranker...\\n\")\n",
    "\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "print(\"‚úÖ Re-ranker loaded!\")\n",
    "print(\"üìä Model: MS-MARCO MiniLM (trained on search relevance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents(query, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    Re-rank documents using a cross-encoder.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        documents: List of retrieved documents\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        Re-ranked documents (best first)\n",
    "    \"\"\"\n",
    "    # Create query-document pairs for scoring\n",
    "    pairs = [[query, doc.page_content] for doc in documents]\n",
    "    \n",
    "    # Get relevance scores from cross-encoder\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Sort documents by score (descending)\n",
    "    scored_docs = list(zip(documents, scores))\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top-k documents\n",
    "    return [(doc, score) for doc, score in scored_docs[:top_k]]\n",
    "\n",
    "print(\"‚úÖ Re-ranking function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test re-ranking\n",
    "query = \"How many vacation days do I get per year?\"\n",
    "\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Initial retrieval (get more candidates)\n",
    "initial_results = hybrid_retriever.invoke(query)\n",
    "\n",
    "print(\"üì• Initial Retrieval (Top 4):\")\n",
    "for i, doc in enumerate(initial_results[:4], 1):\n",
    "    print(f\"   {i}. [{doc.metadata.get('topic', '?')}] {doc.page_content[:60]}...\")\n",
    "\n",
    "# Step 2: Re-rank\n",
    "reranked = rerank_documents(query, initial_results, top_k=3)\n",
    "\n",
    "print(\"\\nüéØ After Re-ranking (Top 3):\")\n",
    "for i, (doc, score) in enumerate(reranked, 1):\n",
    "    print(f\"   {i}. [Score: {score:.3f}] [{doc.metadata.get('topic', '?')}] {doc.page_content[:50]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Re-ranking improved result ordering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight:\n",
    "\n",
    "The cross-encoder gives a **relevance score** for each query-document pair.\n",
    "This helps push the most relevant documents to the top!\n",
    "\n",
    "**Best Practice**: Retrieve 20-50 candidates, re-rank to top 3-5.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Part 5: Multi-Query Retrieval\n",
    "\n",
    "**Problem**: User queries are often vague or incomplete.\n",
    "\n",
    "**Solution**: Use an LLM to generate multiple variations of the query, then retrieve for all of them!\n",
    "\n",
    "This increases the chance of finding relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM for query generation\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.3,  # Slightly creative for diverse queries\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized for multi-query generation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Multi-Query Retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=basic_retriever,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-Query Retriever created!\")\n",
    "print(\"üìä Will generate multiple query variations automatically\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-query retrieval\n",
    "import logging\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.DEBUG)\n",
    "\n",
    "query = \"time off\"\n",
    "\n",
    "print(f\"üîç Original Query: '{query}'\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# This will generate multiple queries and retrieve for all\n",
    "results = multi_query_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\nüì• Retrieved {len(results)} unique documents!\")\n",
    "print(\"\\nüîπ Sample Results:\")\n",
    "for i, doc in enumerate(results[:3], 1):\n",
    "    print(f\"   {i}. [{doc.metadata.get('topic', '?')}] {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Key Insight:\n",
    "\n",
    "The LLM automatically generated queries like:\n",
    "- \"vacation policy\"\n",
    "- \"annual leave days\"\n",
    "- \"PTO policy\"\n",
    "\n",
    "This catches more relevant documents than just searching \"time off\"!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≥ Part 6: Parent Document Retriever\n",
    "\n",
    "**Problem**: Small chunks = precise search, but limited context for LLM.\n",
    "\n",
    "**Solution**: \n",
    "- **Search** on small child chunks (precise matching)\n",
    "- **Return** larger parent chunks (rich context)\n",
    "\n",
    "Best of both worlds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitters for parent and child\n",
    "parent_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,   # Larger chunks for context\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,   # Smaller chunks for precise search\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Text splitters created!\")\n",
    "print(\"üìä Parent: 800 chars | Child: 200 chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vector store for parent-child retrieval\n",
    "parent_vectorstore = FAISS.from_documents(\n",
    "    child_splitter.split_documents(sample_documents),\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# In-memory store for parent documents\n",
    "docstore = InMemoryStore()\n",
    "\n",
    "# Create Parent Document Retriever\n",
    "parent_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=parent_vectorstore,\n",
    "    docstore=docstore,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "parent_retriever.add_documents(sample_documents)\n",
    "\n",
    "print(\"‚úÖ Parent Document Retriever created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Parent Document Retriever\n",
    "query = \"password reset steps\"\n",
    "\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Normal retrieval (small chunks)\n",
    "normal_results = basic_retriever.invoke(query)\n",
    "print(\"üìÑ Normal Retrieval (small chunks):\")\n",
    "print(f\"   Chunk length: {len(normal_results[0].page_content)} chars\")\n",
    "print(f\"   Content: {normal_results[0].page_content[:100]}...\")\n",
    "\n",
    "# Parent document retrieval (returns larger context)\n",
    "parent_results = parent_retriever.invoke(query)\n",
    "print(\"\\nüå≥ Parent Document Retrieval (full context):\")\n",
    "print(f\"   Chunk length: {len(parent_results[0].page_content)} chars\")\n",
    "print(f\"   Content: {parent_results[0].page_content[:200]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Parent retriever returns more context for the LLM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Part 7: Build Complete Advanced RAG Chain\n",
    "\n",
    "Now let's put it all together into a production-ready RAG system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    \"\"\"\n",
    "    Advanced RAG system with:\n",
    "    - Hybrid search (BM25 + Semantic)\n",
    "    - Re-ranking with cross-encoder\n",
    "    - Customizable retrieval\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents, embeddings, llm, reranker):\n",
    "        # Text splitting\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "        self.chunks = splitter.split_documents(documents)\n",
    "        \n",
    "        # Create retrievers\n",
    "        self.vectorstore = FAISS.from_documents(self.chunks, embeddings)\n",
    "        self.semantic_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        self.bm25_retriever = BM25Retriever.from_documents(self.chunks)\n",
    "        self.bm25_retriever.k = 10\n",
    "        \n",
    "        # Hybrid retriever\n",
    "        self.hybrid_retriever = EnsembleRetriever(\n",
    "            retrievers=[self.bm25_retriever, self.semantic_retriever],\n",
    "            weights=[0.4, 0.6]  # Slightly favor semantic\n",
    "        )\n",
    "        \n",
    "        # Re-ranker and LLM\n",
    "        self.reranker = reranker\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Prompt template\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "You are a helpful assistant. Answer the question based ONLY on the following context.\n",
    "If you cannot find the answer in the context, say \"I don't have that information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Advanced RAG system initialized!\")\n",
    "    \n",
    "    def retrieve_and_rerank(self, query, top_k=3):\n",
    "        \"\"\"Retrieve documents and rerank them\"\"\"\n",
    "        # Step 1: Hybrid retrieval\n",
    "        docs = self.hybrid_retriever.invoke(query)\n",
    "        \n",
    "        # Step 2: Re-rank\n",
    "        pairs = [[query, doc.page_content] for doc in docs]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Sort by score\n",
    "        scored = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return [doc for doc, score in scored[:top_k]]\n",
    "    \n",
    "    def answer(self, question):\n",
    "        \"\"\"Get answer using advanced RAG\"\"\"\n",
    "        # Retrieve and rerank\n",
    "        docs = self.retrieve_and_rerank(question, top_k=3)\n",
    "        \n",
    "        # Combine context\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = self.prompt.format(context=context, question=question)\n",
    "        response = self.llm.invoke(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"sources\": [doc.metadata.get('source', 'unknown') for doc in docs]\n",
    "        }\n",
    "\n",
    "# Create Advanced RAG system\n",
    "advanced_rag = AdvancedRAG(\n",
    "    documents=sample_documents,\n",
    "    embeddings=embeddings,\n",
    "    llm=llm,\n",
    "    reranker=reranker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Advanced RAG system!\n",
    "def ask_advanced(question):\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = advanced_rag.answer(question)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Answer:\\n{result['answer']}\")\n",
    "    print(f\"\\nüìö Sources: {', '.join(result['sources'])}\")\n",
    "    print(\"=\" * 60)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with various queries\n",
    "ask_advanced(\"What is policy LP-2023-V2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_advanced(\"How do I reset my password and what are the requirements?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_advanced(\"What health insurance options are available and their costs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_advanced(\"Tell me about the API rate limits and error codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 8: Mini Challenge\n",
    "\n",
    "### üèÜ Challenge: Customize Your Advanced RAG\n",
    "\n",
    "**Your Tasks:**\n",
    "1. Add a new document about a topic of your choice\n",
    "2. Experiment with different hybrid search weights\n",
    "3. Compare results with basic vs advanced RAG\n",
    "\n",
    "**Hints:**\n",
    "```python\n",
    "# Try different weights:\n",
    "# More keyword focus: weights=[0.7, 0.3]\n",
    "# More semantic focus: weights=[0.3, 0.7]\n",
    "```\n",
    "\n",
    "**Bonus:** Try implementing query expansion manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Try customizing the Advanced RAG system\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Summary - What We Learned Today\n",
    "\n",
    "### 1. Hybrid Search üîÄ\n",
    "- Combines BM25 (keyword) + Semantic (embedding) search\n",
    "- Best for diverse query types\n",
    "- Use `EnsembleRetriever` in LangChain\n",
    "\n",
    "### 2. Re-ranking üéØ\n",
    "- Cross-encoders provide more accurate relevance scores\n",
    "- Retrieve more, re-rank to top-k\n",
    "- Improves precision significantly\n",
    "\n",
    "### 3. Multi-Query Retrieval üîÑ\n",
    "- LLM generates query variations\n",
    "- Increases recall (finds more relevant docs)\n",
    "- Great for vague or ambiguous queries\n",
    "\n",
    "### 4. Parent Document Retriever üå≥\n",
    "- Search on small chunks, return large context\n",
    "- Best of both: precise search + rich context\n",
    "- Use for complex documents\n",
    "\n",
    "### 5. Scaling Strategies üìà\n",
    "- Metadata filtering reduces search space\n",
    "- ANN algorithms for large datasets\n",
    "- Choose right vector DB for your scale\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "‚úÖ **Hybrid search** handles both exact matches and semantic similarity\n",
    "\n",
    "‚úÖ **Re-ranking** improves result quality with minimal latency cost\n",
    "\n",
    "‚úÖ **Query transformation** helps with vague user inputs\n",
    "\n",
    "‚úÖ **Start simple, add complexity as needed**\n",
    "\n",
    "‚úÖ **Always evaluate** - measure retrieval AND generation quality\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips\n",
    "\n",
    "1. **Hybrid weights** depend on your use case - experiment!\n",
    "2. **Re-rank top 20-50** candidates, return top 3-5\n",
    "3. **Use metadata filtering** before vector search for large datasets\n",
    "4. **Monitor latency** - re-ranking adds overhead\n",
    "5. **Test with real queries** from your users\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Day 3: Communication Skills**\n",
    "- Presenting AI projects to stakeholders\n",
    "- Technical storytelling\n",
    "\n",
    "**Week Project:**\n",
    "- Build a chatbot that answers from uploaded documents\n",
    "- Apply all the techniques learned this week!\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You now have **production-ready RAG skills**!\n",
    "\n",
    "You can:\n",
    "- ‚úÖ Implement hybrid search for robust retrieval\n",
    "- ‚úÖ Add re-ranking for better precision\n",
    "- ‚úÖ Use query transformation for better coverage\n",
    "- ‚úÖ Build hierarchical document retrieval\n",
    "- ‚úÖ Choose the right techniques for your use case\n",
    "\n",
    "**Keep building and see you tomorrow! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
