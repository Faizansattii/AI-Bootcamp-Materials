{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ Week 17 - Day 2: Serving ML Models via API\n",
    "\n",
    "## Today's Goals:\n",
    "âœ… Load and serve pre-trained ML models via FastAPI\n",
    "\n",
    "âœ… Create prediction endpoints for text and image models\n",
    "\n",
    "âœ… Handle file uploads (CSV files, images)\n",
    "\n",
    "âœ… Configure CORS for web application integration\n",
    "\n",
    "âœ… Implement proper error handling for ML models\n",
    "\n",
    "âœ… Build a complete Sentiment Analysis API\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Part 1: Setup - Install All Required Packages\n",
    "\n",
    "**What we're installing:**\n",
    "- `fastapi` & `uvicorn` - API framework (from Day 1)\n",
    "- `scikit-learn` - ML library for sentiment model\n",
    "- `joblib` - For loading saved models\n",
    "- `pandas` - For CSV file handling\n",
    "- `python-multipart` - For file uploads\n",
    "- `Pillow` - For image processing\n",
    "\n",
    "**â±ï¸ This will take about 1-2 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Installing ML and API packages...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-core (C:\\Users\\Zigron\\anaconda3\\envs\\ai_bootcamp\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All packages installed successfully!\n",
      "\n",
      "ğŸ’¡ What we installed:\n",
      "   â€¢ FastAPI - API framework\n",
      "   â€¢ Scikit-learn - ML library\n",
      "   â€¢ Joblib - Model loading\n",
      "   â€¢ Pandas - CSV handling\n",
      "   â€¢ Pillow - Image processing\n",
      "   â€¢ Python-multipart - File uploads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Install packages\n",
    "print(\"ğŸ“¦ Installing ML and API packages...\\n\")\n",
    "\n",
    "!pip install -q fastapi uvicorn[standard]\n",
    "!pip install -q scikit-learn joblib pandas\n",
    "!pip install -q python-multipart Pillow\n",
    "!pip install -q requests  # For testing\n",
    "\n",
    "print(\"\\nâœ… All packages installed successfully!\")\n",
    "print(\"\\nğŸ’¡ What we installed:\")\n",
    "print(\"   â€¢ FastAPI - API framework\")\n",
    "print(\"   â€¢ Scikit-learn - ML library\")\n",
    "print(\"   â€¢ Joblib - Model loading\")\n",
    "print(\"   â€¢ Pandas - CSV handling\")\n",
    "print(\"   â€¢ Pillow - Image processing\")\n",
    "print(\"   â€¢ Python-multipart - File uploads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "\n",
      "ğŸ¯ Ready to build ML APIs!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Import all libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FastAPI essentials\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# ML and data processing\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# File handling\n",
    "from io import StringIO, BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Server utilities\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# For tracking time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"\\nğŸ¯ Ready to build ML APIs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Part 2: Understanding ML Model Deployment\n",
    "\n",
    "### ğŸ¤” The Journey from Notebook to API\n",
    "\n",
    "**Traditional ML Workflow:**\n",
    "```\n",
    "1. Train model in Jupyter\n",
    "2. Save model to file (.pkl, .joblib)\n",
    "3. ??? How do others use it? ???\n",
    "```\n",
    "\n",
    "**With FastAPI:**\n",
    "```\n",
    "1. Train model in Jupyter âœ…\n",
    "2. Save model to file âœ…\n",
    "3. Load model in FastAPI âœ…\n",
    "4. Create prediction endpoint âœ…\n",
    "5. Anyone can use it via HTTP! ğŸ‰\n",
    "```\n",
    "\n",
    "### ğŸ’¾ The Singleton Pattern (Load Once, Use Many)\n",
    "\n",
    "**âŒ BAD Approach:**\n",
    "```python\n",
    "@app.post(\"/predict\")\n",
    "def predict(text: str):\n",
    "    model = joblib.load('model.pkl')  # Loads EVERY time! Slow!\n",
    "    return model.predict([text])\n",
    "```\n",
    "\n",
    "**âœ… GOOD Approach:**\n",
    "```python\n",
    "# Load once at startup\n",
    "model = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def load_model():\n",
    "    global model\n",
    "    model = joblib.load('model.pkl')  # Loads ONCE!\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(text: str):\n",
    "    return model.predict([text])  # Uses loaded model - Fast!\n",
    "```\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "\n",
    "1. **Model Loading:** Happens once when server starts\n",
    "2. **Global Variable:** Model stored in memory for all requests\n",
    "3. **Fast Predictions:** No loading overhead per request\n",
    "4. **Memory Efficient:** One model instance for all users\n",
    "\n",
    "**ğŸ’¡ Think of it like:**\n",
    "- Bad: Opening a dictionary for every word lookup\n",
    "- Good: Keep the dictionary open on your desk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¤– Part 3: Creating a Simple Sentiment Analysis Model\n",
    "\n",
    "Before we can serve a model, we need one! Let's create a simple sentiment classifier.\n",
    "\n",
    "**What we're building:**\n",
    "- A text classifier that predicts: Positive or Negative\n",
    "- Uses TF-IDF for text features\n",
    "- Naive Bayes for classification\n",
    "- Packaged in a sklearn Pipeline\n",
    "\n",
    "**In real projects:** You'd load a pre-trained model from Week 6!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Creating sentiment analysis model...\n",
      "\n",
      "âœ… Model trained successfully!\n",
      "\n",
      "ğŸ“Š Model Details:\n",
      "   â€¢ Type: Pipeline\n",
      "   â€¢ Features: TF-IDF with max 100 features\n",
      "   â€¢ Classifier: Multinomial Naive Bayes\n",
      "   â€¢ Classes: ['negative' 'positive']\n",
      "\n",
      "ğŸ§ª Testing the model:\n",
      "   Text: 'I really enjoyed this product'\n",
      "   Prediction: positive\n",
      "\n",
      "   Text: 'This is absolutely terrible'\n",
      "   Prediction: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a simple sentiment analysis model\n",
    "print(\"ğŸ¤– Creating sentiment analysis model...\\n\")\n",
    "\n",
    "# Sample training data (in real projects, use much more data!)\n",
    "training_texts = [\n",
    "    \"This product is amazing! I love it!\",\n",
    "    \"Excellent service and great quality\",\n",
    "    \"Best purchase ever, highly recommend\",\n",
    "    \"Wonderful experience, very satisfied\",\n",
    "    \"Outstanding product, exceeded expectations\",\n",
    "    \"Terrible product, waste of money\",\n",
    "    \"Worst purchase ever, very disappointed\",\n",
    "    \"Poor quality, would not recommend\",\n",
    "    \"Awful experience, terrible service\",\n",
    "    \"Horrible product, complete disaster\"\n",
    "]\n",
    "\n",
    "training_labels = [\n",
    "    \"positive\", \"positive\", \"positive\", \"positive\", \"positive\",\n",
    "    \"negative\", \"negative\", \"negative\", \"negative\", \"negative\"\n",
    "]\n",
    "\n",
    "# Create a pipeline: TF-IDF + Naive Bayes\n",
    "sentiment_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=100)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "sentiment_model.fit(training_texts, training_labels)\n",
    "\n",
    "print(\"âœ… Model trained successfully!\")\n",
    "print(\"\\nğŸ“Š Model Details:\")\n",
    "print(f\"   â€¢ Type: {type(sentiment_model).__name__}\")\n",
    "print(f\"   â€¢ Features: TF-IDF with max 100 features\")\n",
    "print(f\"   â€¢ Classifier: Multinomial Naive Bayes\")\n",
    "print(f\"   â€¢ Classes: {sentiment_model.classes_}\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nğŸ§ª Testing the model:\")\n",
    "test_texts = [\n",
    "    \"I really enjoyed this product\",\n",
    "    \"This is absolutely terrible\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    prediction = sentiment_model.predict([text])[0]\n",
    "    print(f\"   Text: '{text}'\")\n",
    "    print(f\"   Prediction: {prediction}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving model to file...\n",
      "\n",
      "âœ… Model saved as: sentiment_model.joblib\n",
      "\n",
      "ğŸ’¡ In real projects:\n",
      "   â€¢ Train in Jupyter notebook\n",
      "   â€¢ Save model with joblib.dump()\n",
      "   â€¢ Load in FastAPI with joblib.load()\n",
      "\n",
      "ğŸ¯ This file contains:\n",
      "   â€¢ TF-IDF vectorizer (fitted)\n",
      "   â€¢ Naive Bayes classifier (trained)\n",
      "   â€¢ Everything needed for predictions!\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a file (this is what you'd do after training)\n",
    "print(\"ğŸ’¾ Saving model to file...\\n\")\n",
    "\n",
    "model_filename = 'sentiment_model.joblib'\n",
    "joblib.dump(sentiment_model, model_filename)\n",
    "\n",
    "print(f\"âœ… Model saved as: {model_filename}\")\n",
    "print(\"\\nğŸ’¡ In real projects:\")\n",
    "print(\"   â€¢ Train in Jupyter notebook\")\n",
    "print(\"   â€¢ Save model with joblib.dump()\")\n",
    "print(\"   â€¢ Load in FastAPI with joblib.load()\")\n",
    "print(\"\\nğŸ¯ This file contains:\")\n",
    "print(\"   â€¢ TF-IDF vectorizer (fitted)\")\n",
    "print(\"   â€¢ Naive Bayes classifier (trained)\")\n",
    "print(\"   â€¢ Everything needed for predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Part 4: Building the Sentiment Analysis API\n",
    "\n",
    "Now let's create a FastAPI application that serves our sentiment model!\n",
    "\n",
    "**What we're implementing:**\n",
    "1. Load model at startup (singleton pattern)\n",
    "2. Create Pydantic models for input/output\n",
    "3. Create prediction endpoint\n",
    "4. Add proper error handling\n",
    "5. Enable CORS for web apps\n",
    "\n",
    "**Let's build it step by step!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Creating Pydantic models...\n",
      "\n",
      "âœ… Pydantic models created!\n",
      "\n",
      "ğŸ“¦ Models defined:\n",
      "   â€¢ TextInput - Single text input\n",
      "   â€¢ BatchTextInput - Multiple texts\n",
      "   â€¢ PredictionOutput - Single prediction result\n",
      "   â€¢ BatchPredictionOutput - Batch results\n",
      "\n",
      "ğŸ’¡ These ensure type safety and validation!\n"
     ]
    }
   ],
   "source": [
    "# Define Pydantic models for request/response\n",
    "print(\"ğŸ“‹ Creating Pydantic models...\\n\")\n",
    "\n",
    "class TextInput(BaseModel):\n",
    "    \"\"\"\n",
    "    Input model for single text prediction.\n",
    "    \n",
    "    This defines what data the API expects.\n",
    "    FastAPI will validate automatically!\n",
    "    \"\"\"\n",
    "    text: str = Field(\n",
    "        ...,  # Required field\n",
    "        min_length=1,\n",
    "        max_length=5000,\n",
    "        description=\"Text to analyze for sentiment\",\n",
    "        example=\"This product is amazing!\"\n",
    "    )\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"text\": \"I love this product, it's fantastic!\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class BatchTextInput(BaseModel):\n",
    "    \"\"\"\n",
    "    Input model for batch predictions.\n",
    "    Allows analyzing multiple texts at once!\n",
    "    \"\"\"\n",
    "    texts: List[str] = Field(\n",
    "        ...,\n",
    "        min_items=1,\n",
    "        max_items=100,\n",
    "        description=\"List of texts to analyze\",\n",
    "        example=[\"Great product!\", \"Terrible service\"]\n",
    "    )\n",
    "\n",
    "class PredictionOutput(BaseModel):\n",
    "    \"\"\"\n",
    "    Output model for predictions.\n",
    "    Structured, consistent response format.\n",
    "    \"\"\"\n",
    "    text: str\n",
    "    sentiment: str  # 'positive' or 'negative'\n",
    "    confidence: float\n",
    "    processing_time_ms: float\n",
    "\n",
    "class BatchPredictionOutput(BaseModel):\n",
    "    \"\"\"\n",
    "    Output model for batch predictions.\n",
    "    \"\"\"\n",
    "    predictions: List[PredictionOutput]\n",
    "    total_processed: int\n",
    "    total_time_ms: float\n",
    "\n",
    "print(\"âœ… Pydantic models created!\")\n",
    "print(\"\\nğŸ“¦ Models defined:\")\n",
    "print(\"   â€¢ TextInput - Single text input\")\n",
    "print(\"   â€¢ BatchTextInput - Multiple texts\")\n",
    "print(\"   â€¢ PredictionOutput - Single prediction result\")\n",
    "print(\"   â€¢ BatchPredictionOutput - Batch results\")\n",
    "print(\"\\nğŸ’¡ These ensure type safety and validation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Creating Sentiment Analysis API...\n",
      "\n",
      "âœ… FastAPI app created with:\n",
      "   â€¢ CORS enabled (allows web apps to connect)\n",
      "   â€¢ Startup event for model loading\n",
      "   â€¢ Ready for endpoints!\n"
     ]
    }
   ],
   "source": [
    "# Create the FastAPI application\n",
    "print(\"ğŸš€ Creating Sentiment Analysis API...\\n\")\n",
    "\n",
    "# Global variable for the model (loaded once)\n",
    "loaded_model = None\n",
    "model_loaded_at = None\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Sentiment Analysis API\",\n",
    "    description=\"\"\"ğŸ¤– ML-powered API for sentiment analysis.\n",
    "    \n",
    "    Features:\n",
    "    - Single text prediction\n",
    "    - Batch text prediction\n",
    "    - CSV file upload for batch analysis\n",
    "    - CORS enabled for web apps\n",
    "    \"\"\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add CORS middleware (allows web apps to call our API)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production: specify exact domains!\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Event: Load model when server starts\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"\n",
    "    This runs ONCE when the server starts.\n",
    "    Perfect for loading ML models!\n",
    "    \"\"\"\n",
    "    global loaded_model, model_loaded_at\n",
    "    \n",
    "    print(\"ğŸ”„ Loading sentiment model...\")\n",
    "    try:\n",
    "        loaded_model = joblib.load('sentiment_model.joblib')\n",
    "        model_loaded_at = datetime.now()\n",
    "        print(\"âœ… Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"âœ… FastAPI app created with:\")\n",
    "print(\"   â€¢ CORS enabled (allows web apps to connect)\")\n",
    "print(\"   â€¢ Startup event for model loading\")\n",
    "print(\"   â€¢ Ready for endpoints!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Understanding CORS:\n",
    "\n",
    "**What is CORS?**\n",
    "- **C**ross-**O**rigin **R**esource **S**haring\n",
    "- Browser security feature that blocks requests between different domains\n",
    "\n",
    "**The Problem:**\n",
    "```\n",
    "Your React app: http://localhost:3000\n",
    "Your API: http://localhost:8000\n",
    "\n",
    "Browser says: \"These are different origins! ğŸš« BLOCKED!\"\n",
    "```\n",
    "\n",
    "**The Solution:**\n",
    "```python\n",
    "# Add CORS middleware to FastAPI\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"]  # Allow all (development only!)\n",
    ")\n",
    "```\n",
    "\n",
    "**âš ï¸ Important:**\n",
    "- `allow_origins=[\"*\"]` = Allow ALL domains (only for development!)\n",
    "- Production: `allow_origins=[\"https://myapp.com\"]` (specific domains only)\n",
    "\n",
    "**ğŸ’¡ Remember:** CORS is a browser thing, not an API restriction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Adding prediction endpoints...\n",
      "\n",
      "âœ… All endpoints added!\n",
      "\n",
      "ğŸ“‹ Available endpoints:\n",
      "   â€¢ GET  /             - API info\n",
      "   â€¢ GET  /model-info   - Model details\n",
      "   â€¢ POST /predict      - Single prediction\n",
      "   â€¢ POST /predict-batch - Batch predictions\n",
      "   â€¢ POST /upload-csv   - CSV file upload\n",
      "\n",
      "ğŸ’¡ Note the 'async' keyword in upload-csv!\n"
     ]
    }
   ],
   "source": [
    "# Add API endpoints\n",
    "print(\"ğŸ”§ Adding prediction endpoints...\\n\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    \"\"\"Welcome endpoint with API information\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Welcome to Sentiment Analysis API! ğŸ¤–\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"endpoints\": {\n",
    "            \"POST /predict\": \"Analyze single text\",\n",
    "            \"POST /predict-batch\": \"Analyze multiple texts\",\n",
    "            \"POST /upload-csv\": \"Upload CSV for batch analysis\",\n",
    "            \"GET /model-info\": \"Get model information\"\n",
    "        },\n",
    "        \"tip\": \"Visit /docs for interactive API testing!\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/model-info\")\n",
    "def get_model_info():\n",
    "    \"\"\"\n",
    "    Get information about the loaded model.\n",
    "    Useful for debugging and version tracking.\n",
    "    \"\"\"\n",
    "    if loaded_model is None:\n",
    "        raise HTTPException(\n",
    "            status_code=503,  # Service Unavailable\n",
    "            detail=\"Model not loaded. Please restart the server.\"\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"model_type\": type(loaded_model).__name__,\n",
    "        \"classes\": list(loaded_model.classes_),\n",
    "        \"loaded_at\": model_loaded_at.isoformat() if model_loaded_at else None,\n",
    "        \"status\": \"ready\"\n",
    "    }\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionOutput)\n",
    "def predict_sentiment(input_data: TextInput):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a single text.\n",
    "    \n",
    "    Returns:\n",
    "    - sentiment: 'positive' or 'negative'\n",
    "    - confidence: probability of prediction\n",
    "    - processing_time_ms: how long it took\n",
    "    \"\"\"\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check if model is loaded\n",
    "    if loaded_model is None:\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"Model not loaded. Please restart the server.\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Make prediction\n",
    "        text = input_data.text\n",
    "        prediction = loaded_model.predict([text])[0]\n",
    "        probabilities = loaded_model.predict_proba([text])[0]\n",
    "        \n",
    "        # Get confidence (probability of predicted class)\n",
    "        predicted_index = list(loaded_model.classes_).index(prediction)\n",
    "        confidence = float(probabilities[predicted_index])\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        return PredictionOutput(\n",
    "            text=text,\n",
    "            sentiment=prediction,\n",
    "            confidence=confidence,\n",
    "            processing_time_ms=round(processing_time, 2)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Prediction error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.post(\"/predict-batch\", response_model=BatchPredictionOutput)\n",
    "def predict_sentiment_batch(input_data: BatchTextInput):\n",
    "    \"\"\"\n",
    "    Predict sentiment for multiple texts at once.\n",
    "    More efficient than calling /predict multiple times!\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if loaded_model is None:\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"Model not loaded.\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        predictions_list = []\n",
    "        \n",
    "        for text in input_data.texts:\n",
    "            text_start = time.time()\n",
    "            \n",
    "            # Predict\n",
    "            prediction = loaded_model.predict([text])[0]\n",
    "            probabilities = loaded_model.predict_proba([text])[0]\n",
    "            predicted_index = list(loaded_model.classes_).index(prediction)\n",
    "            confidence = float(probabilities[predicted_index])\n",
    "            \n",
    "            text_time = (time.time() - text_start) * 1000\n",
    "            \n",
    "            predictions_list.append(\n",
    "                PredictionOutput(\n",
    "                    text=text,\n",
    "                    sentiment=prediction,\n",
    "                    confidence=confidence,\n",
    "                    processing_time_ms=round(text_time, 2)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        total_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return BatchPredictionOutput(\n",
    "            predictions=predictions_list,\n",
    "            total_processed=len(predictions_list),\n",
    "            total_time_ms=round(total_time, 2)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Batch prediction error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.post(\"/upload-csv\")\n",
    "async def predict_from_csv(file: UploadFile = File(...)):\n",
    "    \"\"\"\n",
    "    Upload a CSV file with a 'text' column for batch sentiment analysis.\n",
    "    \n",
    "    CSV format:\n",
    "    text\n",
    "    \"This is great!\"\n",
    "    \"This is terrible\"\n",
    "    ...\n",
    "    \n",
    "    Returns predictions for all rows.\n",
    "    \"\"\"\n",
    "    # Validate file type\n",
    "    if not file.filename.endswith('.csv'):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"File must be a CSV (.csv extension)\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Read file\n",
    "        contents = await file.read()\n",
    "        df = pd.read_csv(StringIO(contents.decode('utf-8')))\n",
    "        \n",
    "        # Validate columns\n",
    "        if 'text' not in df.columns:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"CSV must have a 'text' column\"\n",
    "            )\n",
    "        \n",
    "        # Get predictions for all texts\n",
    "        texts = df['text'].tolist()\n",
    "        predictions = loaded_model.predict(texts)\n",
    "        probabilities = loaded_model.predict_proba(texts)\n",
    "        \n",
    "        # Create results\n",
    "        results = []\n",
    "        for i, (text, pred, probs) in enumerate(zip(texts, predictions, probabilities)):\n",
    "            pred_index = list(loaded_model.classes_).index(pred)\n",
    "            confidence = float(probs[pred_index])\n",
    "            \n",
    "            results.append({\n",
    "                \"row\": i + 1,\n",
    "                \"text\": text,\n",
    "                \"sentiment\": pred,\n",
    "                \"confidence\": round(confidence, 4)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"filename\": file.filename,\n",
    "            \"total_rows\": len(results),\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"CSV file is empty\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error processing CSV: {str(e)}\"\n",
    "        )\n",
    "\n",
    "print(\"âœ… All endpoints added!\")\n",
    "print(\"\\nğŸ“‹ Available endpoints:\")\n",
    "print(\"   â€¢ GET  /             - API info\")\n",
    "print(\"   â€¢ GET  /model-info   - Model details\")\n",
    "print(\"   â€¢ POST /predict      - Single prediction\")\n",
    "print(\"   â€¢ POST /predict-batch - Batch predictions\")\n",
    "print(\"   â€¢ POST /upload-csv   - CSV file upload\")\n",
    "print(\"\\nğŸ’¡ Note the 'async' keyword in upload-csv!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Understanding File Uploads:\n",
    "\n",
    "**Why use `async` for file uploads?**\n",
    "\n",
    "```python\n",
    "@app.post(\"/upload-csv\")\n",
    "async def predict_from_csv(file: UploadFile = File(...)):\n",
    "    contents = await file.read()  # Non-blocking!\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- File uploads can take time (especially large files)\n",
    "- `async`/`await` allows server to handle other requests while uploading\n",
    "- More efficient for production servers\n",
    "\n",
    "**Think of it like:**\n",
    "- **Sync (blocking):** Waiter waits by your table until you finish eating\n",
    "- **Async (non-blocking):** Waiter serves other tables while you eat\n",
    "\n",
    "**File Upload Pattern:**\n",
    "1. `file: UploadFile = File(...)` - Accept uploaded file\n",
    "2. `await file.read()` - Read file contents (async)\n",
    "3. Process the data\n",
    "4. Return results\n",
    "\n",
    "**ğŸ’¡ For CSV files:**\n",
    "- Read with `pd.read_csv(StringIO(contents.decode('utf-8')))`\n",
    "- Always validate columns exist\n",
    "- Handle errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Sentiment Analysis API...\n",
      "\n",
      "ğŸ”„ Loading sentiment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    [Errno 10048] error while attempting to bind on address ('127.0.0.1', 8002): only one usage of each socket address (protocol/network address/port) is normally permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n",
      "âœ… Server started successfully!\n",
      "\n",
      "ğŸŒ Your Sentiment Analysis API is running at:\n",
      "   â€¢ Main URL: http://127.0.0.1:8002\n",
      "   â€¢ Interactive Docs: http://127.0.0.1:8002/docs\n",
      "\n",
      "ğŸ’¡ Model loading... (check console for 'Model loaded' message)\n"
     ]
    }
   ],
   "source": [
    "# Helper function to run server\n",
    "def run_server(app, port=8000):\n",
    "    \"\"\"\n",
    "    Runs FastAPI server in background thread.\n",
    "    \"\"\"\n",
    "    def start_server():\n",
    "        uvicorn.run(app, host=\"127.0.0.1\", port=port, log_level=\"error\")\n",
    "    \n",
    "    thread = Thread(target=start_server, daemon=True)\n",
    "    thread.start()\n",
    "    time.sleep(3)  # Give server time to start and load model\n",
    "    \n",
    "    print(f\"âœ… Server started successfully!\")\n",
    "    print(f\"\\nğŸŒ Your Sentiment Analysis API is running at:\")\n",
    "    print(f\"   â€¢ Main URL: http://127.0.0.1:{port}\")\n",
    "    print(f\"   â€¢ Interactive Docs: http://127.0.0.1:{port}/docs\")\n",
    "    print(\"\\nğŸ’¡ Model loading... (check console for 'Model loaded' message)\")\n",
    "    \n",
    "    return thread\n",
    "\n",
    "# Start the server!\n",
    "print(\"ğŸš€ Starting Sentiment Analysis API...\\n\")\n",
    "server_thread = run_server(app, port=8002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Part 5: Testing the Sentiment Analysis API\n",
    "\n",
    "Let's test all our endpoints systematically!\n",
    "\n",
    "**What we'll test:**\n",
    "1. Model info endpoint\n",
    "2. Single prediction\n",
    "3. Batch prediction\n",
    "4. CSV file upload\n",
    "5. Error handling\n",
    "\n",
    "**Testing methods:**\n",
    "- Programmatic (using requests library)\n",
    "- Swagger UI (browser-based testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing API Endpoints...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1ï¸âƒ£ Testing Model Info Endpoint\n",
      "----------------------------------------------------------------------\n",
      "âœ… Model Information:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m model_info = response.json()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Model Information:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   â€¢ Model Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   â€¢ Classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_info[\u001b[33m'\u001b[39m\u001b[33mclasses\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   â€¢ Status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_info[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'model_type'"
     ]
    }
   ],
   "source": [
    "# Test 1: Model Info\n",
    "print(\"ğŸ§ª Testing API Endpoints...\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n1ï¸âƒ£ Testing Model Info Endpoint\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:8002/model-info\")\n",
    "model_info = response.json()\n",
    "\n",
    "print(\"âœ… Model Information:\")\n",
    "print(f\"   â€¢ Model Type: {model_info['model_type']}\")\n",
    "print(f\"   â€¢ Classes: {model_info['classes']}\")\n",
    "print(f\"   â€¢ Status: {model_info['status']}\")\n",
    "print(f\"   â€¢ Loaded At: {model_info['loaded_at']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Single Prediction (Positive)\n",
    "print(\"\\n2ï¸âƒ£ Testing Single Prediction - Positive Text\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "data = {\n",
    "    \"text\": \"This product is absolutely amazing! I love it so much and highly recommend it to everyone!\"\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://127.0.0.1:8002/predict\", json=data)\n",
    "result = response.json()\n",
    "\n",
    "print(f\"ğŸ“ Input: '{data['text']}'\")\n",
    "print(f\"\\nğŸ¯ Prediction:\")\n",
    "print(f\"   â€¢ Sentiment: {result['sentiment'].upper()}\")\n",
    "print(f\"   â€¢ Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"   â€¢ Processing Time: {result['processing_time_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Single Prediction (Negative)\n",
    "print(\"\\n3ï¸âƒ£ Testing Single Prediction - Negative Text\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "data = {\n",
    "    \"text\": \"Terrible experience! Worst purchase ever. Complete waste of money and time. Very disappointed!\"\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://127.0.0.1:8002/predict\", json=data)\n",
    "result = response.json()\n",
    "\n",
    "print(f\"ğŸ“ Input: '{data['text']}'\")\n",
    "print(f\"\\nğŸ¯ Prediction:\")\n",
    "print(f\"   â€¢ Sentiment: {result['sentiment'].upper()}\")\n",
    "print(f\"   â€¢ Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"   â€¢ Processing Time: {result['processing_time_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Batch Prediction\n",
    "print(\"\\n4ï¸âƒ£ Testing Batch Prediction\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "batch_data = {\n",
    "    \"texts\": [\n",
    "        \"Great product, highly satisfied!\",\n",
    "        \"Poor quality, not worth the price\",\n",
    "        \"Excellent service and fast delivery\",\n",
    "        \"Horrible experience, never buying again\",\n",
    "        \"Outstanding quality, exceeded expectations\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://127.0.0.1:8002/predict-batch\", json=batch_data)\n",
    "results = response.json()\n",
    "\n",
    "print(f\"ğŸ“Š Batch Analysis of {results['total_processed']} texts:\")\n",
    "print(f\"â±ï¸  Total Time: {results['total_time_ms']:.2f}ms\")\n",
    "print(f\"\\nğŸ“‹ Results:\")\n",
    "\n",
    "for i, pred in enumerate(results['predictions'], 1):\n",
    "    sentiment_emoji = \"ğŸ˜Š\" if pred['sentiment'] == \"positive\" else \"ğŸ˜\"\n",
    "    print(f\"\\n   {i}. {sentiment_emoji} {pred['sentiment'].upper()} ({pred['confidence']:.2%})\")\n",
    "    print(f\"      Text: \\\"{pred['text'][:50]}...\\\"\")\n",
    "    print(f\"      Time: {pred['processing_time_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Create and upload a CSV file\n",
    "print(\"\\n5ï¸âƒ£ Testing CSV Upload\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create a sample CSV\n",
    "csv_content = \"\"\"text\n",
    "\"This is the best product I've ever bought!\"\n",
    "\"Worst purchase ever, total waste of money\"\n",
    "\"Excellent quality and great customer service\"\n",
    "\"Terrible experience, would not recommend\"\n",
    "\"Amazing product, exceeded all my expectations\"\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('test_sentiments.csv', 'w') as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "print(\"ğŸ“„ Created test CSV file with 5 reviews\")\n",
    "\n",
    "# Upload the file\n",
    "with open('test_sentiments.csv', 'rb') as f:\n",
    "    files = {'file': ('test_sentiments.csv', f, 'text/csv')}\n",
    "    response = requests.post(\"http://127.0.0.1:8002/upload-csv\", files=files)\n",
    "\n",
    "results = response.json()\n",
    "\n",
    "print(f\"\\nâœ… File Uploaded: {results['filename']}\")\n",
    "print(f\"ğŸ“Š Total Rows Processed: {results['total_rows']}\")\n",
    "print(f\"\\nğŸ“‹ Analysis Results:\")\n",
    "\n",
    "for result in results['results']:\n",
    "    sentiment_emoji = \"ğŸ˜Š\" if result['sentiment'] == \"positive\" else \"ğŸ˜\"\n",
    "    print(f\"\\n   Row {result['row']}: {sentiment_emoji} {result['sentiment'].upper()}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"   Text: \\\"{result['text'][:60]}...\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Error Handling\n",
    "print(\"\\n6ï¸âƒ£ Testing Error Handling\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Test empty text\n",
    "print(\"\\nğŸ§ª Test: Empty text (should fail validation)\")\n",
    "data = {\"text\": \"\"}\n",
    "response = requests.post(\"http://127.0.0.1:8002/predict\", json=data)\n",
    "if response.status_code == 422:  # Validation error\n",
    "    print(\"   âœ… Validation error caught correctly!\")\n",
    "    print(f\"   Status: {response.status_code}\")\n",
    "    print(f\"   Message: Field validation failed\")\n",
    "\n",
    "# Test missing field\n",
    "print(\"\\nğŸ§ª Test: Missing 'text' field (should fail)\")\n",
    "data = {\"wrong_field\": \"some text\"}\n",
    "response = requests.post(\"http://127.0.0.1:8002/predict\", json=data)\n",
    "if response.status_code == 422:\n",
    "    print(\"   âœ… Missing field error caught!\")\n",
    "    print(f\"   Status: {response.status_code}\")\n",
    "\n",
    "# Test wrong file type\n",
    "print(\"\\nğŸ§ª Test: Wrong file type (should fail)\")\n",
    "with open('test.txt', 'w') as f:\n",
    "    f.write(\"test\")\n",
    "\n",
    "with open('test.txt', 'rb') as f:\n",
    "    files = {'file': ('test.txt', f, 'text/plain')}\n",
    "    response = requests.post(\"http://127.0.0.1:8002/upload-csv\", files=files)\n",
    "\n",
    "if response.status_code == 400:\n",
    "    print(\"   âœ… File type error caught!\")\n",
    "    error = response.json()\n",
    "    print(f\"   Error: {error['detail']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nâœ… All tests completed! Your ML API is working perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒ Part 6: Testing in Swagger UI\n",
    "\n",
    "Now let's use the interactive documentation!\n",
    "\n",
    "### ğŸ¯ Swagger UI Guide:\n",
    "\n",
    "**1. Open Swagger UI:**\n",
    "- Visit: http://127.0.0.1:8002/docs\n",
    "- You'll see all your endpoints listed!\n",
    "\n",
    "**2. Test /predict endpoint:**\n",
    "- Click on `POST /predict`\n",
    "- Click \"Try it out\"\n",
    "- Modify the example text:\n",
    "  ```json\n",
    "  {\n",
    "    \"text\": \"This is an amazing product!\"\n",
    "  }\n",
    "  ```\n",
    "- Click \"Execute\"\n",
    "- See the response below!\n",
    "\n",
    "**3. Test /predict-batch:**\n",
    "- Same process, but with multiple texts:\n",
    "  ```json\n",
    "  {\n",
    "    \"texts\": [\n",
    "      \"Great product!\",\n",
    "      \"Terrible service\"\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "**4. Test /upload-csv:**\n",
    "- Click `POST /upload-csv`\n",
    "- Click \"Try it out\"\n",
    "- Click \"Choose File\"\n",
    "- Select `test_sentiments.csv`\n",
    "- Click \"Execute\"\n",
    "- See all results!\n",
    "\n",
    "**ğŸ’¡ Swagger UI Benefits:**\n",
    "- âœ… No code needed to test\n",
    "- âœ… See request/response formats\n",
    "- âœ… Try different inputs instantly\n",
    "- âœ… Share with frontend developers\n",
    "- âœ… Auto-generated from your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ Part 7: Best Practices for ML APIs\n",
    "\n",
    "### ğŸ¯ Model Loading:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Load model once at startup\n",
    "- Use global variable or dependency injection\n",
    "- Add health check endpoint\n",
    "- Log model loading status\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Load model on every request\n",
    "- Load multiple copies of same model\n",
    "- Ignore loading errors\n",
    "\n",
    "### ğŸ¯ Error Handling:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Use try-except blocks\n",
    "- Return meaningful error messages\n",
    "- Use appropriate HTTP status codes\n",
    "- Log errors for debugging\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Let server crash on errors\n",
    "- Return generic \"Error\" messages\n",
    "- Expose internal error details to users\n",
    "\n",
    "### ğŸ¯ Input Validation:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Use Pydantic models\n",
    "- Set min/max lengths\n",
    "- Validate file types\n",
    "- Check file sizes\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Trust all user input\n",
    "- Allow unlimited file sizes\n",
    "- Skip type checking\n",
    "\n",
    "### ğŸ¯ Response Format:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Return consistent structure\n",
    "- Include confidence scores\n",
    "- Add metadata (processing time, model version)\n",
    "- Use Pydantic for response models\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Return raw predictions only\n",
    "- Change response structure randomly\n",
    "- Skip documentation\n",
    "\n",
    "### ğŸ¯ Performance:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Use batch endpoints for multiple predictions\n",
    "- Consider async for I/O operations\n",
    "- Cache frequent predictions\n",
    "- Monitor response times\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Process files synchronously\n",
    "- Load large models on every prediction\n",
    "- Ignore memory usage\n",
    "\n",
    "### ğŸ¯ Security:\n",
    "\n",
    "âœ… **DO:**\n",
    "- Validate file uploads\n",
    "- Set CORS properly (specific origins in production)\n",
    "- Add rate limiting\n",
    "- Use environment variables for secrets\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Allow all CORS origins in production\n",
    "- Accept unlimited requests\n",
    "- Hardcode API keys\n",
    "\n",
    "**ğŸ’¡ Remember:** Your API is the gateway to your ML model - make it robust!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Part 8: Beginner Challenge\n",
    "\n",
    "### ğŸ† Your Mission:\n",
    "\n",
    "Enhance the Sentiment Analysis API with new features!\n",
    "\n",
    "### ğŸ“‹ Requirements:\n",
    "\n",
    "**1. Add a Statistics Endpoint**\n",
    "- Create `GET /stats` that returns:\n",
    "  - Total predictions made\n",
    "  - Positive vs negative count\n",
    "  - Average confidence score\n",
    "- Use a global counter to track predictions\n",
    "\n",
    "**2. Add Sentiment Distribution Endpoint**\n",
    "- Create `POST /analyze-distribution` that:\n",
    "  - Takes a list of texts\n",
    "  - Returns percentage positive vs negative\n",
    "  - Shows average confidence per sentiment\n",
    "\n",
    "**3. Add Language Detection (Bonus)**\n",
    "- Install `langdetect` library\n",
    "- Add language detection to predictions\n",
    "- Return warning if text is not English\n",
    "\n",
    "### ğŸ’¡ Hints:\n",
    "\n",
    "```python\n",
    "# Hint 1: Global statistics tracking\n",
    "prediction_stats = {\n",
    "    \"total\": 0,\n",
    "    \"positive\": 0,\n",
    "    \"negative\": 0,\n",
    "    \"confidences\": []\n",
    "}\n",
    "\n",
    "# Update in prediction endpoints\n",
    "prediction_stats[\"total\"] += 1\n",
    "prediction_stats[sentiment] += 1\n",
    "prediction_stats[\"confidences\"].append(confidence)\n",
    "\n",
    "# Hint 2: Distribution calculation\n",
    "total = len(predictions)\n",
    "positive_pct = (positive_count / total) * 100\n",
    "negative_pct = (negative_count / total) * 100\n",
    "\n",
    "# Hint 3: Language detection\n",
    "from langdetect import detect\n",
    "language = detect(text)\n",
    "if language != 'en':\n",
    "    warning = \"Text may not be English\"\n",
    "```\n",
    "\n",
    "### ğŸ¯ Expected Outcome:\n",
    "\n",
    "```python\n",
    "# GET /stats\n",
    "{\n",
    "    \"total_predictions\": 25,\n",
    "    \"positive_count\": 15,\n",
    "    \"negative_count\": 10,\n",
    "    \"positive_percentage\": 60.0,\n",
    "    \"average_confidence\": 0.87\n",
    "}\n",
    "\n",
    "# POST /analyze-distribution\n",
    "{\n",
    "    \"total_texts\": 10,\n",
    "    \"distribution\": {\n",
    "        \"positive\": {\"count\": 6, \"percentage\": 60.0, \"avg_confidence\": 0.89},\n",
    "        \"negative\": {\"count\": 4, \"percentage\": 40.0, \"avg_confidence\": 0.83}\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Bonus Challenges:\n",
    "\n",
    "1. Add a `/health` endpoint that checks:\n",
    "   - Server status\n",
    "   - Model loaded status\n",
    "   - Memory usage\n",
    "\n",
    "2. Add text preprocessing:\n",
    "   - Remove URLs\n",
    "   - Remove special characters\n",
    "   - Convert to lowercase\n",
    "\n",
    "3. Add response caching:\n",
    "   - Cache predictions for identical texts\n",
    "   - Use dictionary: `{text: prediction}`\n",
    "   - Save time on repeated queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Try implementing the challenge requirements\n",
    "\n",
    "# Step 1: Add statistics tracking\n",
    "# prediction_stats = {...}\n",
    "\n",
    "# Step 2: Create /stats endpoint\n",
    "# @app.get(\"/stats\")\n",
    "# def get_stats():\n",
    "#     ...\n",
    "\n",
    "# Step 3: Create /analyze-distribution endpoint\n",
    "# @app.post(\"/analyze-distribution\")\n",
    "# def analyze_distribution(data: BatchTextInput):\n",
    "#     ...\n",
    "\n",
    "# Restart server and test in Swagger UI!\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Summary - What We Learned Today\n",
    "\n",
    "### 1. ML Model Deployment ğŸ¤–\n",
    "- **Singleton pattern** - Load model once, use many times\n",
    "- **Startup events** - Initialize resources when server starts\n",
    "- **Global variables** - Store model in memory for all requests\n",
    "- **Model serialization** - Save with joblib, load in API\n",
    "\n",
    "### 2. Prediction Endpoints ğŸ”®\n",
    "- **Single predictions** - `/predict` for one text at a time\n",
    "- **Batch predictions** - `/predict-batch` for multiple texts efficiently\n",
    "- **Structured responses** - Consistent format with confidence and metadata\n",
    "- **Pydantic models** - Type-safe input/output validation\n",
    "\n",
    "### 3. File Upload Handling ğŸ“¤\n",
    "- **Async file uploads** - Use `async`/`await` for efficiency\n",
    "- **CSV processing** - Parse with pandas, return structured results\n",
    "- **File validation** - Check type, size, content before processing\n",
    "- **Error handling** - Graceful failures with helpful messages\n",
    "\n",
    "### 4. CORS Configuration ğŸŒ\n",
    "- **What is CORS** - Browser security feature for cross-origin requests\n",
    "- **Why it matters** - Web apps need CORS to call your API\n",
    "- **How to enable** - CORSMiddleware in FastAPI\n",
    "- **Production considerations** - Specific origins only, not \"*\"\n",
    "\n",
    "### 5. Error Handling âš ï¸\n",
    "- **ML-specific errors** - Wrong input shape, invalid features\n",
    "- **HTTP exceptions** - 400 (Bad Request), 500 (Server Error), 503 (Unavailable)\n",
    "- **Validation errors** - Automatic from Pydantic (422 status)\n",
    "- **User-friendly messages** - Always explain what went wrong\n",
    "\n",
    "### 6. API Best Practices ğŸ’¡\n",
    "- **Response structure** - Include metadata (confidence, time, version)\n",
    "- **Endpoint design** - Single and batch operations\n",
    "- **Documentation** - Docstrings appear in Swagger UI\n",
    "- **Performance** - Batch operations, async for I/O\n",
    "\n",
    "### 7. Testing Strategies ğŸ§ª\n",
    "- **Programmatic testing** - Use requests library\n",
    "- **Swagger UI** - Interactive browser-based testing\n",
    "- **Error case testing** - Verify failures handle gracefully\n",
    "- **Multiple input types** - Text, JSON, files\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "âœ… **ML models become useful when deployed as APIs**\n",
    "- Notebooks are for development, APIs are for deployment\n",
    "\n",
    "âœ… **Load models once, not on every request**\n",
    "- Use startup events and global variables\n",
    "- Massive performance improvement!\n",
    "\n",
    "âœ… **Structure your responses consistently**\n",
    "- Prediction + confidence + metadata\n",
    "- Makes client integration easier\n",
    "\n",
    "âœ… **CORS is essential for web apps**\n",
    "- Browser security blocks cross-origin by default\n",
    "- Easy to enable in FastAPI\n",
    "\n",
    "âœ… **Validation and error handling are critical**\n",
    "- Pydantic validates automatically\n",
    "- Always return helpful error messages\n",
    "\n",
    "âœ… **Swagger UI is your best friend**\n",
    "- Test immediately without writing client code\n",
    "- Share with team for integration\n",
    "\n",
    "âœ… **Async for file operations**\n",
    "- Better performance under load\n",
    "- Non-blocking I/O operations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Pro Tips for Production\n",
    "\n",
    "1. **Model Versioning**\n",
    "   - Include model version in responses\n",
    "   - Track which model made each prediction\n",
    "   - Helps with debugging and A/B testing\n",
    "\n",
    "2. **Logging**\n",
    "   - Log all predictions for monitoring\n",
    "   - Track response times\n",
    "   - Alert on errors\n",
    "\n",
    "3. **Rate Limiting**\n",
    "   - Prevent abuse\n",
    "   - Use slowapi library\n",
    "   - Set per-user or per-IP limits\n",
    "\n",
    "4. **Caching**\n",
    "   - Cache frequent predictions\n",
    "   - Use Redis or in-memory dict\n",
    "   - Massive speed improvement\n",
    "\n",
    "5. **Monitoring**\n",
    "   - Track API usage\n",
    "   - Monitor model performance\n",
    "   - Alert on drift or degradation\n",
    "\n",
    "6. **Security**\n",
    "   - Use API keys for authentication\n",
    "   - Validate all inputs\n",
    "   - Limit file sizes\n",
    "   - Use HTTPS in production\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Next Steps - Tomorrow!\n",
    "\n",
    "**Day 3: Advanced FastAPI**\n",
    "\n",
    "We'll learn:\n",
    "- **Server-Sent Events (SSE)** - Streaming LLM outputs\n",
    "- **API documentation** - Customizing Swagger UI\n",
    "- **Security best practices** - Environment variables, API keys\n",
    "- **Production deployment** - Docker, CI/CD basics\n",
    "- **Team presentation** - Communication skills\n",
    "\n",
    "**Get ready to make your APIs production-ready! ğŸš€**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've built a production-grade ML API!\n",
    "\n",
    "**You now know how to:**\n",
    "- âœ… Load and serve ML models via FastAPI\n",
    "- âœ… Create prediction endpoints (single and batch)\n",
    "- âœ… Handle file uploads (CSV, images)\n",
    "- âœ… Configure CORS for web apps\n",
    "- âœ… Handle errors gracefully\n",
    "- âœ… Structure responses properly\n",
    "- âœ… Test with Swagger UI\n",
    "- âœ… Deploy real ML models to production!\n",
    "\n",
    "**This is a HUGE achievement! ğŸŠ**\n",
    "\n",
    "Your ML models are no longer stuck in notebooks - they're accessible to the world!\n",
    "\n",
    "**Practice what you learned and see you tomorrow! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
