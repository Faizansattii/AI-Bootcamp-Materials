{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Week 17 - Day 3: Advanced FastAPI - Streaming, Security & Production\n",
    "\n",
    "## Today's Goals:\n",
    "‚úÖ Implement Server-Sent Events (SSE) for streaming responses\n",
    "\n",
    "‚úÖ Stream LLM outputs in real-time (ChatGPT-style)\n",
    "\n",
    "‚úÖ Manage secrets with environment variables\n",
    "\n",
    "‚úÖ Customize API documentation (Swagger UI)\n",
    "\n",
    "‚úÖ Implement security best practices\n",
    "\n",
    "‚úÖ Prepare APIs for production deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Part 1: Setup - Install Packages\n",
    "\n",
    "**What we're installing:**\n",
    "- `fastapi` & `uvicorn` - API framework (continuing from Days 1-2)\n",
    "- `python-dotenv` - Environment variable management\n",
    "- `slowapi` - Rate limiting for security\n",
    "- Standard libraries - asyncio, os, logging\n",
    "\n",
    "**‚è±Ô∏è This will take about 30 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Install packages\n",
    "print(\"üì¶ Installing FastAPI and security packages...\\n\")\n",
    "\n",
    "!pip install -q fastapi uvicorn[standard]\n",
    "!pip install -q python-dotenv\n",
    "!pip install -q slowapi\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")\n",
    "print(\"\\nüí° What we installed:\")\n",
    "print(\"   ‚Ä¢ FastAPI - API framework\")\n",
    "print(\"   ‚Ä¢ python-dotenv - Environment variables\")\n",
    "print(\"   ‚Ä¢ slowapi - Rate limiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Import all libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FastAPI essentials\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Environment and security\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Rate limiting\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "from slowapi.errors import RateLimitExceeded\n",
    "\n",
    "# Async and streaming\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Server utilities\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"\\nüéØ Ready for advanced FastAPI features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì° Part 2: Understanding Server-Sent Events (SSE)\n",
    "\n",
    "### ü§î What is Streaming?\n",
    "\n",
    "**Traditional API (Request-Response):**\n",
    "```\n",
    "Client: \"Give me the weather\"\n",
    "         ‚Üì\n",
    "    [Waiting...]\n",
    "         ‚Üì\n",
    "Server: \"Here's the complete response: Sunny, 75¬∞F\"\n",
    "```\n",
    "\n",
    "**Streaming API (Server-Sent Events):**\n",
    "```\n",
    "Client: \"Generate a story\"\n",
    "         ‚Üì\n",
    "Server: \"Once\" ... \"upon\" ... \"a\" ... \"time\" ...\n",
    "         ‚Üë\n",
    "    (Real-time streaming!)\n",
    "```\n",
    "\n",
    "### üéØ Real-World Examples:\n",
    "\n",
    "**ChatGPT-style responses:**\n",
    "- User asks question\n",
    "- AI generates answer word-by-word\n",
    "- User sees text appearing in real-time\n",
    "\n",
    "**Live updates:**\n",
    "- Stock prices updating\n",
    "- Sports scores streaming\n",
    "- Social media feeds\n",
    "\n",
    "**Long-running tasks:**\n",
    "- Processing large files\n",
    "- Training ML models\n",
    "- Batch operations\n",
    "\n",
    "### üí° Why SSE?\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ **Better UX** - Users see progress, not just waiting\n",
    "- ‚úÖ **Simple** - Works over standard HTTP\n",
    "- ‚úÖ **One-way** - Server ‚Üí Client (perfect for AI)\n",
    "- ‚úÖ **Automatic reconnection** - Browsers handle this\n",
    "\n",
    "**SSE vs WebSockets:**\n",
    "\n",
    "| Feature | SSE | WebSockets |\n",
    "|---------|-----|------------|\n",
    "| **Direction** | Server ‚Üí Client only | Bidirectional |\n",
    "| **Protocol** | HTTP | ws:// protocol |\n",
    "| **Complexity** | Simple | More complex |\n",
    "| **Use Case** | Notifications, AI streaming | Chat, gaming |\n",
    "\n",
    "**üí° For AI APIs, SSE is usually perfect!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Part 3: Your First Streaming Endpoint\n",
    "\n",
    "Let's build a simple streaming endpoint that sends messages one by one!\n",
    "\n",
    "**What we're building:**\n",
    "- Endpoint that counts from 1 to 10\n",
    "- Sends each number with a delay\n",
    "- Client receives them in real-time\n",
    "\n",
    "**Key concepts:**\n",
    "- `async def` - Asynchronous function\n",
    "- `yield` - Send data incrementally\n",
    "- `StreamingResponse` - FastAPI's streaming class\n",
    "- SSE format: `\"data: <content>\\n\\n\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FastAPI app\n",
    "print(\"üöÄ Creating Advanced FastAPI application...\\n\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Advanced API with Streaming\",\n",
    "    description=\"\"\"üî• Production-ready API featuring:\n",
    "    \n",
    "    - Server-Sent Events (SSE) for streaming\n",
    "    - LLM-style text generation\n",
    "    - Environment variable security\n",
    "    - Rate limiting\n",
    "    - Comprehensive logging\n",
    "    \"\"\",\n",
    "    version=\"3.0.0\",\n",
    "    contact={\n",
    "        \"name\": \"AI Bootcamp Team\",\n",
    "        \"email\": \"support@aibootcamp.com\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ FastAPI app created with:\")\n",
    "print(\"   ‚Ä¢ Custom title and description\")\n",
    "print(\"   ‚Ä¢ Version tracking\")\n",
    "print(\"   ‚Ä¢ Contact information\")\n",
    "print(\"   ‚Ä¢ Logging configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple streaming endpoint\n",
    "print(\"üì° Adding streaming endpoints...\\n\")\n",
    "\n",
    "async def simple_stream():\n",
    "    \"\"\"\n",
    "    Generator function that yields data for SSE.\n",
    "    \n",
    "    Key points:\n",
    "    - Must be async\n",
    "    - Use 'yield' not 'return'\n",
    "    - Format: \"data: <content>\\n\\n\"\n",
    "    \"\"\"\n",
    "    for i in range(1, 11):\n",
    "        # Format for SSE: \"data: \" prefix + double newline\n",
    "        message = f\"data: Count: {i}\\n\\n\"\n",
    "        yield message\n",
    "        \n",
    "        # Simulate work (in real apps, this is model generation)\n",
    "        await asyncio.sleep(0.5)\n",
    "    \n",
    "    # Send completion message\n",
    "    yield \"data: [DONE]\\n\\n\"\n",
    "\n",
    "@app.get(\"/stream-simple\")\n",
    "async def stream_simple():\n",
    "    \"\"\"\n",
    "    Simple streaming endpoint that counts from 1 to 10.\n",
    "    \n",
    "    Returns:\n",
    "    StreamingResponse with text/event-stream media type\n",
    "    \n",
    "    Try in browser: http://localhost:8003/stream-simple\n",
    "    You'll see numbers appear one by one!\n",
    "    \"\"\"\n",
    "    logger.info(\"Simple stream requested\")\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        simple_stream(),\n",
    "        media_type=\"text/event-stream\",\n",
    "        headers={\n",
    "            \"Cache-Control\": \"no-cache\",\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Simple streaming endpoint added!\")\n",
    "print(\"\\nüìã Endpoint: GET /stream-simple\")\n",
    "print(\"   ‚Ä¢ Counts from 1 to 10\")\n",
    "print(\"   ‚Ä¢ 0.5 second delay between numbers\")\n",
    "print(\"   ‚Ä¢ Uses SSE format\")\n",
    "print(\"\\nüí° SSE Format Explained:\")\n",
    "print(\"   'data: <message>\\\\n\\\\n'\")\n",
    "print(\"   ‚Üë\")\n",
    "print(\"   Must start with 'data: ' and end with double newline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Understanding the Code:\n",
    "\n",
    "**1. Async Generator Function:**\n",
    "```python\n",
    "async def simple_stream():\n",
    "    yield \"data: message\\n\\n\"\n",
    "```\n",
    "- `async def` - Can use `await` inside\n",
    "- `yield` - Sends data incrementally (not `return`!)\n",
    "- Function becomes a **generator**\n",
    "\n",
    "**2. SSE Format:**\n",
    "```python\n",
    "\"data: Count: 1\\n\\n\"\n",
    " ‚Üë     ‚Üë         ‚Üë\n",
    " |     |         Double newline (required!)\n",
    " |     Your message\n",
    " SSE prefix (required!)\n",
    "```\n",
    "\n",
    "**3. StreamingResponse:**\n",
    "```python\n",
    "StreamingResponse(\n",
    "    simple_stream(),           # Generator function\n",
    "    media_type=\"text/event-stream\"  # SSE content type\n",
    ")\n",
    "```\n",
    "\n",
    "**üí° Think of it like:**\n",
    "- Regular response = Sending a complete email\n",
    "- Streaming response = Live video call\n",
    "\n",
    "**Why `await asyncio.sleep(0.5)`?**\n",
    "- Simulates time-consuming work (like AI generation)\n",
    "- In real apps: model inference, database queries, etc.\n",
    "- Gives other requests a chance to be processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Part 4: LLM-Style Text Streaming\n",
    "\n",
    "Now let's build something more realistic - streaming text generation like ChatGPT!\n",
    "\n",
    "**What we're building:**\n",
    "- Takes a prompt from the user\n",
    "- Generates response word-by-word\n",
    "- Streams to client in real-time\n",
    "\n",
    "**In production:**\n",
    "- You'd use actual LLM (OpenAI, Anthropic, etc.)\n",
    "- We'll simulate it for learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic model for streaming requests\n",
    "print(\"üìã Creating Pydantic models for streaming...\\n\")\n",
    "\n",
    "class StreamRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Input model for streaming text generation.\n",
    "    \"\"\"\n",
    "    prompt: str = Field(\n",
    "        ...,\n",
    "        min_length=1,\n",
    "        max_length=1000,\n",
    "        description=\"User prompt for text generation\",\n",
    "        example=\"Write a short story about a robot\"\n",
    "    )\n",
    "    max_tokens: Optional[int] = Field(\n",
    "        default=100,\n",
    "        ge=10,\n",
    "        le=500,\n",
    "        description=\"Maximum words to generate\"\n",
    "    )\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"prompt\": \"Tell me a joke about programming\",\n",
    "                \"max_tokens\": 50\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Pydantic models created!\")\n",
    "print(\"\\nüí° Model Features:\")\n",
    "print(\"   ‚Ä¢ Prompt validation (length limits)\")\n",
    "print(\"   ‚Ä¢ Optional max_tokens parameter\")\n",
    "print(\"   ‚Ä¢ Example data for Swagger UI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate LLM text generation\n",
    "print(\"ü§ñ Creating LLM-style streaming generator...\\n\")\n",
    "\n",
    "async def generate_text_stream(prompt: str, max_tokens: int = 100):\n",
    "    \"\"\"\n",
    "    Simulates LLM text generation with streaming.\n",
    "    \n",
    "    In production, replace this with actual LLM API calls:\n",
    "    - OpenAI GPT-4\n",
    "    - Anthropic Claude\n",
    "    - Local models (Llama, etc.)\n",
    "    \n",
    "    This simulates word-by-word generation.\n",
    "    \"\"\"\n",
    "    # Simulated responses based on prompt keywords\n",
    "    responses = {\n",
    "        \"joke\": \"Why do programmers prefer dark mode? Because light attracts bugs! üòÑ\",\n",
    "        \"story\": \"Once upon a time, in a world of circuits and code, there lived a curious AI named Claude. Claude loved to help humans learn new things. Every day, Claude would answer questions, write code, and explain complex concepts in simple terms. The end! üìö\",\n",
    "        \"poem\": \"Roses are red, Violets are blue, FastAPI is awesome, And so are you! üåπ\",\n",
    "        \"default\": \"This is a simulated AI response to your prompt. In production, this would be generated by a real language model like GPT-4 or Claude. The response would be contextual and based on your specific prompt. For now, this demonstrates how streaming works! ‚ú®\"\n",
    "    }\n",
    "    \n",
    "    # Select response based on prompt\n",
    "    response_text = responses[\"default\"]\n",
    "    for key in responses:\n",
    "        if key in prompt.lower():\n",
    "            response_text = responses[key]\n",
    "            break\n",
    "    \n",
    "    # Split into words and stream\n",
    "    words = response_text.split()\n",
    "    words = words[:max_tokens]  # Respect max_tokens\n",
    "    \n",
    "    # Send metadata first\n",
    "    yield f\"data: {{\\\"type\\\": \\\"start\\\", \\\"prompt\\\": \\\"{prompt}\\\"}}\\n\\n\"\n",
    "    \n",
    "    # Stream each word\n",
    "    for i, word in enumerate(words):\n",
    "        # Create JSON message\n",
    "        message = {\n",
    "            \"type\": \"token\",\n",
    "            \"content\": word + \" \",\n",
    "            \"index\": i\n",
    "        }\n",
    "        yield f\"data: {json.dumps(message)}\\n\\n\"\n",
    "        \n",
    "        # Simulate generation time (50-150ms per word)\n",
    "        await asyncio.sleep(0.05 + (i % 3) * 0.05)\n",
    "    \n",
    "    # Send completion\n",
    "    completion = {\n",
    "        \"type\": \"done\",\n",
    "        \"total_tokens\": len(words),\n",
    "        \"finish_reason\": \"completed\"\n",
    "    }\n",
    "    yield f\"data: {json.dumps(completion)}\\n\\n\"\n",
    "\n",
    "@app.post(\"/stream-text\")\n",
    "async def stream_text(request: StreamRequest):\n",
    "    \"\"\"\n",
    "    Stream AI-generated text in real-time (ChatGPT-style).\n",
    "    \n",
    "    This endpoint demonstrates LLM streaming:\n",
    "    - Send prompt\n",
    "    - Receive words one-by-one\n",
    "    - See text appear in real-time\n",
    "    \n",
    "    Try prompts with: 'joke', 'story', or 'poem'\n",
    "    \"\"\"\n",
    "    logger.info(f\"Text generation requested: {request.prompt[:50]}...\")\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        generate_text_stream(request.prompt, request.max_tokens),\n",
    "        media_type=\"text/event-stream\",\n",
    "        headers={\n",
    "            \"Cache-Control\": \"no-cache\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"X-Accel-Buffering\": \"no\"  # Disable nginx buffering\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ LLM-style streaming endpoint added!\")\n",
    "print(\"\\nüìã Endpoint: POST /stream-text\")\n",
    "print(\"   ‚Ä¢ Accepts prompt and max_tokens\")\n",
    "print(\"   ‚Ä¢ Streams word-by-word\")\n",
    "print(\"   ‚Ä¢ Returns JSON messages\")\n",
    "print(\"\\nüí° Try prompts with: 'joke', 'story', 'poem'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Understanding LLM Streaming:\n",
    "\n",
    "**Message Types:**\n",
    "\n",
    "```json\n",
    "// 1. Start message\n",
    "{\n",
    "    \"type\": \"start\",\n",
    "    \"prompt\": \"Tell me a joke\"\n",
    "}\n",
    "\n",
    "// 2. Token messages (one per word)\n",
    "{\n",
    "    \"type\": \"token\",\n",
    "    \"content\": \"Why \",\n",
    "    \"index\": 0\n",
    "}\n",
    "\n",
    "// 3. Completion message\n",
    "{\n",
    "    \"type\": \"done\",\n",
    "    \"total_tokens\": 15,\n",
    "    \"finish_reason\": \"completed\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Why JSON in SSE?**\n",
    "- ‚úÖ Structured data\n",
    "- ‚úÖ Easy to parse on client\n",
    "- ‚úÖ Can include metadata (token count, etc.)\n",
    "- ‚úÖ Matches real LLM API formats (OpenAI, Anthropic)\n",
    "\n",
    "**Production Integration:**\n",
    "```python\n",
    "# OpenAI example (actual code)\n",
    "async def openai_stream(prompt):\n",
    "    response = await openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True  # Enable streaming!\n",
    "    )\n",
    "    \n",
    "    async for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            yield f\"data: {chunk.choices[0].delta.content}\\n\\n\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîí Part 5: Environment Variables & Security\n",
    "\n",
    "**Never hardcode secrets in your code!**\n",
    "\n",
    "### ‚ùå BAD Practice:\n",
    "```python\n",
    "API_KEY = \"sk-12345abcdef\"  # NEVER DO THIS!\n",
    "DATABASE_URL = \"postgresql://user:pass@localhost/db\"\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- Gets committed to Git (public!)\n",
    "- Anyone with code access sees secrets\n",
    "- Can't change without code update\n",
    "- Security nightmare!\n",
    "\n",
    "### ‚úÖ GOOD Practice:\n",
    "```python\n",
    "import os\n",
    "API_KEY = os.getenv(\"API_KEY\")  # Secure!\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Secrets stay out of code\n",
    "- ‚úÖ Different values per environment (dev/prod)\n",
    "- ‚úÖ Easy to rotate secrets\n",
    "- ‚úÖ Works with deployment platforms\n",
    "\n",
    "### üí° Using .env Files:\n",
    "\n",
    "**.env file (never commit!):**\n",
    "```\n",
    "API_KEY=your-secret-key-here\n",
    "DATABASE_URL=postgresql://...\n",
    "DEBUG=True\n",
    "```\n",
    "\n",
    "**Load in code:**\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Loads .env automatically!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example .env file\n",
    "print(\"üîí Setting up environment variables...\\n\")\n",
    "\n",
    "# Create .env file\n",
    "env_content = \"\"\"# API Configuration\n",
    "API_KEY=demo-key-12345\n",
    "API_SECRET=demo-secret-67890\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=sqlite:///./test.db\n",
    "\n",
    "# Application Settings\n",
    "DEBUG=True\n",
    "MAX_REQUESTS_PER_MINUTE=60\n",
    "LOG_LEVEL=INFO\n",
    "\n",
    "# External Services\n",
    "OPENAI_API_KEY=your-openai-key-here\n",
    "STRIPE_SECRET_KEY=your-stripe-key-here\n",
    "\"\"\"\n",
    "\n",
    "with open('.env', 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"‚úÖ .env file created!\")\n",
    "print(\"\\nüìÑ Contents:\")\n",
    "print(env_content)\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT:\")\n",
    "print(\"   ‚Ä¢ Add .env to .gitignore\")\n",
    "print(\"   ‚Ä¢ Never commit .env to Git\")\n",
    "print(\"   ‚Ä¢ Use .env.example for documentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and use environment variables\n",
    "print(\"üì• Loading environment variables...\\n\")\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "API_SECRET = os.getenv(\"API_SECRET\")\n",
    "DEBUG = os.getenv(\"DEBUG\", \"False\") == \"True\"  # Convert string to boolean\n",
    "MAX_REQUESTS = int(os.getenv(\"MAX_REQUESTS_PER_MINUTE\", \"60\"))\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded!\")\n",
    "print(\"\\nüîë Configuration:\")\n",
    "print(f\"   ‚Ä¢ API_KEY: {API_KEY[:8]}*** (hidden)\")\n",
    "print(f\"   ‚Ä¢ DEBUG: {DEBUG}\")\n",
    "print(f\"   ‚Ä¢ MAX_REQUESTS: {MAX_REQUESTS}\")\n",
    "print(\"\\nüí° In production:\")\n",
    "print(\"   ‚Ä¢ Set env vars in hosting platform\")\n",
    "print(\"   ‚Ä¢ Use secrets management (AWS Secrets Manager, etc.)\")\n",
    "print(\"   ‚Ä¢ Never print actual values in logs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add endpoint that uses environment variables\n",
    "print(\"üîê Adding secure endpoint with API key validation...\\n\")\n",
    "\n",
    "@app.get(\"/secure-endpoint\")\n",
    "async def secure_endpoint(api_key: str):\n",
    "    \"\"\"\n",
    "    Secure endpoint that requires API key.\n",
    "    \n",
    "    In production:\n",
    "    - Use proper authentication (OAuth, JWT)\n",
    "    - Store keys in database\n",
    "    - Add rate limiting\n",
    "    - Use HTTPS only\n",
    "    \n",
    "    Query parameter:\n",
    "    - api_key: Your API key from .env file\n",
    "    \"\"\"\n",
    "    # Validate API key\n",
    "    if api_key != API_KEY:\n",
    "        logger.warning(f\"Invalid API key attempt: {api_key[:8]}***\")\n",
    "        raise HTTPException(\n",
    "            status_code=401,  # Unauthorized\n",
    "            detail=\"Invalid API key\"\n",
    "        )\n",
    "    \n",
    "    logger.info(\"Secure endpoint accessed successfully\")\n",
    "    \n",
    "    return {\n",
    "        \"message\": \"Access granted!\",\n",
    "        \"user\": \"authenticated\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.get(\"/config\")\n",
    "async def get_config():\n",
    "    \"\"\"\n",
    "    Get non-sensitive configuration.\n",
    "    \n",
    "    NEVER expose secrets in public endpoints!\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"debug_mode\": DEBUG,\n",
    "        \"max_requests_per_minute\": MAX_REQUESTS,\n",
    "        \"log_level\": os.getenv(\"LOG_LEVEL\", \"INFO\"),\n",
    "        \"api_version\": \"3.0.0\"\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Secure endpoints added!\")\n",
    "print(\"\\nüìã Endpoints:\")\n",
    "print(\"   ‚Ä¢ GET /secure-endpoint?api_key=XXX\")\n",
    "print(\"   ‚Ä¢ GET /config (public configuration)\")\n",
    "print(\"\\nüí° Try:\")\n",
    "print(f\"   ‚Ä¢ Valid key: {API_KEY}\")\n",
    "print(\"   ‚Ä¢ Invalid key: wrong-key (will fail!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö° Part 6: Rate Limiting (Prevent Abuse)\n",
    "\n",
    "**Why rate limiting?**\n",
    "- Prevent abuse (DDoS attacks)\n",
    "- Fair usage across users\n",
    "- Protect server resources\n",
    "- Comply with upstream API limits\n",
    "\n",
    "**Example scenario:**\n",
    "```\n",
    "Without rate limiting:\n",
    "Attacker sends 10,000 requests/second ‚Üí Server crashes üí•\n",
    "\n",
    "With rate limiting:\n",
    "Allow 60 requests/minute per IP ‚Üí Attacker blocked ‚úÖ\n",
    "Normal users ‚Üí No impact üòä\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup rate limiting\n",
    "print(\"‚ö° Configuring rate limiting...\\n\")\n",
    "\n",
    "# Initialize limiter\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
    "\n",
    "@app.get(\"/limited\")\n",
    "@limiter.limit(\"5/minute\")  # 5 requests per minute\n",
    "async def limited_endpoint(request: Request):\n",
    "    \"\"\"\n",
    "    Rate-limited endpoint - max 5 requests per minute.\n",
    "    \n",
    "    Try calling this multiple times quickly!\n",
    "    After 5 requests, you'll get a 429 error.\n",
    "    \n",
    "    Rate limit resets after 1 minute.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"message\": \"Success!\",\n",
    "        \"note\": \"This endpoint is rate-limited to 5 requests/minute\",\n",
    "        \"requests_remaining\": \"Check X-RateLimit-Remaining header\"\n",
    "    }\n",
    "\n",
    "@app.post(\"/stream-text-limited\")\n",
    "@limiter.limit(\"10/minute\")  # More generous for streaming\n",
    "async def stream_text_limited(request: Request, stream_req: StreamRequest):\n",
    "    \"\"\"\n",
    "    Rate-limited streaming endpoint.\n",
    "    Max 10 requests per minute.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Rate-limited stream requested: {stream_req.prompt[:30]}...\")\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        generate_text_stream(stream_req.prompt, stream_req.max_tokens),\n",
    "        media_type=\"text/event-stream\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Rate limiting configured!\")\n",
    "print(\"\\n‚ö° Limits:\")\n",
    "print(\"   ‚Ä¢ /limited: 5 requests/minute\")\n",
    "print(\"   ‚Ä¢ /stream-text-limited: 10 requests/minute\")\n",
    "print(\"\\nüí° Response headers:\")\n",
    "print(\"   ‚Ä¢ X-RateLimit-Limit: Total allowed\")\n",
    "print(\"   ‚Ä¢ X-RateLimit-Remaining: Requests left\")\n",
    "print(\"   ‚Ä¢ X-RateLimit-Reset: When limit resets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 7: Start Server & Test Everything\n",
    "\n",
    "Let's run our advanced API and test all features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run server\n",
    "def run_server(app, port=8000):\n",
    "    def start_server():\n",
    "        uvicorn.run(app, host=\"127.0.0.1\", port=port, log_level=\"info\")\n",
    "    \n",
    "    thread = Thread(target=start_server, daemon=True)\n",
    "    thread.start()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    print(f\"‚úÖ Server started!\")\n",
    "    print(f\"\\nüåê Advanced FastAPI running at:\")\n",
    "    print(f\"   ‚Ä¢ Main URL: http://127.0.0.1:{port}\")\n",
    "    print(f\"   ‚Ä¢ Swagger UI: http://127.0.0.1:{port}/docs\")\n",
    "    print(f\"\\nüî• Try these endpoints:\")\n",
    "    print(f\"   ‚Ä¢ Streaming: http://127.0.0.1:{port}/stream-simple\")\n",
    "    print(f\"   ‚Ä¢ LLM Stream: POST to /stream-text\")\n",
    "    print(f\"   ‚Ä¢ Secure: /secure-endpoint?api_key={API_KEY}\")\n",
    "    print(f\"   ‚Ä¢ Rate Limited: /limited (try 6 times!)\")\n",
    "    \n",
    "    return thread\n",
    "\n",
    "# Start server\n",
    "print(\"üöÄ Starting Advanced FastAPI Server...\\n\")\n",
    "server_thread = run_server(app, port=8003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Testing in Browser:\n",
    "\n",
    "**1. Test Simple Streaming:**\n",
    "- Open: http://127.0.0.1:8003/stream-simple\n",
    "- Watch numbers appear one by one!\n",
    "\n",
    "**2. Test in Swagger UI:**\n",
    "- Open: http://127.0.0.1:8003/docs\n",
    "- Try POST /stream-text:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Tell me a joke\",\n",
    "    \"max_tokens\": 50\n",
    "  }\n",
    "  ```\n",
    "- Watch the streaming response!\n",
    "\n",
    "**3. Test Security:**\n",
    "- Try /secure-endpoint with correct API key\n",
    "- Try with wrong API key (should fail!)\n",
    "\n",
    "**4. Test Rate Limiting:**\n",
    "- Call /limited 6 times quickly\n",
    "- 6th request should return 429 error\n",
    "\n",
    "**üí° Pro Tip:** Open browser DevTools (F12) ‚Üí Network tab to see SSE messages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test endpoints programmatically\n",
    "print(\"üß™ Testing Advanced API Features...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: Configuration endpoint\n",
    "print(\"\\n1Ô∏è‚É£ Testing Configuration Endpoint\")\n",
    "print(\"-\" * 70)\n",
    "response = requests.get(\"http://127.0.0.1:8003/config\")\n",
    "config = response.json()\n",
    "print(\"‚úÖ Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# Test 2: Secure endpoint with valid key\n",
    "print(\"\\n2Ô∏è‚É£ Testing Secure Endpoint (Valid Key)\")\n",
    "print(\"-\" * 70)\n",
    "response = requests.get(\n",
    "    f\"http://127.0.0.1:8003/secure-endpoint?api_key={API_KEY}\"\n",
    ")\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úÖ Authentication successful!\")\n",
    "    print(f\"   Response: {response.json()}\")\n",
    "\n",
    "# Test 3: Secure endpoint with invalid key\n",
    "print(\"\\n3Ô∏è‚É£ Testing Secure Endpoint (Invalid Key)\")\n",
    "print(\"-\" * 70)\n",
    "response = requests.get(\n",
    "    \"http://127.0.0.1:8003/secure-endpoint?api_key=wrong-key\"\n",
    ")\n",
    "if response.status_code == 401:\n",
    "    print(\"‚úÖ Authentication failed as expected!\")\n",
    "    print(f\"   Error: {response.json()['detail']}\")\n",
    "\n",
    "# Test 4: Rate limiting\n",
    "print(\"\\n4Ô∏è‚É£ Testing Rate Limiting\")\n",
    "print(\"-\" * 70)\n",
    "print(\"Sending 6 requests to /limited (limit is 5/minute)...\")\n",
    "for i in range(1, 7):\n",
    "    response = requests.get(\"http://127.0.0.1:8003/limited\")\n",
    "    if response.status_code == 200:\n",
    "        print(f\"   ‚úÖ Request {i}: Success\")\n",
    "    elif response.status_code == 429:\n",
    "        print(f\"   üö´ Request {i}: Rate limited!\")\n",
    "        print(f\"      Error: {response.json()['detail']}\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n‚úÖ All tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 8: Production Best Practices Checklist\n",
    "\n",
    "Before deploying to production, ensure you've done all of these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production checklist generator\n",
    "print(\"üìã Generating Production Deployment Checklist...\\n\")\n",
    "\n",
    "checklist = \"\"\"\n",
    "SECURITY CHECKLIST\n",
    "===============================================================\n",
    "[ ] Environment variables used for all secrets\n",
    "[ ] .env file in .gitignore\n",
    "[ ] HTTPS enabled (TLS certificates)\n",
    "[ ] CORS configured with specific origins (not \"*\")\n",
    "[ ] API authentication implemented (API keys/OAuth)\n",
    "[ ] Rate limiting configured\n",
    "[ ] Input validation with Pydantic\n",
    "[ ] SQL injection prevention (use ORMs)\n",
    "[ ] Error messages don't expose internals\n",
    "[ ] Security headers configured\n",
    "\n",
    "MONITORING & LOGGING\n",
    "===============================================================\n",
    "[ ] Structured logging implemented\n",
    "[ ] Request/response logging\n",
    "[ ] Error tracking (Sentry, etc.)\n",
    "[ ] Performance monitoring (latency, throughput)\n",
    "[ ] Resource monitoring (CPU, memory)\n",
    "[ ] Alert system for critical errors\n",
    "[ ] Model performance tracking\n",
    "[ ] API usage analytics\n",
    "\n",
    "PERFORMANCE\n",
    "===============================================================\n",
    "[ ] Models loaded at startup (singleton pattern)\n",
    "[ ] Database connection pooling\n",
    "[ ] Caching implemented (Redis/in-memory)\n",
    "[ ] Async operations for I/O\n",
    "[ ] Response compression (gzip)\n",
    "[ ] CDN for static assets\n",
    "[ ] Load balancing configured\n",
    "[ ] Auto-scaling setup\n",
    "\n",
    "TESTING\n",
    "===============================================================\n",
    "[ ] Unit tests written (>80% coverage)\n",
    "[ ] Integration tests for all endpoints\n",
    "[ ] Load testing performed\n",
    "[ ] Security testing done\n",
    "[ ] Error handling tested\n",
    "[ ] Edge cases covered\n",
    "\n",
    "DOCUMENTATION\n",
    "===============================================================\n",
    "[ ] README with setup instructions\n",
    "[ ] API documentation complete (Swagger)\n",
    "[ ] Code comments for complex logic\n",
    "[ ] Architecture diagram\n",
    "[ ] Deployment guide\n",
    "[ ] Troubleshooting guide\n",
    "[ ] Example requests/responses\n",
    "[ ] Changelog maintained\n",
    "\n",
    "DEPLOYMENT\n",
    "===============================================================\n",
    "[ ] Docker container created\n",
    "[ ] CI/CD pipeline configured\n",
    "[ ] Automated testing in pipeline\n",
    "[ ] Staging environment setup\n",
    "[ ] Rollback strategy defined\n",
    "[ ] Health check endpoint\n",
    "[ ] Graceful shutdown handling\n",
    "[ ] Environment-specific configs\n",
    "\n",
    "MAINTENANCE\n",
    "===============================================================\n",
    "[ ] Dependency updates scheduled\n",
    "[ ] Security patches process\n",
    "[ ] Backup strategy implemented\n",
    "[ ] Disaster recovery plan\n",
    "[ ] On-call rotation defined\n",
    "[ ] Incident response procedures\n",
    "\n",
    "COMMUNICATION\n",
    "===============================================================\n",
    "[ ] Status page for API\n",
    "[ ] Change log published\n",
    "[ ] User notification system\n",
    "[ ] Support channel established\n",
    "[ ] SLA defined and documented\n",
    "\"\"\"\n",
    "\n",
    "print(checklist)\n",
    "\n",
    "# Save to file with UTF-8 encoding (explicitly)\n",
    "with open('production_checklist.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(checklist)\n",
    "\n",
    "print(\"\\n‚úÖ Checklist saved to: production_checklist.txt\")\n",
    "print(\"\\nüí° Review this before every production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 9: Beginner Challenge\n",
    "\n",
    "### üèÜ Your Mission:\n",
    "\n",
    "Enhance the API with production-ready features!\n",
    "\n",
    "### üìã Requirements:\n",
    "\n",
    "**1. Add Health Check Endpoint**\n",
    "- Create `/health` endpoint\n",
    "- Check: Server status, model loaded, database connection\n",
    "- Return: Status, uptime, version\n",
    "\n",
    "**2. Add Request Logging Middleware**\n",
    "- Log all incoming requests\n",
    "- Include: Method, path, IP, timestamp\n",
    "- Log response time\n",
    "\n",
    "**3. Add Custom Error Handler**\n",
    "- Catch all exceptions\n",
    "- Return consistent error format\n",
    "- Log errors with full stack trace\n",
    "\n",
    "### üí° Hints:\n",
    "\n",
    "```python\n",
    "# Hint 1: Health check\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"uptime\": calculate_uptime(),\n",
    "        \"version\": \"3.0.0\"\n",
    "    }\n",
    "\n",
    "# Hint 2: Request logging middleware\n",
    "@app.middleware(\"http\")\n",
    "async def log_requests(request: Request, call_next):\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    duration = time.time() - start_time\n",
    "    logger.info(f\"{request.method} {request.url.path} - {duration:.2f}s\")\n",
    "    return response\n",
    "\n",
    "# Hint 3: Error handler\n",
    "@app.exception_handler(Exception)\n",
    "async def global_exception_handler(request: Request, exc: Exception):\n",
    "    logger.error(f\"Error: {exc}\")\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\"error\": \"Internal server error\"}\n",
    "    )\n",
    "```\n",
    "\n",
    "### üéØ Expected Outcome:\n",
    "\n",
    "- Health check at `/health` shows system status\n",
    "- All requests logged with timing\n",
    "- Errors handled gracefully\n",
    "\n",
    "### üåü Bonus Challenges:\n",
    "\n",
    "1. **Add metrics endpoint** `/metrics`:\n",
    "   - Total requests\n",
    "   - Average response time\n",
    "   - Error rate\n",
    "\n",
    "2. **Add API versioning**:\n",
    "   - `/v1/predict` and `/v2/predict`\n",
    "   - Different logic per version\n",
    "\n",
    "3. **Add request ID tracking**:\n",
    "   - Generate unique ID per request\n",
    "   - Include in logs\n",
    "   - Return in response headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Implement the challenge requirements\n",
    "\n",
    "# Step 1: Add health check endpoint\n",
    "\n",
    "# Step 2: Add logging middleware\n",
    "\n",
    "# Step 3: Add error handler\n",
    "\n",
    "# Restart server and test!\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Summary - What We Learned Today\n",
    "\n",
    "### 1. Server-Sent Events (SSE) üì°\n",
    "- **Streaming responses** - Send data incrementally\n",
    "- **SSE format** - `\"data: <content>\\\\n\\\\n\"`\n",
    "- **StreamingResponse** - FastAPI's streaming class\n",
    "- **Async generators** - Use `yield` with `async def`\n",
    "- **Perfect for AI** - ChatGPT-style text generation\n",
    "\n",
    "### 2. Environment Variables üîí\n",
    "- **Never hardcode secrets** - Use `os.getenv()`\n",
    "- **.env files** - Store configuration securely\n",
    "- **python-dotenv** - Load .env automatically\n",
    "- **.gitignore** - Never commit secrets!\n",
    "- **Different per environment** - Dev vs Production\n",
    "\n",
    "### 3. Rate Limiting ‚ö°\n",
    "- **Prevent abuse** - Limit requests per user/IP\n",
    "- **slowapi library** - Easy rate limiting\n",
    "- **Decorator pattern** - `@limiter.limit(\"5/minute\")`\n",
    "- **Response headers** - X-RateLimit-* headers\n",
    "- **Fair usage** - Protect resources\n",
    "\n",
    "### 4. API Documentation üìö\n",
    "- **Custom API info** - Title, description, version\n",
    "- **Endpoint documentation** - Summaries and descriptions\n",
    "- **Response models** - Pydantic for structure\n",
    "- **Example data** - Helps users understand\n",
    "- **Automatic Swagger** - All from code!\n",
    "\n",
    "### 5. Security Best Practices üîê\n",
    "- **Authentication** - API keys, OAuth\n",
    "- **HTTPS only** - Encrypt all traffic\n",
    "- **CORS properly** - Specific origins in production\n",
    "- **Input validation** - Never trust user input\n",
    "- **Error handling** - Don't expose internals\n",
    "\n",
    "### 6. Logging & Monitoring üìä\n",
    "- **Structured logging** - Timestamp, level, message\n",
    "- **Request logging** - Track all API calls\n",
    "- **Error logging** - Debug production issues\n",
    "- **Performance metrics** - Response times, errors\n",
    "\n",
    "### 7. Production Readiness üöÄ\n",
    "- **Comprehensive checklist** - 50+ items to verify\n",
    "- **Testing** - Unit, integration, load tests\n",
    "- **Documentation** - README, API docs, guides\n",
    "- **Deployment** - Docker, CI/CD, monitoring\n",
    "- **Maintenance** - Updates, backups, incidents\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "‚úÖ **Streaming enhances user experience**\n",
    "- Real-time feedback vs waiting\n",
    "- Essential for AI applications\n",
    "\n",
    "‚úÖ **Security is not optional**\n",
    "- Environment variables for secrets\n",
    "- Rate limiting prevents abuse\n",
    "- HTTPS is mandatory in production\n",
    "\n",
    "‚úÖ **Good logging saves hours of debugging**\n",
    "- Log everything important\n",
    "- Include context (request IDs, etc.)\n",
    "- Monitor in production\n",
    "\n",
    "‚úÖ **Documentation is for everyone**\n",
    "- Future you\n",
    "- Team members\n",
    "- API consumers\n",
    "\n",
    "‚úÖ **Production is different from development**\n",
    "- Different secrets\n",
    "- Stricter security\n",
    "- More monitoring\n",
    "- Better error handling\n",
    "\n",
    "‚úÖ **Checklists prevent mistakes**\n",
    "- Review before every deployment\n",
    "- Don't skip items\n",
    "- Add project-specific checks\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips for Production\n",
    "\n",
    "1. **Test in Staging First**\n",
    "   - Identical to production\n",
    "   - Catch issues before users do\n",
    "\n",
    "2. **Monitor Everything**\n",
    "   - Logs, metrics, alerts\n",
    "   - Know issues before users report them\n",
    "\n",
    "3. **Have a Rollback Plan**\n",
    "   - Things will go wrong\n",
    "   - Quick rollback saves users\n",
    "\n",
    "4. **Document Everything**\n",
    "   - Architecture decisions\n",
    "   - Deployment procedures\n",
    "   - Troubleshooting guides\n",
    "\n",
    "5. **Security is Ongoing**\n",
    "   - Regular dependency updates\n",
    "   - Security audits\n",
    "   - Penetration testing\n",
    "\n",
    "6. **Performance Matters**\n",
    "   - Users expect fast responses\n",
    "   - Monitor and optimize\n",
    "   - Load test before launch\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Week 17 Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "**Day 1: FastAPI Fundamentals ‚úÖ**\n",
    "- Built Calculator API\n",
    "- Learned REST principles\n",
    "- Mastered Swagger UI\n",
    "\n",
    "**Day 2: ML Model APIs ‚úÖ**\n",
    "- Deployed sentiment analysis model\n",
    "- Handled file uploads\n",
    "- Enabled CORS\n",
    "\n",
    "**Day 3: Production Ready ‚úÖ**\n",
    "- Implemented streaming (SSE)\n",
    "- Secured with environment variables\n",
    "- Added rate limiting\n",
    "- Production checklist\n",
    "\n",
    "### üèÜ You Can Now:\n",
    "- ‚úÖ Build complete REST APIs with FastAPI\n",
    "- ‚úÖ Deploy ML models as endpoints\n",
    "- ‚úÖ Stream responses in real-time\n",
    "- ‚úÖ Secure APIs properly\n",
    "- ‚úÖ Handle production workloads\n",
    "- ‚úÖ Monitor and debug effectively\n",
    "- ‚úÖ Deploy to production with confidence\n",
    "\n",
    "**You're now a FastAPI expert! üöÄ**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**Week 18: Docker & CI/CD**\n",
    "- Containerize your APIs\n",
    "- Automated testing\n",
    "- Continuous deployment\n",
    "\n",
    "**Week 19: MLOps & Automation**\n",
    "- Model versioning\n",
    "- Automated retraining\n",
    "- Monitoring ML performance\n",
    "\n",
    "**Week 20: AWS Deployment**\n",
    "- Deploy to cloud\n",
    "- Auto-scaling\n",
    "- Production architecture\n",
    "\n",
    "---\n",
    "\n",
    "## üéä Congratulations!\n",
    "\n",
    "You've completed Week 17 - API Development with FastAPI!\n",
    "\n",
    "**From zero to production in 3 days:**\n",
    "- Day 1: Learned the basics\n",
    "- Day 2: Added ML models\n",
    "- Day 3: Made it production-ready\n",
    "\n",
    "**This knowledge is immediately applicable:**\n",
    "- Deploy your bootcamp projects as APIs\n",
    "- Build portfolio projects\n",
    "- Prepare for job interviews\n",
    "- Freelance API development\n",
    "\n",
    "**Keep practicing and building! üí™**\n",
    "\n",
    "**See you in Week 18! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}