{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ AI Bootcamp Practice Exercises\n",
    "## Week 4 - Day 1: Data Formats and Storage\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Topics Covered Today:\n",
    "- Overview of common data formats (CSV, JSON, Parquet, SQL tables)\n",
    "- Reading and writing files efficiently in Python\n",
    "- Basics of relational databases and SQL queries\n",
    "- Concept of data schemas and normalization\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Instructions:\n",
    "- Read each **Example** carefully before attempting exercises\n",
    "- Fill in the **TODO** sections with your code\n",
    "- Run each cell to check your output\n",
    "- Expected outputs are provided for reference\n",
    "- Ask for help if you get stuck!\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started! ğŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 1: Introduction & Setup\n",
    "\n",
    "**Data Engineering for AI** focuses on:\n",
    "- Collecting and storing data efficiently\n",
    "- Transforming data into usable formats\n",
    "- Building pipelines for data flow\n",
    "- Ensuring data quality and consistency\n",
    "\n",
    "**Why Data Formats Matter?**\n",
    "- ğŸ“ **Storage Efficiency**: Different formats use space differently\n",
    "- âš¡ **Read/Write Speed**: Some formats are faster than others\n",
    "- ğŸ”„ **Interoperability**: Exchange data between systems\n",
    "- ğŸ¯ **Use Case**: Right format for the right job\n",
    "\n",
    "**Common Data Formats:**\n",
    "- **CSV**: Simple, human-readable, widely supported\n",
    "- **JSON**: Hierarchical, web APIs, configuration files\n",
    "- **Parquet**: Columnar, compressed, analytics\n",
    "- **SQL**: Structured, relational, transactional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.2\n",
      "NumPy version: 1.26.4\n",
      "SQLite version: 3.45.3\n",
      "\n",
      "Data directory: C:\\Users\\Zigron\\Downloads\\data\n",
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SQLite version: {sqlite3.sqlite_version}\")\n",
    "print(f\"\\nData directory: {data_dir.absolute()}\")\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 2: Basic Exercises\n",
    "\n",
    "### Exercise 1: Working with CSV Files\n",
    "**Learn:** CSV (Comma-Separated Values) is the most common format for tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name  Age Department  Salary\n",
      "0    Alice   25         IT   70000\n",
      "1      Bob   30         HR   60000\n",
      "2  Charlie   35    Finance   75000\n",
      "3    David   28         IT   72000\n",
      "4      Eve   32         HR   65000\n",
      "\n",
      "âœ… Saved to: data\\employees.csv\n",
      "File size: 126 bytes\n",
      "\n",
      "Read from CSV:\n",
      "      Name  Age Department  Salary\n",
      "0    Alice   25         IT   70000\n",
      "1      Bob   30         HR   60000\n",
      "2  Charlie   35    Finance   75000\n",
      "3    David   28         IT   72000\n",
      "4      Eve   32         HR   65000\n",
      "\n",
      "Read specific columns:\n",
      "      Name  Salary\n",
      "0    Alice   70000\n",
      "1      Bob   60000\n",
      "2  Charlie   75000\n",
      "3    David   72000\n",
      "4      Eve   65000\n"
     ]
    }
   ],
   "source": [
    "# Example: Reading and writing CSV files\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'Department': ['IT', 'HR', 'Finance', 'IT', 'HR'],\n",
    "    'Salary': [70000, 60000, 75000, 72000, 65000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Write to CSV\n",
    "csv_path = data_dir / 'employees.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nâœ… Saved to: {csv_path}\")\n",
    "print(f\"File size: {csv_path.stat().st_size} bytes\")\n",
    "\n",
    "# Read from CSV\n",
    "df_read = pd.read_csv(csv_path)\n",
    "print(\"\\nRead from CSV:\")\n",
    "print(df_read)\n",
    "\n",
    "# Read specific columns only\n",
    "df_partial = pd.read_csv(csv_path, usecols=['Name', 'Salary'])\n",
    "print(\"\\nRead specific columns:\")\n",
    "print(df_partial)\n",
    "\n",
    "# Expected Output:\n",
    "# âœ… Saved to: data/employees.csv\n",
    "# File size: 124 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a DataFrame with product data:\n",
    "# Products: ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones']\n",
    "# Prices: [1200, 25, 75, 300, 150]\n",
    "# Stock: [15, 100, 80, 25, 50]\n",
    "# Category: ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories']\n",
    "# TODO: Save it to 'data/products.csv'\n",
    "# TODO: Read it back and display\n",
    "# TODO: Read only 'Products' and 'Prices' columns\n",
    "# TODO: Check the file size\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Create DataFrame\n",
    "products_data = # Your code here\n",
    "\n",
    "products_df = # Your code here\n",
    "\n",
    "print(\"Products DataFrame:\")\n",
    "print(products_df)\n",
    "\n",
    "# Save to CSV\n",
    "products_csv = # Your code here\n",
    "# Your code here to save\n",
    "\n",
    "print(f\"\\nâœ… Saved to: {products_csv}\")\n",
    "print(f\"File size: {products_csv.stat().st_size} bytes\")\n",
    "\n",
    "# Read back\n",
    "products_read = # Your code here\n",
    "print(\"\\nRead from CSV:\")\n",
    "print(products_read)\n",
    "\n",
    "# Read specific columns\n",
    "products_partial = # Your code here\n",
    "print(\"\\nSpecific columns:\")\n",
    "print(products_partial)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2: Working with JSON Files\n",
    "**Learn:** JSON (JavaScript Object Notation) is ideal for hierarchical/nested data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Working with JSON\n",
    "\n",
    "# Create nested data structure\n",
    "data = {\n",
    "    'company': 'TechCorp',\n",
    "    'employees': [\n",
    "        {'name': 'Alice', 'age': 25, 'skills': ['Python', 'SQL']},\n",
    "        {'name': 'Bob', 'age': 30, 'skills': ['Java', 'JavaScript']},\n",
    "        {'name': 'Charlie', 'age': 35, 'skills': ['Python', 'Machine Learning']}\n",
    "    ],\n",
    "    'founded': 2010,\n",
    "    'active': True\n",
    "}\n",
    "\n",
    "print(\"Original data structure:\")\n",
    "print(json.dumps(data, indent=2))\n",
    "\n",
    "# Write to JSON file\n",
    "json_path = data_dir / 'company.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "print(f\"\\nâœ… Saved to: {json_path}\")\n",
    "print(f\"File size: {json_path.stat().st_size} bytes\")\n",
    "\n",
    "# Read from JSON file\n",
    "with open(json_path, 'r') as f:\n",
    "    data_read = json.load(f)\n",
    "\n",
    "print(\"\\nRead from JSON:\")\n",
    "print(f\"Company: {data_read['company']}\")\n",
    "print(f\"Number of employees: {len(data_read['employees'])}\")\n",
    "print(f\"First employee: {data_read['employees'][0]['name']}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_from_json = pd.DataFrame(data_read['employees'])\n",
    "print(\"\\nAs DataFrame:\")\n",
    "print(df_from_json)\n",
    "\n",
    "# Expected Output:\n",
    "# Company: TechCorp\n",
    "# Number of employees: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a nested JSON structure for a school:\n",
    "# school_name: 'AI Academy'\n",
    "# courses: [\n",
    "#   {name: 'Machine Learning', duration: 12, students: 30, instructor: 'Dr. Smith'},\n",
    "#   {name: 'Deep Learning', duration: 16, students: 25, instructor: 'Dr. Johnson'},\n",
    "#   {name: 'Data Science', duration: 10, students: 35, instructor: 'Dr. Brown'}\n",
    "# ]\n",
    "# established: 2020\n",
    "# online: True\n",
    "# TODO: Save it to 'data/school.json'\n",
    "# TODO: Read it back\n",
    "# TODO: Print the name of the course with most students\n",
    "# TODO: Convert courses to a DataFrame\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Create data structure\n",
    "school_data = # Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"School data:\")\n",
    "print(json.dumps(school_data, indent=2))\n",
    "\n",
    "# Save to JSON\n",
    "school_json = # Your code here\n",
    "# Your code here to save\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… Saved to: {school_json}\")\n",
    "\n",
    "# Read back\n",
    "with open(school_json, 'r') as f:\n",
    "    school_read = # Your code here\n",
    "\n",
    "print(\"\\nSchool info:\")\n",
    "print(f\"Name: {school_read['school_name']}\")\n",
    "print(f\"Established: {school_read['established']}\")\n",
    "print(f\"Number of courses: {len(school_read['courses'])}\")\n",
    "\n",
    "# Find course with most students\n",
    "max_students_course = # Your code here\n",
    "print(f\"\\nCourse with most students: {max_students_course}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "courses_df = # Your code here\n",
    "print(\"\\nCourses DataFrame:\")\n",
    "print(courses_df)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 3: Comparing File Formats\n",
    "**Learn:** Different formats have different characteristics and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comparing CSV, JSON, and Parquet\n",
    "\n",
    "# Create larger dataset for comparison\n",
    "np.random.seed(42)\n",
    "n_rows = 10000\n",
    "\n",
    "df_large = pd.DataFrame({\n",
    "    'id': range(n_rows),\n",
    "    'name': [f'Person_{i}' for i in range(n_rows)],\n",
    "    'age': np.random.randint(18, 80, n_rows),\n",
    "    'salary': np.random.randint(30000, 150000, n_rows),\n",
    "    'department': np.random.choice(['IT', 'HR', 'Sales', 'Finance'], n_rows),\n",
    "    'experience': np.random.randint(0, 30, n_rows)\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {n_rows} rows, {len(df_large.columns)} columns\")\n",
    "print(df_large.head())\n",
    "\n",
    "# Save in different formats and compare\n",
    "import time\n",
    "\n",
    "formats = {}\n",
    "\n",
    "# CSV\n",
    "csv_file = data_dir / 'large_data.csv'\n",
    "start = time.time()\n",
    "df_large.to_csv(csv_file, index=False)\n",
    "csv_write_time = time.time() - start\n",
    "csv_size = csv_file.stat().st_size\n",
    "formats['CSV'] = {'write_time': csv_write_time, 'size': csv_size}\n",
    "\n",
    "# JSON\n",
    "json_file = data_dir / 'large_data.json'\n",
    "start = time.time()\n",
    "df_large.to_json(json_file, orient='records', indent=2)\n",
    "json_write_time = time.time() - start\n",
    "json_size = json_file.stat().st_size\n",
    "formats['JSON'] = {'write_time': json_write_time, 'size': json_size}\n",
    "\n",
    "# Parquet (requires pyarrow or fastparquet)\n",
    "try:\n",
    "    parquet_file = data_dir / 'large_data.parquet'\n",
    "    start = time.time()\n",
    "    df_large.to_parquet(parquet_file, index=False)\n",
    "    parquet_write_time = time.time() - start\n",
    "    parquet_size = parquet_file.stat().st_size\n",
    "    formats['Parquet'] = {'write_time': parquet_write_time, 'size': parquet_size}\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Parquet not available ({e})\")\n",
    "    print(\"Install with: pip install pyarrow\")\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FORMAT COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "for format_name, metrics in formats.items():\n",
    "    print(f\"\\n{format_name}:\")\n",
    "    print(f\"  Write time: {metrics['write_time']:.4f} seconds\")\n",
    "    print(f\"  File size: {metrics['size']:,} bytes ({metrics['size']/1024:.2f} KB)\")\n",
    "\n",
    "# Expected Output:\n",
    "# CSV: ~250 KB, fast write\n",
    "# JSON: ~400 KB, slower write\n",
    "# Parquet: ~100 KB, very fast (compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a dataset with 5000 rows and these columns:\n",
    "# - transaction_id (sequential)\n",
    "# - customer_id (random 1-1000)\n",
    "# - amount (random 10-1000)\n",
    "# - product (random choice from ['A', 'B', 'C', 'D', 'E'])\n",
    "# - date (random dates in 2024)\n",
    "# TODO: Save in CSV, JSON formats\n",
    "# TODO: Try Parquet if available\n",
    "# TODO: Compare file sizes and write times\n",
    "# TODO: Read each format and compare read times\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import time\n",
    "\n",
    "# Create dataset\n",
    "np.random.seed(100)\n",
    "n = 5000\n",
    "\n",
    "transactions_df = # Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Transactions dataset:\")\n",
    "print(transactions_df.head())\n",
    "print(f\"\\nShape: {transactions_df.shape}\")\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# CSV\n",
    "csv_file = data_dir / 'transactions.csv'\n",
    "start = time.time()\n",
    "# Your code here to save\n",
    "csv_write = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "# Your code here to read\n",
    "csv_read = time.time() - start\n",
    "\n",
    "results['CSV'] = {\n",
    "    'size': csv_file.stat().st_size,\n",
    "    'write_time': csv_write,\n",
    "    'read_time': csv_read\n",
    "}\n",
    "\n",
    "# JSON\n",
    "json_file = data_dir / 'transactions.json'\n",
    "start = time.time()\n",
    "# Your code here to save\n",
    "json_write = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "# Your code here to read\n",
    "json_read = time.time() - start\n",
    "\n",
    "results['JSON'] = {\n",
    "    'size': json_file.stat().st_size,\n",
    "    'write_time': json_write,\n",
    "    'read_time': json_read\n",
    "}\n",
    "\n",
    "# Parquet (optional)\n",
    "try:\n",
    "    parquet_file = data_dir / 'transactions.parquet'\n",
    "    # Your code here\n",
    "    pass\n",
    "except:\n",
    "    print(\"\\nParquet not available\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Format':<10} {'Size (KB)':<15} {'Write (s)':<15} {'Read (s)':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for fmt, metrics in results.items():\n",
    "    print(f\"{fmt:<10} {metrics['size']/1024:<15.2f} {metrics['write_time']:<15.4f} {metrics['read_time']:<15.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 4: Introduction to SQLite\n",
    "**Learn:** SQLite is a lightweight database perfect for local storage and learning SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating and querying SQLite database\n",
    "\n",
    "# Create database connection\n",
    "db_path = data_dir / 'company.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS employees (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        name TEXT NOT NULL,\n",
    "        age INTEGER,\n",
    "        department TEXT,\n",
    "        salary REAL\n",
    "    )\n",
    "''')\n",
    "\n",
    "print(\"âœ… Table 'employees' created\")\n",
    "\n",
    "# Insert data\n",
    "employees = [\n",
    "    ('Alice', 25, 'IT', 70000),\n",
    "    ('Bob', 30, 'HR', 60000),\n",
    "    ('Charlie', 35, 'Finance', 75000),\n",
    "    ('David', 28, 'IT', 72000),\n",
    "    ('Eve', 32, 'HR', 65000)\n",
    "]\n",
    "\n",
    "cursor.executemany(\n",
    "    'INSERT INTO employees (name, age, department, salary) VALUES (?, ?, ?, ?)',\n",
    "    employees\n",
    ")\n",
    "conn.commit()\n",
    "print(f\"âœ… Inserted {cursor.rowcount} rows\")\n",
    "\n",
    "# Query data\n",
    "print(\"\\nAll employees:\")\n",
    "cursor.execute('SELECT * FROM employees')\n",
    "for row in cursor.fetchall():\n",
    "    print(row)\n",
    "\n",
    "# Query with WHERE clause\n",
    "print(\"\\nIT department employees:\")\n",
    "cursor.execute('SELECT name, salary FROM employees WHERE department = ?', ('IT',))\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[0]}: ${row[1]:,}\")\n",
    "\n",
    "# Aggregate query\n",
    "print(\"\\nAverage salary by department:\")\n",
    "cursor.execute('''\n",
    "    SELECT department, AVG(salary) as avg_salary \n",
    "    FROM employees \n",
    "    GROUP BY department\n",
    "''')\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[0]}: ${row[1]:,.2f}\")\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "print(\"\\nâœ… Database connection closed\")\n",
    "\n",
    "# Expected Output:\n",
    "# âœ… Table 'employees' created\n",
    "# âœ… Inserted 5 rows\n",
    "# All employees: (list of tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a new database 'store.db'\n",
    "# TODO: Create a table 'products' with columns:\n",
    "#       - product_id (INTEGER PRIMARY KEY AUTOINCREMENT)\n",
    "#       - product_name (TEXT NOT NULL)\n",
    "#       - category (TEXT)\n",
    "#       - price (REAL)\n",
    "#       - stock (INTEGER)\n",
    "# TODO: Insert these products:\n",
    "#       ('Laptop', 'Electronics', 1200, 15)\n",
    "#       ('Mouse', 'Accessories', 25, 100)\n",
    "#       ('Keyboard', 'Accessories', 75, 80)\n",
    "#       ('Monitor', 'Electronics', 300, 25)\n",
    "#       ('Headphones', 'Accessories', 150, 50)\n",
    "# TODO: Query all products\n",
    "# TODO: Query products with price > 100\n",
    "# TODO: Calculate average price by category\n",
    "# TODO: Find the product with highest stock\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Create database\n",
    "db_path = data_dir / 'store.db'\n",
    "conn = # Your code here\n",
    "cursor = # Your code here\n",
    "\n",
    "# Create table\n",
    "cursor.execute('''\n",
    "    # Your SQL here\n",
    "''')\n",
    "print(\"âœ… Table 'products' created\")\n",
    "\n",
    "# Insert data\n",
    "products = # Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cursor.executemany(\n",
    "    # Your SQL here\n",
    ")\n",
    "conn.commit()\n",
    "print(f\"âœ… Inserted {cursor.rowcount} rows\")\n",
    "\n",
    "# Query all products\n",
    "print(\"\\nğŸ“¦ All Products:\")\n",
    "cursor.execute(# Your SQL here\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row}\")\n",
    "\n",
    "# Products with price > 100\n",
    "print(\"\\nğŸ’° Products with price > $100:\")\n",
    "cursor.execute(# Your SQL here\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[0]}: ${row[1]:.2f}\")\n",
    "\n",
    "# Average price by category\n",
    "print(\"\\nğŸ“Š Average price by category:\")\n",
    "cursor.execute(# Your SQL here\n",
    "\n",
    "\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row[0]}: ${row[1]:.2f}\")\n",
    "\n",
    "# Product with highest stock\n",
    "print(\"\\nğŸ“ˆ Product with highest stock:\")\n",
    "cursor.execute(# Your SQL here\n",
    "row = cursor.fetchone()\n",
    "print(f\"  {row[0]}: {row[1]} units\")\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "print(\"\\nâœ… Connection closed\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 5: Pandas with SQL\n",
    "**Learn:** Pandas can read from and write to SQL databases easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Pandas + SQL integration\n",
    "\n",
    "# Create sample DataFrame\n",
    "df_sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'product': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'C'],\n",
    "    'quantity': [5, 3, 7, 2, 4, 6, 3, 5, 8, 4],\n",
    "    'revenue': [500, 300, 700, 200, 400, 600, 300, 500, 800, 400]\n",
    "})\n",
    "\n",
    "print(\"Sales DataFrame:\")\n",
    "print(df_sales)\n",
    "\n",
    "# Write DataFrame to SQL database\n",
    "db_path = data_dir / 'sales.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "df_sales.to_sql('sales', conn, if_exists='replace', index=False)\n",
    "print(\"\\nâœ… DataFrame written to SQL table 'sales'\")\n",
    "\n",
    "# Read from SQL using pandas\n",
    "df_from_sql = pd.read_sql('SELECT * FROM sales', conn)\n",
    "print(\"\\nRead from SQL:\")\n",
    "print(df_from_sql.head())\n",
    "\n",
    "# Query with SQL\n",
    "df_query = pd.read_sql('''\n",
    "    SELECT product, SUM(quantity) as total_quantity, SUM(revenue) as total_revenue\n",
    "    FROM sales\n",
    "    GROUP BY product\n",
    "    ORDER BY total_revenue DESC\n",
    "''', conn)\n",
    "\n",
    "print(\"\\nAggregated sales by product:\")\n",
    "print(df_query)\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nâœ… Connection closed\")\n",
    "\n",
    "# Expected Output:\n",
    "# Aggregated sales showing product totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a DataFrame with customer order data:\n",
    "# - customer_id: [101, 102, 103, 101, 104, 102, 105, 103, 104, 101]\n",
    "# - order_date: 10 dates in January 2024\n",
    "# - product: ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Mouse', 'Laptop', 'Keyboard', 'Monitor', 'Laptop', 'Mouse']\n",
    "# - amount: [1200, 25, 75, 300, 25, 1200, 75, 300, 1200, 25]\n",
    "# TODO: Save to SQL database 'orders.db' in table 'orders'\n",
    "# TODO: Read back all orders\n",
    "# TODO: Query total spending by customer\n",
    "# TODO: Query most popular product (most orders)\n",
    "# TODO: Query orders in first week of January\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Create DataFrame\n",
    "orders_df = # Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Orders DataFrame:\")\n",
    "print(orders_df)\n",
    "\n",
    "# Save to SQL\n",
    "db_path = data_dir / 'orders.db'\n",
    "conn = # Your code here\n",
    "\n",
    "# Your code here to save\n",
    "\n",
    "print(\"\\nâœ… Saved to SQL\")\n",
    "\n",
    "# Read all orders\n",
    "all_orders = # Your code here\n",
    "print(\"\\nğŸ“‹ All Orders:\")\n",
    "print(all_orders)\n",
    "\n",
    "# Total spending by customer\n",
    "customer_spending = # Your code here (use GROUP BY)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ’° Total Spending by Customer:\")\n",
    "print(customer_spending)\n",
    "\n",
    "# Most popular product\n",
    "popular_product = # Your code here (use GROUP BY and ORDER BY)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ† Most Popular Product:\")\n",
    "print(popular_product)\n",
    "\n",
    "# Orders in first week\n",
    "first_week = # Your code here (use WHERE with date condition)\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“… Orders in First Week:\")\n",
    "print(first_week)\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nâœ… Connection closed\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 6: Data Schemas\n",
    "**Learn:** Schemas define the structure, types, and constraints of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating tables with proper schemas\n",
    "\n",
    "db_path = data_dir / 'ecommerce.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create customers table with constraints\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS customers (\n",
    "        customer_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        email TEXT UNIQUE NOT NULL,\n",
    "        name TEXT NOT NULL,\n",
    "        phone TEXT,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Create orders table with foreign key\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS orders (\n",
    "        order_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        customer_id INTEGER NOT NULL,\n",
    "        order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        total_amount REAL NOT NULL CHECK(total_amount >= 0),\n",
    "        status TEXT DEFAULT 'pending',\n",
    "        FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n",
    "    )\n",
    "''')\n",
    "\n",
    "print(\"âœ… Tables created with proper schema\")\n",
    "\n",
    "# Insert sample data\n",
    "cursor.execute(\n",
    "    \"INSERT INTO customers (email, name, phone) VALUES (?, ?, ?)\",\n",
    "    ('alice@email.com', 'Alice Smith', '555-0101')\n",
    ")\n",
    "customer_id = cursor.lastrowid\n",
    "\n",
    "cursor.execute(\n",
    "    \"INSERT INTO orders (customer_id, total_amount, status) VALUES (?, ?, ?)\",\n",
    "    (customer_id, 1250.00, 'completed')\n",
    ")\n",
    "\n",
    "conn.commit()\n",
    "print(\"âœ… Sample data inserted\")\n",
    "\n",
    "# Query with JOIN\n",
    "cursor.execute('''\n",
    "    SELECT c.name, c.email, o.order_date, o.total_amount, o.status\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "''')\n",
    "\n",
    "print(\"\\nOrders with customer details:\")\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row}\")\n",
    "\n",
    "# Get table schema\n",
    "cursor.execute(\"PRAGMA table_info(customers)\")\n",
    "print(\"\\nCustomers table schema:\")\n",
    "for col in cursor.fetchall():\n",
    "    print(f\"  {col[1]} ({col[2]}) - NotNull: {col[3]}, Default: {col[4]}\")\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nâœ… Connection closed\")\n",
    "\n",
    "# Expected Output:\n",
    "# Table schema with constraints displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a library database with proper schema:\n",
    "# Table 1: 'authors'\n",
    "#   - author_id (INTEGER PRIMARY KEY AUTOINCREMENT)\n",
    "#   - name (TEXT NOT NULL)\n",
    "#   - email (TEXT UNIQUE)\n",
    "#   - birth_year (INTEGER)\n",
    "# \n",
    "# Table 2: 'books'\n",
    "#   - book_id (INTEGER PRIMARY KEY AUTOINCREMENT)\n",
    "#   - title (TEXT NOT NULL)\n",
    "#   - author_id (INTEGER, FOREIGN KEY)\n",
    "#   - publication_year (INTEGER)\n",
    "#   - price (REAL CHECK(price >= 0))\n",
    "#   - copies_available (INTEGER DEFAULT 0)\n",
    "# \n",
    "# TODO: Insert 3 authors\n",
    "# TODO: Insert 5 books linked to authors\n",
    "# TODO: Query all books with author names (JOIN)\n",
    "# TODO: Query books published after 2020\n",
    "# TODO: Display both table schemas\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "db_path = data_dir / 'library.db'\n",
    "conn = # Your code here\n",
    "cursor = # Your code here\n",
    "\n",
    "# Create authors table\n",
    "cursor.execute('''\n",
    "    # Your SQL here\n",
    "''')\n",
    "\n",
    "# Create books table\n",
    "cursor.execute('''\n",
    "    # Your SQL here\n",
    "''')\n",
    "\n",
    "print(\"âœ… Tables created\")\n",
    "\n",
    "# Insert authors\n",
    "authors = # Your data here\n",
    "\n",
    "\n",
    "\n",
    "# Your code to insert\n",
    "\n",
    "conn.commit()\n",
    "print(\"âœ… Authors inserted\")\n",
    "\n",
    "# Insert books\n",
    "books = # Your data here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Your code to insert\n",
    "\n",
    "conn.commit()\n",
    "print(\"âœ… Books inserted\")\n",
    "\n",
    "# Query books with authors\n",
    "print(\"\\nğŸ“š Books with Authors:\")\n",
    "cursor.execute(# Your SQL with JOIN\n",
    "\n",
    "\n",
    "\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row}\")\n",
    "\n",
    "# Books published after 2020\n",
    "print(\"\\nğŸ“– Books published after 2020:\")\n",
    "cursor.execute(# Your SQL with WHERE\n",
    "for row in cursor.fetchall():\n",
    "    print(f\"  {row}\")\n",
    "\n",
    "# Display schemas\n",
    "print(\"\\nğŸ“‹ Authors Table Schema:\")\n",
    "cursor.execute(\"PRAGMA table_info(authors)\")\n",
    "for col in cursor.fetchall():\n",
    "    print(f\"  {col[1]} ({col[2]})\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Books Table Schema:\")\n",
    "cursor.execute(\"PRAGMA table_info(books)\")\n",
    "for col in cursor.fetchall():\n",
    "    print(f\"  {col[1]} ({col[2]})\")\n",
    "\n",
    "conn.close()\n",
    "print(\"\\nâœ… Connection closed\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 7: Data Normalization Basics\n",
    "**Learn:** Normalization organizes data to reduce redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Denormalized vs Normalized data\n",
    "\n",
    "# Denormalized (bad - lots of duplication)\n",
    "denormalized_data = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3],\n",
    "    'customer_name': ['Alice', 'Alice', 'Bob'],\n",
    "    'customer_email': ['alice@email.com', 'alice@email.com', 'bob@email.com'],\n",
    "    'customer_phone': ['555-0101', '555-0101', '555-0102'],\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard'],\n",
    "    'amount': [1200, 25, 75]\n",
    "})\n",
    "\n",
    "print(\"âŒ Denormalized Data (redundant):\")\n",
    "print(denormalized_data)\n",
    "print(f\"\\nMemory usage: {denormalized_data.memory_usage(deep=True).sum()} bytes\")\n",
    "\n",
    "# Normalized (good - separate tables)\n",
    "customers = pd.DataFrame({\n",
    "    'customer_id': [1, 2],\n",
    "    'name': ['Alice', 'Bob'],\n",
    "    'email': ['alice@email.com', 'bob@email.com'],\n",
    "    'phone': ['555-0101', '555-0102']\n",
    "})\n",
    "\n",
    "orders_normalized = pd.DataFrame({\n",
    "    'order_id': [1, 2, 3],\n",
    "    'customer_id': [1, 1, 2],\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard'],\n",
    "    'amount': [1200, 25, 75]\n",
    "})\n",
    "\n",
    "print(\"\\nâœ… Normalized Data:\")\n",
    "print(\"\\nCustomers:\")\n",
    "print(customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(orders_normalized)\n",
    "\n",
    "total_memory = (customers.memory_usage(deep=True).sum() + \n",
    "                orders_normalized.memory_usage(deep=True).sum())\n",
    "print(f\"\\nTotal memory usage: {total_memory} bytes\")\n",
    "\n",
    "# Join when needed\n",
    "complete_data = orders_normalized.merge(customers, on='customer_id')\n",
    "print(\"\\nJoined data (when needed):\")\n",
    "print(complete_data[['order_id', 'name', 'email', 'product', 'amount']])\n",
    "\n",
    "print(\"\\nâœ… Benefits of normalization:\")\n",
    "print(\"  - Less storage space\")\n",
    "print(\"  - No duplicate data\")\n",
    "print(\"  - Easier to update (change email once, not multiple times)\")\n",
    "print(\"  - Better data integrity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given: Denormalized student enrollment data\n",
    "denormalized_enrollments = pd.DataFrame({\n",
    "    'enrollment_id': [1, 2, 3, 4, 5],\n",
    "    'student_name': ['John', 'John', 'Emma', 'Emma', 'Mike'],\n",
    "    'student_email': ['john@email.com', 'john@email.com', 'emma@email.com', 'emma@email.com', 'mike@email.com'],\n",
    "    'student_age': [20, 20, 22, 22, 21],\n",
    "    'course_name': ['Python', 'SQL', 'Python', 'Machine Learning', 'SQL'],\n",
    "    'instructor': ['Dr. Smith', 'Dr. Johnson', 'Dr. Smith', 'Dr. Brown', 'Dr. Johnson'],\n",
    "    'course_duration': [12, 8, 12, 16, 8],\n",
    "    'enrollment_date': ['2024-01-10', '2024-01-15', '2024-01-10', '2024-01-20', '2024-01-18']\n",
    "})\n",
    "\n",
    "print(\"âŒ Denormalized Enrollment Data:\")\n",
    "print(denormalized_enrollments)\n",
    "\n",
    "# TODO: Normalize this data into 3 tables:\n",
    "# 1. students: student_id, name, email, age\n",
    "# 2. courses: course_id, course_name, instructor, duration\n",
    "# 3. enrollments: enrollment_id, student_id, course_id, enrollment_date\n",
    "# TODO: Calculate memory usage for both approaches\n",
    "# TODO: Demonstrate joining the normalized tables back together\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NORMALIZING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create students table (unique students)\n",
    "students = # Your code here (use drop_duplicates)\n",
    "\n",
    "\n",
    "\n",
    "# Create courses table (unique courses)\n",
    "courses = # Your code here\n",
    "\n",
    "\n",
    "\n",
    "# Create enrollments table (links)\n",
    "enrollments = # Your code here (merge to get IDs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nâœ… NORMALIZED TABLES:\")\n",
    "print(\"\\n1. Students:\")\n",
    "print(students)\n",
    "\n",
    "print(\"\\n2. Courses:\")\n",
    "print(courses)\n",
    "\n",
    "print(\"\\n3. Enrollments:\")\n",
    "print(enrollments)\n",
    "\n",
    "# Calculate memory usage\n",
    "denorm_memory = # Your code here\n",
    "norm_memory = # Your code here (sum of all three tables)\n",
    "\n",
    "print(\"\\nğŸ“Š MEMORY COMPARISON:\")\n",
    "print(f\"Denormalized: {denorm_memory:,} bytes\")\n",
    "print(f\"Normalized: {norm_memory:,} bytes\")\n",
    "print(f\"Savings: {denorm_memory - norm_memory:,} bytes ({(1-norm_memory/denorm_memory)*100:.1f}%)\")\n",
    "\n",
    "# Join tables back\n",
    "reconstructed = # Your code here (multiple merges)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ”— RECONSTRUCTED DATA (via JOIN):\")\n",
    "print(reconstructed)\n",
    "\n",
    "print(\"\\nâœ… Benefits demonstrated!\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 3: Small Projects\n",
    "\n",
    "### Project 1: Multi-Format Data Pipeline\n",
    "**Task:** Build a pipeline that processes data through multiple formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 1: Multi-Format Data Pipeline\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROJECT: MULTI-FORMAT DATA PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Generate raw data\n",
    "print(\"\\nğŸ“¥ Step 1: Generating raw data...\")\n",
    "np.random.seed(42)\n",
    "n_transactions = 1000\n",
    "\n",
    "raw_data = pd.DataFrame({\n",
    "    'transaction_id': range(1, n_transactions + 1),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=n_transactions, freq='H'),\n",
    "    'customer_id': np.random.randint(1, 100, n_transactions),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books'], n_transactions),\n",
    "    'amount': np.random.uniform(10, 1000, n_transactions),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'Cash', 'PayPal'], n_transactions),\n",
    "    'store_location': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston'], n_transactions)\n",
    "})\n",
    "\n",
    "print(f\"âœ… Generated {len(raw_data)} transactions\")\n",
    "print(raw_data.head())\n",
    "\n",
    "# TODO: Complete the pipeline:\n",
    "# Step 2: Save raw data as CSV\n",
    "# Step 3: Read from CSV, clean data (remove duplicates, handle missing)\n",
    "# Step 4: Save cleaned data as JSON\n",
    "# Step 5: Read from JSON, perform aggregations\n",
    "# Step 6: Save aggregated data to SQLite database\n",
    "# Step 7: Query database for insights\n",
    "# Step 8: Export final report as CSV\n",
    "# Step 9: Compare file sizes at each step\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import time\n",
    "file_sizes = {}\n",
    "\n",
    "# Step 2: Save as CSV\n",
    "print(\"\\nğŸ’¾ Step 2: Saving as CSV...\")\n",
    "csv_path = # Your code here\n",
    "# Your code here\n",
    "file_sizes['raw_csv'] = csv_path.stat().st_size\n",
    "print(f\"âœ… Saved to {csv_path} ({file_sizes['raw_csv']/1024:.2f} KB)\")\n",
    "\n",
    "# Step 3: Clean data\n",
    "print(\"\\nğŸ§¹ Step 3: Cleaning data...\")\n",
    "df_cleaned = # Your code here (read CSV, remove duplicates)\n",
    "\n",
    "\n",
    "print(f\"âœ… Cleaned: {len(raw_data)} â†’ {len(df_cleaned)} rows\")\n",
    "\n",
    "# Step 4: Save as JSON\n",
    "print(\"\\nğŸ’¾ Step 4: Saving as JSON...\")\n",
    "json_path = # Your code here\n",
    "# Your code here\n",
    "file_sizes['cleaned_json'] = json_path.stat().st_size\n",
    "print(f\"âœ… Saved to {json_path} ({file_sizes['cleaned_json']/1024:.2f} KB)\")\n",
    "\n",
    "# Step 5: Aggregate data\n",
    "print(\"\\nğŸ“Š Step 5: Performing aggregations...\")\n",
    "df_from_json = # Your code here\n",
    "\n",
    "# Calculate daily sales by category\n",
    "df_from_json['date'] = pd.to_datetime(df_from_json['timestamp']).dt.date\n",
    "aggregated = # Your code here (groupby date and category)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"âœ… Aggregated to {len(aggregated)} rows\")\n",
    "print(aggregated.head())\n",
    "\n",
    "# Step 6: Save to SQLite\n",
    "print(\"\\nğŸ’¾ Step 6: Saving to SQLite...\")\n",
    "db_path = # Your code here\n",
    "conn = # Your code here\n",
    "# Your code here (save aggregated data)\n",
    "\n",
    "file_sizes['database'] = db_path.stat().st_size\n",
    "print(f\"âœ… Saved to database ({file_sizes['database']/1024:.2f} KB)\")\n",
    "\n",
    "# Step 7: Query for insights\n",
    "print(\"\\nğŸ” Step 7: Querying insights...\")\n",
    "\n",
    "# Top categories by revenue\n",
    "top_categories = # Your code here (SQL query)\n",
    "\n",
    "\n",
    "print(\"\\nTop Categories by Revenue:\")\n",
    "print(top_categories)\n",
    "\n",
    "# Daily trends\n",
    "daily_trends = # Your code here\n",
    "\n",
    "\n",
    "print(\"\\nDaily Sales Trends (sample):\")\n",
    "print(daily_trends.head(10))\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Step 8: Export report\n",
    "print(\"\\nğŸ’¾ Step 8: Exporting final report...\")\n",
    "report_path = # Your code here\n",
    "# Your code here (save top categories and trends)\n",
    "file_sizes['report'] = report_path.stat().st_size\n",
    "print(f\"âœ… Report saved ({file_sizes['report']/1024:.2f} KB)\")\n",
    "\n",
    "# Step 9: Compare file sizes\n",
    "print(\"\\nğŸ“Š Step 9: File Size Comparison:\")\n",
    "print(\"=\"*60)\n",
    "for name, size in file_sizes.items():\n",
    "    print(f\"{name:<20} {size/1024:>10.2f} KB ({size:>10,} bytes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PIPELINE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPipeline Summary:\")\n",
    "print(f\"  Input: {n_transactions} transactions\")\n",
    "print(f\"  Output: Aggregated insights in multiple formats\")\n",
    "print(f\"  Formats used: CSV â†’ JSON â†’ SQLite â†’ CSV\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Project 2: Database Design and Implementation\n",
    "**Task:** Design and implement a normalized database for an e-commerce system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 2: E-Commerce Database Design\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROJECT: E-COMMERCE DATABASE DESIGN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TODO: Design and implement a normalized database with:\n",
    "# \n",
    "# Tables:\n",
    "# 1. customers (customer_id, email, name, address, phone, registration_date)\n",
    "# 2. products (product_id, name, description, category, price, stock)\n",
    "# 3. orders (order_id, customer_id, order_date, status, total_amount)\n",
    "# 4. order_items (item_id, order_id, product_id, quantity, price_at_purchase)\n",
    "# 5. reviews (review_id, product_id, customer_id, rating, comment, review_date)\n",
    "#\n",
    "# Tasks:\n",
    "# 1. Create all tables with proper constraints\n",
    "# 2. Insert sample data (10 customers, 20 products, 30 orders, order items)\n",
    "# 3. Write queries to:\n",
    "#    - Find top 5 customers by total spending\n",
    "#    - Find best selling products\n",
    "#    - Calculate average order value\n",
    "#    - Find products with highest ratings\n",
    "#    - Get customer order history\n",
    "# 4. Export query results to CSV files\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "db_path = data_dir / 'ecommerce_full.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"\\nğŸ“‹ Step 1: Creating database schema...\")\n",
    "\n",
    "# Create tables\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… All tables created\")\n",
    "\n",
    "print(\"\\nğŸ“¥ Step 2: Inserting sample data...\")\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… Sample data inserted\")\n",
    "\n",
    "print(\"\\nğŸ” Step 3: Running analytics queries...\")\n",
    "\n",
    "# Query 1: Top customers\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Query 2: Best selling products\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Query 3: Average order value\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Query 4: Top rated products\n",
    "# Your code here\n",
    "\n",
    "\n",
    "print(\"\\nğŸ’¾ Step 4: Exporting results...\")\n",
    "# Your code here\n",
    "\n",
    "\n",
    "conn.close()\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… E-COMMERCE DATABASE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 4: Final Challenge\n",
    "\n",
    "### Challenge: Complete Data Engineering Pipeline\n",
    "**Task:** Build an end-to-end data engineering solution combining all concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Challenge: Healthcare Data Management System\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL CHALLENGE: HEALTHCARE DATA MANAGEMENT SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TODO: Build a complete data management system for a healthcare clinic:\n",
    "#\n",
    "# PART 1: DATA GENERATION\n",
    "# - Generate synthetic data for 500 patients\n",
    "# - Generate 2000 appointments\n",
    "# - Generate 1500 prescriptions\n",
    "# - Include realistic relationships between entities\n",
    "#\n",
    "# PART 2: DATA STORAGE\n",
    "# - Design normalized database schema (at least 5 tables)\n",
    "# - Implement proper constraints and foreign keys\n",
    "# - Store data efficiently\n",
    "#\n",
    "# PART 3: DATA FORMATS\n",
    "# - Export different views in different formats:\n",
    "#   * Patient summaries â†’ CSV\n",
    "#   * Appointment schedules â†’ JSON\n",
    "#   * Prescription details â†’ SQL database\n",
    "#   * Analytics reports â†’ CSV\n",
    "#\n",
    "# PART 4: ANALYTICS\n",
    "# - Most common diagnoses\n",
    "# - Busiest appointment times\n",
    "# - Average prescriptions per patient\n",
    "# - Monthly patient visits trends\n",
    "# - Doctor workload analysis\n",
    "#\n",
    "# PART 5: REPORTING\n",
    "# - Generate comprehensive reports\n",
    "# - Compare file sizes across formats\n",
    "# - Document the data pipeline\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Your complete solution here\n",
    "# This should be a comprehensive implementation\n",
    "# combining everything learned today\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 5: Summary & Key Concepts\n",
    "\n",
    "### ğŸ¯ What You Learned Today:\n",
    "\n",
    "**1. Data Formats:**\n",
    "- **CSV**: Simple, text-based, widely compatible\n",
    "  - Best for: Tabular data, spreadsheets, simple storage\n",
    "  - Pros: Human-readable, universal support\n",
    "  - Cons: No type information, larger file size\n",
    "\n",
    "- **JSON**: Hierarchical, flexible structure\n",
    "  - Best for: Nested data, APIs, configuration\n",
    "  - Pros: Human-readable, supports nesting\n",
    "  - Cons: Larger file size, slower parsing\n",
    "\n",
    "- **Parquet**: Columnar, compressed, optimized\n",
    "  - Best for: Analytics, big data, data lakes\n",
    "  - Pros: Compressed, fast reads, efficient\n",
    "  - Cons: Not human-readable, requires libraries\n",
    "\n",
    "- **SQL Databases**: Structured, relational, ACID\n",
    "  - Best for: Transactional data, complex queries\n",
    "  - Pros: Data integrity, relationships, queries\n",
    "  - Cons: Setup complexity, schema rigidity\n",
    "\n",
    "**2. File I/O with Pandas:**\n",
    "- `pd.read_csv()` / `.to_csv()`: CSV operations\n",
    "- `pd.read_json()` / `.to_json()`: JSON operations\n",
    "- `pd.read_parquet()` / `.to_parquet()`: Parquet operations\n",
    "- `pd.read_sql()` / `.to_sql()`: SQL operations\n",
    "- Always specify `index=False` when saving if you don't need row indices\n",
    "\n",
    "**3. SQLite Basics:**\n",
    "- `sqlite3.connect()`: Create/connect to database\n",
    "- `cursor.execute()`: Run SQL commands\n",
    "- `conn.commit()`: Save changes\n",
    "- `cursor.fetchall()` / `.fetchone()`: Retrieve results\n",
    "- Always close connections: `conn.close()`\n",
    "\n",
    "**4. SQL Fundamentals:**\n",
    "- **CREATE TABLE**: Define structure\n",
    "- **INSERT INTO**: Add data\n",
    "- **SELECT**: Query data\n",
    "- **WHERE**: Filter rows\n",
    "- **GROUP BY**: Aggregate data\n",
    "- **JOIN**: Combine tables\n",
    "- **ORDER BY**: Sort results\n",
    "\n",
    "**5. Data Schemas:**\n",
    "- Define structure with data types\n",
    "- Primary keys: Unique identifiers\n",
    "- Foreign keys: Link tables\n",
    "- Constraints: NOT NULL, UNIQUE, CHECK\n",
    "- Default values: DEFAULT keyword\n",
    "\n",
    "**6. Normalization:**\n",
    "- **Purpose**: Reduce redundancy, improve integrity\n",
    "- **1NF**: Atomic values, no repeating groups\n",
    "- **2NF**: Remove partial dependencies\n",
    "- **3NF**: Remove transitive dependencies\n",
    "- **Benefits**: Less storage, easier updates, data integrity\n",
    "- **Trade-off**: More joins needed for queries\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Key Takeaways:\n",
    "\n",
    "âœ… **Choose the right format** for your use case  \n",
    "âœ… **CSV for simplicity**, JSON for flexibility, Parquet for performance  \n",
    "âœ… **SQL databases** provide structure and query power  \n",
    "âœ… **Normalize data** to reduce redundancy  \n",
    "âœ… **Define schemas** with proper constraints  \n",
    "âœ… **Always close** database connections  \n",
    "âœ… **Pandas integrates** seamlessly with all formats  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”‘ Format Selection Guide:\n",
    "\n",
    "| Use Case | Best Format | Why? |\n",
    "|----------|-------------|------|\n",
    "| Quick data sharing | CSV | Universal compatibility |\n",
    "| API responses | JSON | Web standard, hierarchical |\n",
    "| Big data analytics | Parquet | Compressed, columnar |\n",
    "| Transactional system | SQL | ACID, relationships |\n",
    "| Configuration files | JSON | Human-readable, flexible |\n",
    "| ML training data | Parquet/HDF5 | Fast reads, efficient |\n",
    "| Reports/exports | CSV/Excel | Business-friendly |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Best Practices:\n",
    "\n",
    "1. **File Operations:**\n",
    "   - Always specify encoding: `encoding='utf-8'`\n",
    "   - Handle errors: Use try-except blocks\n",
    "   - Close files/connections properly\n",
    "   - Check file existence before operations\n",
    "\n",
    "2. **Database Operations:**\n",
    "   - Use parameterized queries (prevent SQL injection)\n",
    "   - Commit transactions explicitly\n",
    "   - Create indexes on frequently queried columns\n",
    "   - Back up databases regularly\n",
    "\n",
    "3. **Schema Design:**\n",
    "   - Use meaningful column names\n",
    "   - Choose appropriate data types\n",
    "   - Add constraints for data integrity\n",
    "   - Document your schema\n",
    "\n",
    "4. **Performance:**\n",
    "   - Use Parquet for large datasets\n",
    "   - Index database columns\n",
    "   - Read only needed columns\n",
    "   - Batch operations when possible\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "\n",
    "- Learn about NoSQL databases (MongoDB, Redis)\n",
    "- Explore cloud storage (S3, Azure Blob Storage)\n",
    "- Study data warehousing concepts\n",
    "- Practice with larger datasets\n",
    "- **Tomorrow: APIs and External Data Sources!** ğŸŒ\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ Tools & Libraries:\n",
    "\n",
    "**File Formats:**\n",
    "- Pandas: All formats\n",
    "- json: Native JSON support\n",
    "- PyArrow: Parquet support\n",
    "- openpyxl: Excel files\n",
    "\n",
    "**Databases:**\n",
    "- sqlite3: Built-in SQLite\n",
    "- SQLAlchemy: ORM framework\n",
    "- psycopg2: PostgreSQL\n",
    "- pymongo: MongoDB\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’ª Skills Checklist:\n",
    "\n",
    "By now you should be able to:\n",
    "- âœ… Read and write CSV files\n",
    "- âœ… Work with JSON data\n",
    "- âœ… Understand Parquet benefits\n",
    "- âœ… Create SQLite databases\n",
    "- âœ… Write basic SQL queries\n",
    "- âœ… Design database schemas\n",
    "- âœ… Normalize data structures\n",
    "- âœ… Choose appropriate formats\n",
    "- âœ… Integrate Pandas with SQL\n",
    "\n",
    "---\n",
    "\n",
    "**Great work completing Day 1! ğŸ‰**\n",
    "\n",
    "You've learned the foundations of data storage and retrieval - essential skills for any data engineer or ML engineer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
