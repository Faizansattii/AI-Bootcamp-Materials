{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ AI Bootcamp Practice Exercises\n",
    "## Week 4 - Day 3: Data Pipelines and Automation\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š Topics Covered Today:\n",
    "- ETL concept (Extract, Transform, Load)\n",
    "- Introduction to Airflow and Prefect (visual overview)\n",
    "- Logging and error handling in data pipelines\n",
    "- Scheduling and maintaining automated data workflows\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Instructions:\n",
    "- Read each **Example** carefully before attempting exercises\n",
    "- Fill in the **TODO** sections with your code\n",
    "- Run each cell to check your output\n",
    "- Expected outputs are provided for reference\n",
    "- Ask for help if you get stuck!\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started! ğŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 1: Introduction & Setup\n",
    "\n",
    "**What is a Data Pipeline?**\n",
    "\n",
    "Think of a data pipeline like a factory assembly line:\n",
    "1. **Input**: Raw materials come in (raw data)\n",
    "2. **Process**: Machines transform materials (clean, process data)\n",
    "3. **Output**: Finished products come out (ready-to-use data)\n",
    "\n",
    "**ETL - Extract, Transform, Load:**\n",
    "- **Extract** ğŸ“¥: Get data from sources (APIs, databases, files)\n",
    "- **Transform** ğŸ”„: Clean and process data (filter, merge, calculate)\n",
    "- **Load** ğŸ’¾: Save data to destination (database, file, data warehouse)\n",
    "\n",
    "**Why Automate?**\n",
    "- ğŸ• **Save Time**: Run automatically instead of manual work\n",
    "- ğŸ¯ **Consistency**: Same process every time\n",
    "- ğŸ“Š **Fresh Data**: Always have up-to-date information\n",
    "- ğŸ”„ **Scalability**: Handle more data easily\n",
    "\n",
    "**Today's Focus:**\n",
    "- Build simple pipelines\n",
    "- Add logging (track what's happening)\n",
    "- Handle errors gracefully\n",
    "- Automate with scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Create directories\n",
    "data_dir = Path('pipeline_data')\n",
    "logs_dir = Path('logs')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"\\nData directory: {data_dir.absolute()}\")\n",
    "print(f\"Logs directory: {logs_dir.absolute()}\")\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 2: Basic Exercises\n",
    "\n",
    "### Exercise 1: Simple Extract Function\n",
    "**Learn:** Create a function to extract data from a source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple extract function\n",
    "\n",
    "def extract_from_api():\n",
    "    \"\"\"\n",
    "    Extract data from API\n",
    "    Returns: DataFrame with extracted data\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“¥ EXTRACTING data from API...\")\n",
    "    \n",
    "    # Fetch data from API\n",
    "    url = 'https://jsonplaceholder.typicode.com/users'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"âœ… Extracted {len(df)} records\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"âŒ Failed to extract: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Test the function\n",
    "users_df = extract_from_api()\n",
    "if users_df is not None:\n",
    "    print(\"\\nFirst 3 users:\")\n",
    "    print(users_df[['id', 'name', 'email']].head(3))\n",
    "\n",
    "# Expected Output:\n",
    "# ğŸ“¥ EXTRACTING data from API...\n",
    "# âœ… Extracted 10 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function extract_posts() that:\n",
    "# 1. Fetches posts from: https://jsonplaceholder.typicode.com/posts\n",
    "# 2. Limits to first 20 posts (use _limit parameter)\n",
    "# 3. Converts to DataFrame\n",
    "# 4. Returns the DataFrame\n",
    "# 5. Prints success message with count\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def extract_posts():\n",
    "    \"\"\"\n",
    "    Extract posts from API\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“¥ EXTRACTING posts...\")\n",
    "    \n",
    "    # Your code here\n",
    "    url = # Your code here\n",
    "    params = # Your code here (dict with _limit)\n",
    "    \n",
    "    response = # Your code here\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Your code here\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"âŒ Failed: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Test your function\n",
    "posts_df = extract_posts()\n",
    "if posts_df is not None:\n",
    "    print(\"\\nFirst 5 posts:\")\n",
    "    print(posts_df[['id', 'userId', 'title']].head())\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Expected Output:\n",
    "# ğŸ“¥ EXTRACTING posts...\n",
    "# âœ… Extracted 20 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2: Simple Transform Function\n",
    "**Learn:** Create a function to clean and transform data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple transform function\n",
    "\n",
    "def transform_users(df):\n",
    "    \"\"\"\n",
    "    Transform user data\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”„ TRANSFORMING data...\")\n",
    "    \n",
    "    # Keep only needed columns\n",
    "    df_clean = df[['id', 'name', 'email', 'phone', 'website']].copy()\n",
    "    \n",
    "    # Clean email (lowercase)\n",
    "    df_clean['email'] = df_clean['email'].str.lower()\n",
    "    \n",
    "    # Add timestamp\n",
    "    df_clean['extracted_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Rename columns\n",
    "    df_clean.columns = ['user_id', 'full_name', 'email', 'phone', 'website', 'extracted_at']\n",
    "    \n",
    "    print(f\"âœ… Transformed {len(df_clean)} records\")\n",
    "    return df_clean\n",
    "\n",
    "# Test the function\n",
    "if users_df is not None:\n",
    "    transformed_df = transform_users(users_df)\n",
    "    print(\"\\nTransformed data:\")\n",
    "    print(transformed_df.head(3))\n",
    "\n",
    "# Expected Output:\n",
    "# ğŸ”„ TRANSFORMING data...\n",
    "# âœ… Transformed 10 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function transform_posts(df) that:\n",
    "# 1. Keeps only columns: id, userId, title, body\n",
    "# 2. Renames: idâ†’post_id, userIdâ†’user_id\n",
    "# 3. Adds a 'word_count' column (count words in title)\n",
    "# 4. Adds 'extracted_at' timestamp\n",
    "# 5. Returns cleaned DataFrame\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def transform_posts(df):\n",
    "    \"\"\"\n",
    "    Transform posts data\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”„ TRANSFORMING posts...\")\n",
    "    \n",
    "    # Select columns\n",
    "    df_clean = # Your code here\n",
    "    \n",
    "    # Add word count\n",
    "    df_clean['word_count'] = # Your code here (use str.split and len)\n",
    "    \n",
    "    # Add timestamp\n",
    "    df_clean['extracted_at'] = # Your code here\n",
    "    \n",
    "    # Rename columns\n",
    "    df_clean.columns = # Your code here\n",
    "    \n",
    "    print(f\"âœ… Transformed {len(df_clean)} records\")\n",
    "    return df_clean\n",
    "\n",
    "# Test your function\n",
    "if posts_df is not None:\n",
    "    transformed_posts = transform_posts(posts_df)\n",
    "    print(\"\\nTransformed posts:\")\n",
    "    print(transformed_posts[['post_id', 'user_id', 'word_count']].head())\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Expected Output:\n",
    "# ğŸ”„ TRANSFORMING posts...\n",
    "# âœ… Transformed 20 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 3: Simple Load Function\n",
    "**Learn:** Create a function to load data to a destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple load function\n",
    "\n",
    "def load_to_csv(df, filename):\n",
    "    \"\"\"\n",
    "    Load data to CSV file\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ’¾ LOADING data to CSV...\")\n",
    "    \n",
    "    filepath = data_dir / filename\n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    file_size = filepath.stat().st_size\n",
    "    print(f\"âœ… Loaded {len(df)} records to {filename}\")\n",
    "    print(f\"   File size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Test the function\n",
    "if 'transformed_df' in locals():\n",
    "    csv_path = load_to_csv(transformed_df, 'users_processed.csv')\n",
    "    print(f\"\\nğŸ“ Saved to: {csv_path}\")\n",
    "\n",
    "# Expected Output:\n",
    "# ğŸ’¾ LOADING data to CSV...\n",
    "# âœ… Loaded 10 records to users_processed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function load_to_database(df, table_name) that:\n",
    "# 1. Connects to SQLite database 'pipeline.db'\n",
    "# 2. Loads DataFrame to specified table\n",
    "# 3. Replaces table if it exists\n",
    "# 4. Prints success message\n",
    "# 5. Closes connection\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def load_to_database(df, table_name):\n",
    "    \"\"\"\n",
    "    Load data to SQLite database\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ’¾ LOADING data to database table '{table_name}'...\")\n",
    "    \n",
    "    db_path = data_dir / 'pipeline.db'\n",
    "    conn = # Your code here\n",
    "    \n",
    "    # Load to database\n",
    "    # Your code here (use to_sql with if_exists='replace')\n",
    "    \n",
    "    # Close connection\n",
    "    # Your code here\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(df)} records to '{table_name}' table\")\n",
    "    print(f\"   Database: {db_path}\")\n",
    "\n",
    "# Test your function\n",
    "if 'transformed_posts' in locals():\n",
    "    load_to_database(transformed_posts, 'posts')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Expected Output:\n",
    "# ğŸ’¾ LOADING data to database table 'posts'...\n",
    "# âœ… Loaded 20 records to 'posts' table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 4: Combining ETL - Your First Pipeline!\n",
    "**Learn:** Put Extract, Transform, Load together in one pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete ETL pipeline\n",
    "\n",
    "def run_users_pipeline():\n",
    "    \"\"\"\n",
    "    Run complete ETL pipeline for users\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸš€ STARTING USERS PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # EXTRACT\n",
    "    users_df = extract_from_api()\n",
    "    if users_df is None:\n",
    "        print(\"âŒ Pipeline failed at EXTRACT stage\")\n",
    "        return\n",
    "    \n",
    "    # TRANSFORM\n",
    "    transformed_df = transform_users(users_df)\n",
    "    \n",
    "    # LOAD\n",
    "    load_to_csv(transformed_df, 'users_final.csv')\n",
    "    \n",
    "    # Summary\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ğŸ“Š Records processed: {len(transformed_df)}\")\n",
    "    print(f\"â±ï¸  Duration: {duration:.2f} seconds\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the pipeline\n",
    "run_users_pipeline()\n",
    "\n",
    "# Expected Output:\n",
    "# Pipeline runs all three stages and prints summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create function run_posts_pipeline() that:\n",
    "# 1. Extracts posts (call extract_posts)\n",
    "# 2. Transforms posts (call transform_posts)\n",
    "# 3. Loads to both CSV and database\n",
    "# 4. Measures execution time\n",
    "# 5. Prints summary with statistics\n",
    "# 6. Handles errors at each stage\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def run_posts_pipeline():\n",
    "    \"\"\"\n",
    "    Run complete ETL pipeline for posts\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸš€ STARTING POSTS PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # EXTRACT\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # TRANSFORM\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # LOAD to CSV\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # LOAD to Database\n",
    "    # Your code here\n",
    "    \n",
    "    \n",
    "    # Summary\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… PIPELINE COMPLETED!\")\n",
    "    print(\"=\"*60)\n",
    "    # Your code here to print statistics\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run your pipeline\n",
    "run_posts_pipeline()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 5: Adding Logging\n",
    "**Learn:** Track what your pipeline is doing with logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Pipeline with logging\n",
    "\n",
    "# Setup logging\n",
    "def setup_logging():\n",
    "    \"\"\"\n",
    "    Setup logging configuration\n",
    "    \"\"\"\n",
    "    log_file = logs_dir / f'pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()  # Also print to console\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "# Create logger\n",
    "logger = setup_logging()\n",
    "\n",
    "def extract_with_logging():\n",
    "    \"\"\"\n",
    "    Extract data with logging\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data extraction\")\n",
    "    \n",
    "    try:\n",
    "        url = 'https://jsonplaceholder.typicode.com/users'\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            logger.info(f\"Successfully extracted {len(data)} records\")\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            logger.error(f\"Failed to extract: Status code {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during extraction: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test with logging\n",
    "print(\"Testing extraction with logging:\")\n",
    "df_with_logs = extract_with_logging()\n",
    "\n",
    "# Expected Output:\n",
    "# Logs showing timestamp, level, and message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function transform_with_logging(df) that:\n",
    "# 1. Logs start of transformation\n",
    "# 2. Logs each transformation step\n",
    "# 3. Logs any errors that occur\n",
    "# 4. Logs completion with record count\n",
    "# 5. Uses try-except for error handling\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def transform_with_logging(df):\n",
    "    \"\"\"\n",
    "    Transform data with logging\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data transformation\")\n",
    "    \n",
    "    try:\n",
    "        # Log original record count\n",
    "        logger.info(f\"Input: {len(df)} records\")\n",
    "        \n",
    "        # Keep needed columns\n",
    "        logger.info(\"Selecting required columns\")\n",
    "        df_clean = # Your code here\n",
    "        \n",
    "        # Clean data\n",
    "        logger.info(\"Cleaning email addresses\")\n",
    "        # Your code here\n",
    "        \n",
    "        # Add timestamp\n",
    "        logger.info(\"Adding timestamp\")\n",
    "        # Your code here\n",
    "        \n",
    "        # Rename columns\n",
    "        logger.info(\"Renaming columns\")\n",
    "        # Your code here\n",
    "        \n",
    "        logger.info(f\"Transformation completed: {len(df_clean)} records\")\n",
    "        return df_clean\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during transformation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test your function\n",
    "if df_with_logs is not None:\n",
    "    transformed_logged = transform_with_logging(df_with_logs)\n",
    "    print(\"\\nâœ… Check the logs directory for detailed logs!\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 6: Error Handling in Pipelines\n",
    "**Learn:** Make your pipeline robust with proper error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Pipeline with error handling\n",
    "\n",
    "def safe_extract():\n",
    "    \"\"\"\n",
    "    Extract with comprehensive error handling\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting safe extraction\")\n",
    "    \n",
    "    try:\n",
    "        url = 'https://jsonplaceholder.typicode.com/users'\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for bad status\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Validate data\n",
    "        if not data:\n",
    "            raise ValueError(\"No data received from API\")\n",
    "        \n",
    "        logger.info(f\"Extraction successful: {len(data)} records\")\n",
    "        return pd.DataFrame(data), None  # Return data and no error\n",
    "        \n",
    "    except requests.Timeout:\n",
    "        error_msg = \"Request timed out\"\n",
    "        logger.error(error_msg)\n",
    "        return None, error_msg\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        error_msg = f\"Request failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return None, error_msg\n",
    "        \n",
    "    except ValueError as e:\n",
    "        error_msg = f\"Data validation failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return None, error_msg\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Unexpected error: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return None, error_msg\n",
    "\n",
    "# Test error handling\n",
    "print(\"Testing safe extraction:\")\n",
    "data, error = safe_extract()\n",
    "\n",
    "if error:\n",
    "    print(f\"âŒ Pipeline failed: {error}\")\n",
    "else:\n",
    "    print(f\"âœ… Pipeline succeeded: {len(data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create function safe_pipeline() that:\n",
    "# 1. Runs complete ETL with error handling\n",
    "# 2. Returns success status and any errors\n",
    "# 3. Logs all steps\n",
    "# 4. Handles errors at each stage (Extract, Transform, Load)\n",
    "# 5. Continues if possible, or stops if critical error\n",
    "# 6. Returns summary of what was accomplished\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def safe_pipeline():\n",
    "    \"\"\"\n",
    "    Complete pipeline with error handling\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"Starting safe pipeline\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    summary = {\n",
    "        'success': False,\n",
    "        'records_processed': 0,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # EXTRACT\n",
    "        logger.info(\"Stage 1: EXTRACT\")\n",
    "        data, error = safe_extract()\n",
    "        \n",
    "        if error:\n",
    "            summary['errors'].append(f\"Extract: {error}\")\n",
    "            return summary\n",
    "        \n",
    "        # TRANSFORM\n",
    "        logger.info(\"Stage 2: TRANSFORM\")\n",
    "        # Your code here\n",
    "        \n",
    "        \n",
    "        \n",
    "        # LOAD\n",
    "        logger.info(\"Stage 3: LOAD\")\n",
    "        # Your code here\n",
    "        \n",
    "        \n",
    "        \n",
    "        summary['success'] = True\n",
    "        summary['records_processed'] = len(data)\n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Pipeline failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        summary['errors'].append(error_msg)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Run the safe pipeline\n",
    "result = safe_pipeline()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Records: {result['records_processed']}\")\n",
    "if result['errors']:\n",
    "    print(\"Errors:\")\n",
    "    for error in result['errors']:\n",
    "        print(f\"  - {error}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 7: Simple Scheduling Concept\n",
    "**Learn:** Understand how to schedule pipelines to run automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“– Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple scheduling demonstration\n",
    "\n",
    "def scheduled_pipeline(run_count=3, interval=2):\n",
    "    \"\"\"\n",
    "    Demonstrate scheduling by running pipeline multiple times\n",
    "    In real life, you'd use: cron, Windows Task Scheduler, or Airflow\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"SCHEDULING DEMO: Will run {run_count} times every {interval} seconds\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(run_count):\n",
    "        print(f\"\\nğŸ• Run {i+1}/{run_count} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Run the pipeline\n",
    "        logger.info(f\"Scheduled run {i+1}\")\n",
    "        data, error = safe_extract()\n",
    "        \n",
    "        if data is not None:\n",
    "            print(f\"âœ… Run {i+1} completed: {len(data)} records\")\n",
    "        else:\n",
    "            print(f\"âŒ Run {i+1} failed: {error}\")\n",
    "        \n",
    "        # Wait before next run (except last run)\n",
    "        if i < run_count - 1:\n",
    "            print(f\"â³ Waiting {interval} seconds until next run...\")\n",
    "            time.sleep(interval)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… ALL SCHEDULED RUNS COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Demo scheduling (runs 3 times, 2 seconds apart)\n",
    "print(\"Demonstrating scheduled pipeline execution:\\n\")\n",
    "scheduled_pipeline(run_count=3, interval=2)\n",
    "\n",
    "print(\"\\nğŸ’¡ In production, you would use:\")\n",
    "print(\"   - Cron jobs (Linux/Mac)\")\n",
    "print(\"   - Windows Task Scheduler (Windows)\")\n",
    "print(\"   - Apache Airflow (advanced)\")\n",
    "print(\"   - Prefect (modern alternative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœï¸ Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a monitoring_pipeline() function that:\n",
    "# 1. Runs every few seconds (simulating hourly in production)\n",
    "# 2. Checks if new data is available\n",
    "# 3. Only processes if data has changed\n",
    "# 4. Keeps track of last run time\n",
    "# 5. Logs all activities\n",
    "# 6. Runs 5 times with 3-second intervals\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def monitoring_pipeline(runs=5, interval=3):\n",
    "    \"\"\"\n",
    "    Pipeline that monitors for changes\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"MONITORING PIPELINE STARTED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    last_count = 0\n",
    "    \n",
    "    for i in range(runs):\n",
    "        run_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"\\nğŸ” Check {i+1}/{runs} at {run_time}\")\n",
    "        logger.info(f\"Monitoring check {i+1}\")\n",
    "        \n",
    "        # Your code here\n",
    "        # Extract data\n",
    "        # Compare with last_count\n",
    "        # Only process if changed\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if i < runs - 1:\n",
    "            print(f\"â³ Next check in {interval} seconds...\")\n",
    "            time.sleep(interval)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… MONITORING COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run your monitoring pipeline\n",
    "monitoring_pipeline(runs=5, interval=3)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 3: Small Projects\n",
    "\n",
    "### Project 1: Complete Data Pipeline\n",
    "**Task:** Build a production-ready ETL pipeline with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 1: Production ETL Pipeline\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROJECT: PRODUCTION ETL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TODO: Build a complete ETL pipeline that:\n",
    "#\n",
    "# FEATURES:\n",
    "# 1. Extracts data from multiple sources (users, posts, comments)\n",
    "# 2. Transforms and cleans all data\n",
    "# 3. Loads to multiple destinations (CSV, JSON, Database)\n",
    "# 4. Includes comprehensive logging\n",
    "# 5. Handles all errors gracefully\n",
    "# 6. Provides detailed statistics\n",
    "# 7. Validates data quality\n",
    "# 8. Can run on schedule\n",
    "#\n",
    "# OUTPUT:\n",
    "# - Clean CSV files\n",
    "# - Normalized database\n",
    "# - JSON exports\n",
    "# - Log files\n",
    "# - Summary report\n",
    "\n",
    "# TODO: Write your code below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    Complete ETL Pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = setup_logging()\n",
    "        self.stats = {\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'records_extracted': 0,\n",
    "            'records_transformed': 0,\n",
    "            'records_loaded': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "    \n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extract from all sources\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Transform and clean data\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def load(self, data):\n",
    "        \"\"\"\n",
    "        Load to all destinations\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run complete pipeline\n",
    "        \"\"\"\n",
    "        self.stats['start_time'] = datetime.now()\n",
    "        self.logger.info(\"Starting pipeline\")\n",
    "        \n",
    "        # Your complete implementation here\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.stats['end_time'] = datetime.now()\n",
    "        self.print_summary()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"\n",
    "        Print pipeline summary\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PIPELINE SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        # Your code here\n",
    "        print(\"=\"*70)\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline = DataPipeline()\n",
    "pipeline.run()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 4: Final Challenge\n",
    "\n",
    "### Challenge: Build a Complete Automated System\n",
    "**Task:** Create an end-to-end automated data pipeline system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Challenge: Automated Data Pipeline System\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL CHALLENGE: AUTOMATED DATA PIPELINE SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TODO: Build a complete system that includes:\n",
    "#\n",
    "# 1. MULTI-SOURCE EXTRACTION\n",
    "#    - Extract from at least 3 API endpoints\n",
    "#    - Handle rate limiting\n",
    "#    - Implement retries\n",
    "#\n",
    "# 2. ADVANCED TRANSFORMATION\n",
    "#    - Clean and validate data\n",
    "#    - Merge data from multiple sources\n",
    "#    - Calculate derived metrics\n",
    "#    - Handle missing data\n",
    "#\n",
    "# 3. MULTI-DESTINATION LOADING\n",
    "#    - Save to SQLite database (normalized schema)\n",
    "#    - Export to CSV for reports\n",
    "#    - Create JSON for APIs\n",
    "#\n",
    "# 4. COMPREHENSIVE LOGGING\n",
    "#    - Log all operations\n",
    "#    - Different log levels (INFO, WARNING, ERROR)\n",
    "#    - Structured log messages\n",
    "#\n",
    "# 5. ERROR HANDLING\n",
    "#    - Try-except at every stage\n",
    "#    - Graceful degradation\n",
    "#    - Error recovery strategies\n",
    "#\n",
    "# 6. MONITORING & ALERTS\n",
    "#    - Track pipeline health\n",
    "#    - Measure performance metrics\n",
    "#    - Generate alerts on failures\n",
    "#\n",
    "# 7. SCHEDULING\n",
    "#    - Run on schedule (demo with loops)\n",
    "#    - Skip if already running\n",
    "#    - Maintain run history\n",
    "#\n",
    "# 8. REPORTING\n",
    "#    - Generate summary reports\n",
    "#    - Track historical metrics\n",
    "#    - Visualize pipeline performance\n",
    "\n",
    "# TODO: Write your complete solution below\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Your comprehensive implementation here\n",
    "# This should be a complete, production-ready pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”¹ Section 5: Summary & Key Concepts\n",
    "\n",
    "### ğŸ¯ What You Learned Today:\n",
    "\n",
    "**1. ETL Concept:**\n",
    "- **E**xtract: Get data from sources (APIs, files, databases)\n",
    "- **T**ransform: Clean and process data (filter, merge, calculate)\n",
    "- **L**oad: Save data to destinations (files, databases)\n",
    "\n",
    "**2. Building Pipelines:**\n",
    "- Break complex tasks into small functions\n",
    "- Extract â†’ Transform â†’ Load pattern\n",
    "- Combine functions into complete pipeline\n",
    "- Return data from each stage\n",
    "\n",
    "**3. Logging:**\n",
    "```python\n",
    "import logging\n",
    "\n",
    "# Setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Use\n",
    "logger.info(\"This is information\")      # Normal events\n",
    "logger.warning(\"This is a warning\")     # Potential issues\n",
    "logger.error(\"This is an error\")        # Errors occurred\n",
    "```\n",
    "\n",
    "**4. Error Handling:**\n",
    "```python\n",
    "try:\n",
    "    # Your code\n",
    "    result = risky_operation()\n",
    "except SpecificError as e:\n",
    "    # Handle specific error\n",
    "    logger.error(f\"Operation failed: {e}\")\n",
    "    return None\n",
    "except Exception as e:\n",
    "    # Handle any other error\n",
    "    logger.error(f\"Unexpected error: {e}\")\n",
    "    return None\n",
    "```\n",
    "\n",
    "**5. Pipeline Best Practices:**\n",
    "- âœ… One function = one responsibility\n",
    "- âœ… Log important events\n",
    "- âœ… Handle errors at each stage\n",
    "- âœ… Return success/failure status\n",
    "- âœ… Measure execution time\n",
    "- âœ… Validate data at each step\n",
    "\n",
    "**6. Scheduling Concepts:**\n",
    "- **Cron** (Linux/Mac): Schedule with time expressions\n",
    "- **Task Scheduler** (Windows): GUI-based scheduling\n",
    "- **Apache Airflow**: Advanced workflow management\n",
    "- **Prefect**: Modern, Python-based orchestration\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ Key Takeaways:\n",
    "\n",
    "âœ… **Pipelines automate** repetitive data tasks  \n",
    "âœ… **ETL is a pattern** used everywhere in data engineering  \n",
    "âœ… **Logging helps debug** and monitor pipelines  \n",
    "âœ… **Error handling makes** pipelines robust  \n",
    "âœ… **Scheduling enables** automatic execution  \n",
    "âœ… **Start simple**, add complexity gradually  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Simple Pipeline Template:\n",
    "\n",
    "```python\n",
    "def simple_pipeline():\n",
    "    \"\"\"Simple ETL pipeline template\"\"\"\n",
    "    \n",
    "    # 1. EXTRACT\n",
    "    print(\"ğŸ“¥ Extracting...\")\n",
    "    data = extract_data()\n",
    "    if data is None:\n",
    "        return \"âŒ Extract failed\"\n",
    "    \n",
    "    # 2. TRANSFORM\n",
    "    print(\"ğŸ”„ Transforming...\")\n",
    "    clean_data = transform_data(data)\n",
    "    if clean_data is None:\n",
    "        return \"âŒ Transform failed\"\n",
    "    \n",
    "    # 3. LOAD\n",
    "    print(\"ğŸ’¾ Loading...\")\n",
    "    success = load_data(clean_data)\n",
    "    if not success:\n",
    "        return \"âŒ Load failed\"\n",
    "    \n",
    "    return \"âœ… Success!\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ Tools for Production:\n",
    "\n",
    "**Workflow Orchestration:**\n",
    "- **Apache Airflow**: Industry standard, feature-rich\n",
    "- **Prefect**: Modern, Python-first, easier to learn\n",
    "- **Luigi**: Spotify's tool, simpler than Airflow\n",
    "- **Dagster**: Data-aware orchestration\n",
    "\n",
    "**Scheduling:**\n",
    "- **Cron**: Built into Linux/Mac\n",
    "- **Windows Task Scheduler**: Built into Windows\n",
    "- **APScheduler**: Python-based scheduler\n",
    "- **Celery**: Distributed task queue\n",
    "\n",
    "**Monitoring:**\n",
    "- **Logging**: Python's logging module\n",
    "- **Prometheus**: Metrics collection\n",
    "- **Grafana**: Visualization dashboard\n",
    "- **Sentry**: Error tracking\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š When to Use What:\n",
    "\n",
    "| Need | Tool | Why |\n",
    "|------|------|-----|\n",
    "| Simple daily task | Cron | Built-in, simple |\n",
    "| Complex workflows | Airflow | Powerful, scalable |\n",
    "| Python-first | Prefect | Modern, easy |\n",
    "| Real-time processing | Kafka + Spark | Stream processing |\n",
    "| Small projects | Python scripts | No overhead |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next Steps:\n",
    "\n",
    "**Beginner:**\n",
    "1. Practice building simple ETL scripts\n",
    "2. Add logging to your existing code\n",
    "3. Learn basic SQL for databases\n",
    "\n",
    "**Intermediate:**\n",
    "1. Try Apache Airflow tutorials\n",
    "2. Build pipelines with multiple data sources\n",
    "3. Learn about data quality checks\n",
    "\n",
    "**Advanced:**\n",
    "1. Study streaming data (Kafka)\n",
    "2. Learn cloud services (AWS, GCP)\n",
    "3. Master Airflow/Prefect\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’ª Skills Checklist:\n",
    "\n",
    "By now you should be able to:\n",
    "- âœ… Build Extract functions\n",
    "- âœ… Build Transform functions\n",
    "- âœ… Build Load functions\n",
    "- âœ… Combine into complete pipeline\n",
    "- âœ… Add logging to track progress\n",
    "- âœ… Handle errors gracefully\n",
    "- âœ… Measure pipeline performance\n",
    "- âœ… Understand scheduling concepts\n",
    "- âœ… Create production-ready pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Remember:\n",
    "\n",
    "> *\"A data pipeline is just a series of small, simple steps working together!\"*\n",
    "\n",
    "- Start with working code\n",
    "- Add logging and error handling\n",
    "- Test thoroughly\n",
    "- Deploy and monitor\n",
    "- Iterate and improve\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You've completed Week 4!**\n",
    "\n",
    "You now understand:\n",
    "- âœ… Day 1: Data Formats and Storage\n",
    "- âœ… Day 2: APIs and External Data\n",
    "- âœ… Day 3: Data Pipelines and Automation\n",
    "\n",
    "**You're ready to build automated data systems for Machine Learning! ğŸš€**\n",
    "\n",
    "Next up: **Machine Learning fundamentals!** ğŸ¤–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
