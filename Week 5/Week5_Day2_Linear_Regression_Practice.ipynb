{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì AI Bootcamp Practice Exercises\n",
    "## Week 5 - Day 2: Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Topics Covered Today:\n",
    "- Understanding the linear equation (y = mx + b)\n",
    "- Cost Function (Mean Squared Error)\n",
    "- Gradient Descent optimization\n",
    "- Building Linear Regression from scratch with NumPy\n",
    "- Visualizing predictions and model performance\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Instructions:\n",
    "- Read each **Example** carefully before attempting exercises\n",
    "- Fill in the **TODO** sections with your code\n",
    "- Run each cell to check your output\n",
    "- Expected outputs are provided for reference\n",
    "- Ask for help if you get stuck!\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîπ Section 1: Introduction & Setup\n",
    "\n",
    "**Linear Regression** is your first machine learning algorithm! It's a supervised learning technique used to predict a continuous value (like house prices, temperatures, or salaries) based on input features.\n",
    "\n",
    "**Why Linear Regression?**\n",
    "- üéØ **Simple & Interpretable**: Easy to understand and explain\n",
    "- üìà **Foundation of ML**: Many advanced algorithms build on this concept\n",
    "- üöÄ **Fast Training**: Computationally efficient\n",
    "- üîç **Great Baseline**: Always try linear regression first!\n",
    "\n",
    "**The Linear Equation:**\n",
    "```\n",
    "y = mx + b\n",
    "```\n",
    "Where:\n",
    "- **y**: Predicted value (output)\n",
    "- **m**: Slope (how much y changes per unit of x)\n",
    "- **x**: Input feature\n",
    "- **b**: Intercept (y-value when x = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîπ Section 2: Basic Exercises\n",
    "\n",
    "### Exercise 1: Understanding the Linear Equation\n",
    "**Learn:** Get comfortable with the equation y = mx + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìñ Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate y for different values of x\n",
    "# Let's say: House Price = $50,000 per bedroom + $100,000 base price\n",
    "\n",
    "m = 50000  # slope (price per bedroom)\n",
    "b = 100000  # intercept (base price)\n",
    "\n",
    "# Predict for different number of bedrooms\n",
    "bedrooms = np.array([1, 2, 3, 4, 5])\n",
    "prices = m * bedrooms + b\n",
    "\n",
    "print(\"Number of Bedrooms | Predicted Price\")\n",
    "print(\"-\" * 40)\n",
    "for bed, price in zip(bedrooms, prices):\n",
    "    print(f\"      {bed}            | ${price:,}\")\n",
    "\n",
    "# Expected Output:\n",
    "# Number of Bedrooms | Predicted Price\n",
    "# ----------------------------------------\n",
    "#       1            | $150,000\n",
    "#       2            | $200,000\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úèÔ∏è Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Salary Prediction\n",
    "# A company pays $5,000 per year of experience + $30,000 base salary\n",
    "# Calculate salaries for people with 0, 2, 5, 10, and 15 years of experience\n",
    "\n",
    "# TODO: Define m (slope) and b (intercept)\n",
    "m = # Your code here (5000)\n",
    "b = # Your code here (30000)\n",
    "\n",
    "# TODO: Create array of years of experience\n",
    "experience_years = # Your code here (use np.array)\n",
    "\n",
    "# TODO: Calculate predicted salaries\n",
    "predicted_salaries = # Your code here\n",
    "\n",
    "# Print results\n",
    "print(\"Years of Experience | Predicted Salary\")\n",
    "print(\"-\" * 45)\n",
    "for years, salary in zip(experience_years, predicted_salaries):\n",
    "    print(f\"        {years:2d}          | ${salary:,}\")\n",
    "\n",
    "# Expected Output:\n",
    "# Years of Experience | Predicted Salary\n",
    "# ---------------------------------------------\n",
    "#         0          | $30,000\n",
    "#         2          | $40,000\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2: Visualizing the Linear Relationship\n",
    "**Learn:** See how changing m and b affects the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìñ Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualize different lines\n",
    "\n",
    "x = np.linspace(0, 10, 50)\n",
    "\n",
    "# Different slopes, same intercept\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, 0.5*x + 2, label='m=0.5, b=2', linewidth=2)\n",
    "plt.plot(x, 1.0*x + 2, label='m=1.0, b=2', linewidth=2)\n",
    "plt.plot(x, 2.0*x + 2, label='m=2.0, b=2', linewidth=2)\n",
    "plt.title('Different Slopes (m)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Same slope, different intercepts\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, 1.0*x + 1, label='m=1.0, b=1', linewidth=2)\n",
    "plt.plot(x, 1.0*x + 3, label='m=1.0, b=3', linewidth=2)\n",
    "plt.plot(x, 1.0*x + 5, label='m=1.0, b=5', linewidth=2)\n",
    "plt.title('Different Intercepts (b)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Notice how:\")\n",
    "print(\"   - Slope (m) controls how steep the line is\")\n",
    "print(\"   - Intercept (b) controls where the line crosses the y-axis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úèÔ∏è Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own visualization\n",
    "# Plot three lines with:\n",
    "# Line 1: m=3, b=1\n",
    "# Line 2: m=-2, b=8\n",
    "# Line 3: m=0, b=5 (horizontal line)\n",
    "\n",
    "x = np.linspace(0, 10, 50)\n",
    "\n",
    "# TODO: Calculate y values for each line\n",
    "y1 = # Your code here\n",
    "y2 = # Your code here\n",
    "y3 = # Your code here\n",
    "\n",
    "# TODO: Plot all three lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Your plotting code here\n",
    "\n",
    "\n",
    "plt.title('My Linear Functions', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 3: Generate Sample Data\n",
    "**Learn:** Create synthetic data to practice linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìñ Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate data with noise\n",
    "\n",
    "# True parameters\n",
    "true_m = 2.5\n",
    "true_b = 5.0\n",
    "\n",
    "# Generate x values\n",
    "n_samples = 100\n",
    "X = np.random.uniform(0, 10, n_samples)\n",
    "\n",
    "# Generate y values with some noise\n",
    "noise = np.random.normal(0, 2, n_samples)  # mean=0, std=2\n",
    "y = true_m * X + true_b + noise\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, s=50, label='Data points')\n",
    "plt.plot(X, true_m * X + true_b, 'r--', linewidth=2, label=f'True line: y = {true_m}x + {true_b}')\n",
    "plt.title('Sample Data with Noise', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {n_samples} data points\")\n",
    "print(f\"True parameters: m = {true_m}, b = {true_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úèÔ∏è Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate your own dataset\n",
    "# Create 50 data points with true_m=3, true_b=10, noise std=3\n",
    "\n",
    "# Set parameters\n",
    "true_m = # Your code here\n",
    "true_b = # Your code here\n",
    "n_samples = # Your code here\n",
    "\n",
    "# Generate data\n",
    "X = # np.random.uniform(0, 20, n_samples)\n",
    "noise = # np.random.normal(0, 3, n_samples)\n",
    "y = # true_m * X + true_b + noise\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Your plotting code here\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîπ Section 3: Intermediate Exercises\n",
    "\n",
    "### Exercise 4: Cost Function (Mean Squared Error)\n",
    "**Learn:** Measure how good our predictions are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìñ Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate MSE for different parameters\n",
    "\n",
    "def calculate_mse(X, y, m, b):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error\n",
    "    MSE = (1/n) * Œ£(y_true - y_pred)¬≤\n",
    "    \"\"\"\n",
    "    y_pred = m * X + b\n",
    "    mse = np.mean((y - y_pred) ** 2)\n",
    "    return mse\n",
    "\n",
    "# Using the data from Exercise 3\n",
    "# Try different parameter values\n",
    "test_params = [\n",
    "    (2.0, 5.0),\n",
    "    (2.5, 5.0),  # True parameters\n",
    "    (3.0, 5.0),\n",
    "    (2.5, 3.0),\n",
    "]\n",
    "\n",
    "print(\"Testing different parameters:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'m':<8} {'b':<8} {'MSE':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for m, b in test_params:\n",
    "    mse = calculate_mse(X, y, m, b)\n",
    "    print(f\"{m:<8.2f} {b:<8.2f} {mse:<15.2f}\")\n",
    "\n",
    "print(\"\\nüìä Lower MSE = Better fit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úèÔ∏è Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the MSE function and test it\n",
    "\n",
    "def my_mse(X, y, m, b):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error\n",
    "    \n",
    "    Parameters:\n",
    "    X: input features\n",
    "    y: true values\n",
    "    m: slope\n",
    "    b: intercept\n",
    "    \n",
    "    Returns:\n",
    "    mse: mean squared error\n",
    "    \"\"\"\n",
    "    # TODO: Calculate predictions\n",
    "    y_pred = # Your code here\n",
    "    \n",
    "    # TODO: Calculate squared errors\n",
    "    squared_errors = # Your code here\n",
    "    \n",
    "    # TODO: Calculate mean\n",
    "    mse = # Your code here\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Test your function\n",
    "test_mse = my_mse(X, y, 2.5, 5.0)\n",
    "print(f\"MSE with m=2.5, b=5.0: {test_mse:.2f}\")\n",
    "\n",
    "# TODO: Try finding better parameters by trial and error\n",
    "# Test at least 5 different combinations of m and b\n",
    "# Print which combination gives the lowest MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîπ Section 4: Advanced Exercises\n",
    "\n",
    "### Exercise 5: Implement Linear Regression from Scratch\n",
    "**Learn:** Build the complete algorithm using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìñ Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete Linear Regression implementation\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.m = 0  # slope\n",
    "        self.b = 0  # intercept\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # Make predictions\n",
    "            y_pred = self.m * X + self.b\n",
    "            \n",
    "            # Calculate cost (MSE)\n",
    "            cost = (1/n_samples) * np.sum((y - y_pred) ** 2)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            dm = -(2/n_samples) * np.sum(X * (y - y_pred))\n",
    "            db = -(2/n_samples) * np.sum(y - y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.m = self.m - self.learning_rate * dm\n",
    "            self.b = self.b - self.learning_rate * db\n",
    "            \n",
    "            # Print progress every 100 iterations\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"Iteration {i+1}: Cost = {cost:.4f}, m = {self.m:.4f}, b = {self.b:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        return self.m * X + self.b\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Final parameters: m = {model.m:.4f}, b = {model.b:.4f}\")\n",
    "print(f\"True parameters: m = {true_m}, b = {true_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìä Visualize Training Progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost function over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.cost_history, linewidth=2)\n",
    "plt.title('Cost Function Over Time', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìâ The cost decreases over time - our model is learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìà Visualize Final Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, s=50, label='Actual data')\n",
    "plt.plot(X, y_pred, 'r-', linewidth=2, label=f'Predicted line: y = {model.m:.2f}x + {model.b:.2f}')\n",
    "plt.plot(X, true_m * X + true_b, 'g--', linewidth=2, alpha=0.7, label=f'True line: y = {true_m}x + {true_b}')\n",
    "plt.title('Linear Regression Results', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate final MSE\n",
    "final_mse = np.mean((y - y_pred) ** 2)\n",
    "print(f\"Final MSE: {final_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úèÔ∏è Your Turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a simplified version of Linear Regression\n",
    "# Complete the missing parts\n",
    "\n",
    "class MyLinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=500):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.m = 0\n",
    "        self.b = 0\n",
    "        self.costs = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n = len(X)\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # TODO: Calculate predictions\n",
    "            y_pred = # Your code here\n",
    "            \n",
    "            # TODO: Calculate MSE cost\n",
    "            cost = # Your code here\n",
    "            self.costs.append(cost)\n",
    "            \n",
    "            # TODO: Calculate gradients\n",
    "            dm = # Your code here: -(2/n) * sum(X * (y - y_pred))\n",
    "            db = # Your code here: -(2/n) * sum(y - y_pred)\n",
    "            \n",
    "            # TODO: Update parameters\n",
    "            self.m = # Your code here\n",
    "            self.b = # Your code here\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.m * X + self.b\n",
    "\n",
    "# Test your implementation\n",
    "my_model = MyLinearRegression(learning_rate=0.01, n_iterations=500)\n",
    "my_model.fit(X, y)\n",
    "\n",
    "print(f\"Your model: m = {my_model.m:.4f}, b = {my_model.b:.4f}\")\n",
    "print(f\"Example model: m = {model.m:.4f}, b = {model.b:.4f}\")\n",
    "\n",
    "# TODO: Visualize your results\n",
    "# Plot the cost history and final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîπ Section 5: Final Challenge\n",
    "\n",
    "### Challenge: Real Dataset - Salary Prediction\n",
    "**Task:** Apply Linear Regression to predict salaries based on years of experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Salary Prediction Dataset\n",
    "\n",
    "# Create realistic salary data\n",
    "np.random.seed(42)\n",
    "years_experience = np.array([1.1, 1.3, 1.5, 2.0, 2.2, 2.9, 3.0, 3.2, 3.2, 3.7, 3.9, 4.0, 4.0, 4.1, 4.5, 4.9, 5.1, 5.3, 5.9, 6.0, 6.8, 7.1, 7.9, 8.2, 8.7, 9.0, 9.5, 9.6, 10.3, 10.5])\n",
    "salary = np.array([39343, 46205, 37731, 43525, 39891, 56642, 60150, 54445, 64445, 57189, 63218, 55794, 56957, 57081, 61111, 67938, 66029, 83088, 81363, 93940, 91738, 98273, 101302, 113812, 109431, 105582, 116969, 112635, 122391, 121872])\n",
    "\n",
    "print(\"üìä Salary Dataset\")\n",
    "print(f\"Number of samples: {len(years_experience)}\")\n",
    "print(f\"Experience range: {years_experience.min():.1f} - {years_experience.max():.1f} years\")\n",
    "print(f\"Salary range: ${salary.min():,} - ${salary.max():,}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(years_experience, salary, alpha=0.6, s=80, color='blue', edgecolors='black')\n",
    "plt.title('Salary vs Years of Experience', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the challenge\n",
    "# 1. Train a Linear Regression model on this data\n",
    "# 2. Plot the cost function history\n",
    "# 3. Visualize predictions vs actual data\n",
    "# 4. Make predictions for someone with 5 and 15 years of experience\n",
    "# 5. Calculate and print the final MSE\n",
    "\n",
    "# Step 1: Train model\n",
    "salary_model = # Your code here\n",
    "\n",
    "# Step 2: Plot cost history\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Visualize predictions\n",
    "# Your code here\n",
    "\n",
    "# Step 4: Make predictions\n",
    "pred_5_years = # Your code here\n",
    "pred_15_years = # Your code here\n",
    "\n",
    "print(f\"\\nüí∞ Salary Predictions:\")\n",
    "print(f\"5 years experience: ${pred_5_years:,.2f}\")\n",
    "print(f\"15 years experience: ${pred_15_years:,.2f}\")\n",
    "\n",
    "# Step 5: Calculate MSE\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîπ Section 6: Summary & Key Concepts\n",
    "\n",
    "### üéØ What You Learned Today:\n",
    "\n",
    "**1. Linear Regression Fundamentals:**\n",
    "- The equation: `y = mx + b`\n",
    "- Slope (m) controls the steepness\n",
    "- Intercept (b) is the starting point\n",
    "- Used for predicting continuous values\n",
    "\n",
    "**2. Cost Function (MSE):**\n",
    "- Measures prediction error\n",
    "- MSE = Average of squared errors\n",
    "- Lower MSE = Better model\n",
    "- Goal: Minimize MSE\n",
    "\n",
    "**3. Gradient Descent:**\n",
    "- Optimization algorithm to find best parameters\n",
    "- Iteratively updates m and b\n",
    "- Learning rate controls step size\n",
    "- Converges to minimum error\n",
    "\n",
    "**4. Implementation:**\n",
    "- Built Linear Regression from scratch using NumPy\n",
    "- Trained models on synthetic and real data\n",
    "- Visualized results and training progress\n",
    "- Made predictions on new data\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Key Takeaways:\n",
    "\n",
    "‚úÖ **Linear Regression is simple yet powerful** - great baseline for any regression problem  \n",
    "‚úÖ **Gradient Descent finds optimal parameters** - used in most ML algorithms  \n",
    "‚úÖ **Visualization helps understanding** - always plot your data and results  \n",
    "‚úÖ **Lower learning rate = slower but more stable** - higher = faster but may overshoot  \n",
    "‚úÖ **More iterations = better convergence** - but watch for diminishing returns  \n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "- Tomorrow: **Polynomial Features and Regularization**\n",
    "- Learn about overfitting and underfitting\n",
    "- Explore Ridge and Lasso regression\n",
    "- Build more complex models\n",
    "\n",
    "---\n",
    "\n",
    "### üí™ Keep Practicing!\n",
    "\n",
    "Try these additional challenges:\n",
    "1. Experiment with different learning rates\n",
    "2. Add more noise to the data and see how it affects results\n",
    "3. Try Linear Regression on different datasets\n",
    "4. Compare your implementation with scikit-learn\n",
    "\n",
    "**Congratulations on building your first ML algorithm from scratch! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
