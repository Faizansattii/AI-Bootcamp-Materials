{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ AI Bootcamp - Week 5 Day 4\n",
    "## Feature Engineering: Complete Hands-on Guide\n",
    "\n",
    "Today you'll learn:\n",
    "- âœ… Categorical variable encoding\n",
    "- âœ… Feature scaling techniques  \n",
    "- âœ… PCA for dimensionality reduction\n",
    "- âœ… Real Titanic dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print('âœ… Ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Categorical Encoding\n",
    "\n",
    "ML models need numbers, not text! Let's convert categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple categorical data\n",
    "colors = pd.Series(['Red', 'Blue', 'Green', 'Red', 'Blue'])\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "colors_encoded = le.fit_transform(colors)\n",
    "print('Label Encoded:', colors_encoded)\n",
    "\n",
    "# One-Hot Encoding\n",
    "colors_onehot = pd.get_dummies(colors, prefix='color')\n",
    "print('\\nOne-Hot Encoded:\\n', colors_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Scaling\n",
    "\n",
    "Put all features on the same scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data with different scales\n",
    "data = pd.DataFrame({\n",
    "    'Age': [25, 30, 35, 40],\n",
    "    'Salary': [50000, 60000, 75000, 90000]\n",
    "})\n",
    "\n",
    "print('Original data:\\n', data)\n",
    "\n",
    "# Standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "data_std = pd.DataFrame(\n",
    "    scaler.fit_transform(data),\n",
    "    columns=['Age_std', 'Salary_std']\n",
    ")\n",
    "print('\\nStandardized:\\n', data_std)\n",
    "\n",
    "# Normalization (0-1 range)\n",
    "normalizer = MinMaxScaler()\n",
    "data_norm = pd.DataFrame(\n",
    "    normalizer.fit_transform(data),\n",
    "    columns=['Age_norm', 'Salary_norm']\n",
    ")\n",
    "print('\\nNormalized:\\n', data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: PCA - Principal Component Analysis\n",
    "\n",
    "Reduce dimensions while keeping variance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample high-dimensional data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=200, n_features=10, \n",
    "                           n_informative=8, n_redundant=2, \n",
    "                           random_state=42)\n",
    "\n",
    "print(f'Original shape: {X.shape}')\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f'After PCA: {X_pca.shape}')\n",
    "print(f'Variance explained: {pca.explained_variance_ratio_}')\n",
    "print(f'Total variance: {sum(pca.explained_variance_ratio_):.2%}')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA: 10D â†’ 2D')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: TITANIC DATASET - Complete Example\n",
    "\n",
    "Let's apply everything to the famous Titanic dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic data\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'\\nFirst few rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values and data types\n",
    "print('Missing values:\\n', df.isnull().sum())\n",
    "print('\\nData types:\\n', df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "# Select features\n",
    "df_clean = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].copy()\n",
    "\n",
    "# Handle missing values\n",
    "df_clean['Age'].fillna(df_clean['Age'].median(), inplace=True)\n",
    "df_clean['Fare'].fillna(df_clean['Fare'].median(), inplace=True)\n",
    "df_clean['Embarked'].fillna(df_clean['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "df_clean['Sex'] = LabelEncoder().fit_transform(df_clean['Sex'])\n",
    "df_clean['Embarked'] = LabelEncoder().fit_transform(df_clean['Embarked'])\n",
    "\n",
    "print('Clean data:')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_clean.drop('Survived', axis=1)\n",
    "y = df_clean['Survived']\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (required for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print('Features after standardization:')\n",
    "print(pd.DataFrame(X_scaled, columns=X.columns).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca_full = PCA()\n",
    "X_pca_full = pca_full.fit_transform(X_scaled)\n",
    "\n",
    "# Plot variance explained\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n",
    "        pca_full.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.title('Variance by Component')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_)+1),\n",
    "         np.cumsum(pca_full.explained_variance_ratio_), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Variance by component: {pca_full.explained_variance_ratio_}')\n",
    "print(f'\\nComponents needed for 95% variance: '\n",
    "      f'{np.argmax(np.cumsum(pca_full.explained_variance_ratio_) >= 0.95) + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "survived = y == 1\n",
    "plt.scatter(X_pca_2d[survived, 0], X_pca_2d[survived, 1], \n",
    "           c='green', label='Survived', alpha=0.6)\n",
    "plt.scatter(X_pca_2d[~survived, 0], X_pca_2d[~survived, 1], \n",
    "           c='red', label='Did not survive', alpha=0.6)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Titanic Data: 7D â†’ 2D with PCA')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f'2D variance explained: {sum(pca_2d.explained_variance_ratio_):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance: Original vs PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train on original features\n",
    "model_original = LogisticRegression(max_iter=1000)\n",
    "model_original.fit(X_train, y_train)\n",
    "acc_original = accuracy_score(y_test, model_original.predict(X_test))\n",
    "\n",
    "# Train on PCA features (5 components)\n",
    "pca_5 = PCA(n_components=5)\n",
    "X_train_pca = pca_5.fit_transform(X_train)\n",
    "X_test_pca = pca_5.transform(X_test)\n",
    "\n",
    "model_pca = LogisticRegression(max_iter=1000)\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "acc_pca = accuracy_score(y_test, model_pca.predict(X_test_pca))\n",
    "\n",
    "print(f'Accuracy with {X_train.shape[1]} original features: {acc_original:.3f}')\n",
    "print(f'Accuracy with 5 PCA components: {acc_pca:.3f}')\n",
    "print(f'\\nVariance kept with 5 components: {sum(pca_5.explained_variance_ratio_):.2%}')\n",
    "print(f'\\nâœ… Using {5/X_train.shape[1]:.1%} of features, '\n",
    "      f'we kept {acc_pca/acc_original:.1%} of performance!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Your Challenge\n",
    "\n",
    "Try the following:\n",
    "1. Add more features from Titanic (Name length, Cabin, etc.)\n",
    "2. Try different numbers of PCA components\n",
    "3. Compare StandardScaler vs MinMaxScaler for PCA\n",
    "4. Visualize feature importance before and after PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "Today you learned:\n",
    "- âœ… Label encoding for ordinal data\n",
    "- âœ… One-hot encoding for nominal data\n",
    "- âœ… Standardization & normalization\n",
    "- âœ… PCA for dimensionality reduction\n",
    "- âœ… Real-world application on Titanic!\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Always standardize before PCA\n",
    "- Check variance explained\n",
    "- Balance dimensionality vs performance\n",
    "- Feature engineering is iterative!\n",
    "\n",
    "**Next:** Week 6 - More ML algorithms! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
