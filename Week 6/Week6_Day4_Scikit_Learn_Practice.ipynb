{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udf93 Week 6 - Day 4: Scikit-Learn Basics\n",
        "\n",
        "## Today's Goals:\n",
        "\u2705 Understand the Scikit-Learn API\n",
        "\n",
        "\u2705 Learn data splitting and scaling\n",
        "\n",
        "\u2705 Build ML pipelines\n",
        "\n",
        "\u2705 Train, predict, and evaluate models\n",
        "\n",
        "\u2705 Save and load models with joblib\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-Learn imports\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('\u2705 Libraries imported!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: The Scikit-Learn Workflow\n",
        "\n",
        "**4 Simple Steps:**\n",
        "1. Import & Instantiate\n",
        "2. Fit (Train)\n",
        "3. Predict\n",
        "4. Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(f'Dataset: {iris.data.shape[0]} samples, {iris.data.shape[1]} features')\n",
        "print(f'Classes: {iris.target_names}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Import & Instantiate\n",
        "model = LogisticRegression(max_iter=200)\n",
        "print('Step 1: \u2705 Model created')\n",
        "\n",
        "# Step 2: Fit (Train)\n",
        "model.fit(X, y)\n",
        "print('Step 2: \u2705 Model trained')\n",
        "\n",
        "# Step 3: Predict\n",
        "predictions = model.predict(X[:5])\n",
        "print('Step 3: \u2705 Predictions made')\n",
        "print(f'Predictions: {predictions}')\n",
        "\n",
        "# Step 4: Evaluate\n",
        "accuracy = model.score(X, y)\n",
        "print('Step 4: \u2705 Model evaluated')\n",
        "print(f'Accuracy: {accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Train-Test Split\n",
        "\n",
        "**Never test on training data!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f'Training samples: {len(X_train)}')\n",
        "print(f'Test samples: {len(X_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on training data\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Test on test data\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.3f}')\n",
        "print(f'\\nSample predictions (first 5):')\n",
        "for i in range(5):\n",
        "    print(f'  True: {y_test[i]}, Predicted: {y_pred[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Feature Scaling\n",
        "\n",
        "**Why scale?** Different features have different ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample data with different scales\n",
        "sample_data = np.array([\n",
        "    [25, 50000],  # Age: 25, Salary: $50,000\n",
        "    [30, 60000],  # Age: 30, Salary: $60,000\n",
        "    [35, 70000]   # Age: 35, Salary: $70,000\n",
        "])\n",
        "\n",
        "print('Original data:')\n",
        "print(sample_data)\n",
        "print(f'\\nAge range: {sample_data[:, 0].min()} - {sample_data[:, 0].max()}')\n",
        "print(f'Salary range: ${sample_data[:, 1].min()} - ${sample_data[:, 1].max()}')\n",
        "print('\\n\u26a0\ufe0f Very different scales!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(sample_data)\n",
        "\n",
        "print('After StandardScaler:')\n",
        "print(scaled_data)\n",
        "print('\\n\u2705 Now both features are on similar scale!')\n",
        "print(f'Mean \u2248 0, Std Dev \u2248 1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaling Impact on Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WITHOUT scaling\n",
        "svm_no_scale = SVC()\n",
        "svm_no_scale.fit(X_train, y_train)\n",
        "score_no_scale = svm_no_scale.score(X_test, y_test)\n",
        "\n",
        "print('\u274c SVM without scaling:')\n",
        "print(f'   Accuracy: {score_no_scale:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WITH scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_scaled = SVC()\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "score_scaled = svm_scaled.score(X_test_scaled, y_test)\n",
        "\n",
        "print('\u2705 SVM WITH scaling:')\n",
        "print(f'   Accuracy: {score_scaled:.3f}')\n",
        "print(f'\\n\ud83d\udcc8 Improvement: {(score_scaled - score_no_scale)*100:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Pipelines - Clean ML Workflows\n",
        "\n",
        "**Pipelines chain steps together!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC())\n",
        "])\n",
        "\n",
        "print('\u2705 Pipeline created with 2 steps:')\n",
        "print('   1. StandardScaler')\n",
        "print('   2. SVM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train (scaling happens automatically!)\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('\u2705 Pipeline trained')\n",
        "\n",
        "# Predict (scaling happens automatically!)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.3f}')\n",
        "print('\\n\u2705 Much cleaner! Pipeline handles everything!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Cross-Validation\n",
        "\n",
        "**More reliable than single train-test split!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single train-test split\n",
        "pipeline.fit(X_train, y_train)\n",
        "single_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "print('Single Train-Test Split:')\n",
        "print(f'  Accuracy: {single_score:.3f}')\n",
        "print('  \u2192 Based on ONE split')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5-Fold Cross-Validation\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "\n",
        "print('\\n5-Fold Cross-Validation:')\n",
        "print(f'  Fold scores: {[\"{:.3f}\".format(s) for s in cv_scores]}')\n",
        "print(f'  Mean: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})')\n",
        "print('  \u2192 Based on 5 different splits')\n",
        "print('\\n\u2705 Cross-validation is more reliable!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Saving & Loading Models\n",
        "\n",
        "**Train once, use anywhere!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a model\n",
        "best_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "best_pipeline.fit(X_train, y_train)\n",
        "original_score = best_pipeline.score(X_test, y_test)\n",
        "\n",
        "print('\u2705 Model trained!')\n",
        "print(f'Accuracy: {original_score:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "joblib.dump(best_pipeline, 'iris_model.pkl')\n",
        "print('\u2705 Model saved as \"iris_model.pkl\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_model = joblib.load('iris_model.pkl')\n",
        "loaded_score = loaded_model.score(X_test, y_test)\n",
        "\n",
        "print('\u2705 Model loaded successfully!')\n",
        "print(f'\\nOriginal accuracy: {original_score:.3f}')\n",
        "print(f'Loaded accuracy: {loaded_score:.3f}')\n",
        "print('\\n\u2705 Scores match! Model saved and loaded correctly.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: Project - Compare Multiple Models on Titanic\n",
        "\n",
        "**Let's apply everything we learned!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Titanic dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "print('\u2705 Titanic dataset loaded!')\n",
        "print(f'Shape: {titanic.shape}')\n",
        "print(f'\\nFirst few rows:')\n",
        "titanic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare']\n",
        "df = titanic[features + ['survived']].copy()\n",
        "\n",
        "# Handle missing values\n",
        "df['age'].fillna(df['age'].median(), inplace=True)\n",
        "df['fare'].fillna(df['fare'].median(), inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode sex\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "print('\u2705 Data preprocessed!')\n",
        "print(f'Final shape: {df.shape}')\n",
        "print(f'Survival rate: {df[\"survived\"].mean():.2%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X = df[features].values\n",
        "y = df['survived'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f'Training: {len(X_train)} samples')\n",
        "print(f'Test: {len(X_test)} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create 3 different pipelines\n",
        "models = {\n",
        "    'Logistic Regression': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('lr', LogisticRegression(max_iter=200))\n",
        "    ]),\n",
        "    \n",
        "    'SVM': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('svm', SVC())\n",
        "    ]),\n",
        "    \n",
        "    'Random Forest': Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "    ])\n",
        "}\n",
        "\n",
        "print('\u2705 Created 3 pipelines:')\n",
        "for name in models.keys():\n",
        "    print(f'  \u2022 {name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and compare all models\n",
        "results = []\n",
        "\n",
        "for name, pipeline in models.items():\n",
        "    # Train\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    \n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': f\"{accuracy:.3f}\"\n",
        "    })\n",
        "    \n",
        "    print(f'\u2705 {name}: {accuracy:.3f}')\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results)\n",
        "print('\\n' + '='*40)\n",
        "print('\ud83c\udfc6 MODEL COMPARISON')\n",
        "print('='*40)\n",
        "print(results_df.to_string(index=False))\n",
        "print('='*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model\n",
        "accuracies = [float(r['Accuracy']) for r in results]\n",
        "best_idx = accuracies.index(max(accuracies))\n",
        "best_model_name = results[best_idx]['Model']\n",
        "\n",
        "print(f'\u2b50 Best Model: {best_model_name}')\n",
        "print(f'   Accuracy: {results[best_idx][\"Accuracy\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix for best model\n",
        "best_pipeline = models[best_model_name]\n",
        "y_pred = best_pipeline.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Died', 'Survived'],\n",
        "            yticklabels=['Died', 'Survived'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "joblib.dump(best_pipeline, 'titanic_best_model.pkl')\n",
        "print(f'\u2705 Best model ({best_model_name}) saved!')\n",
        "print(f'\\n\ud83c\udf89 Project Complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udcda Summary\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "**1. Scikit-Learn Workflow:**\n",
        "- Import \u2192 Fit \u2192 Predict \u2192 Evaluate\n",
        "- Consistent API for all algorithms\n",
        "\n",
        "**2. Train-Test Split:**\n",
        "- Never test on training data\n",
        "- Use `test_size=0.2` (20% for testing)\n",
        "\n",
        "**3. Feature Scaling:**\n",
        "- `StandardScaler`: Mean=0, Std=1\n",
        "- Critical for SVM, KNN, Neural Nets\n",
        "\n",
        "**4. Pipelines:**\n",
        "- Chain preprocessing + model\n",
        "- Prevents data leakage\n",
        "- Cleaner code\n",
        "\n",
        "**5. Cross-Validation:**\n",
        "- More reliable than single split\n",
        "- Use `cv=5` for 5-fold CV\n",
        "\n",
        "**6. Model Persistence:**\n",
        "- `joblib.dump()`: Save model\n",
        "- `joblib.load()`: Load model\n",
        "\n",
        "### \ud83c\udfaf Key Takeaways:\n",
        "- Always use pipelines for production code\n",
        "- Scale features for distance-based algorithms\n",
        "- Cross-validation > single train-test split\n",
        "- Compare multiple models to find the best\n",
        "- Save trained models for reuse\n",
        "\n",
        "---\n",
        "\n",
        "**Great job! You've mastered Scikit-Learn basics! \ud83c\udf89**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}