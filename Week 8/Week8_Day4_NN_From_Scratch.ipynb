{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udf93 Week 8 - Day 4: Neural Network from Scratch\n",
        "\n",
        "## Today's Goals:\n",
        "\u2705 Build a complete neural network using only NumPy\n",
        "\n",
        "\u2705 Implement forward propagation\n",
        "\n",
        "\u2705 Implement backpropagation\n",
        "\n",
        "\u2705 Train on MNIST digits (binary classification)\n",
        "\n",
        "\u2705 Evaluate with accuracy and loss curves\n",
        "\n",
        "---\n",
        "\n",
        "**This is it!** Today we put everything together and build a real neural network!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print('\u2705 Libraries imported!')\n",
        "print('\ud83d\ude80 Ready to build a neural network from scratch!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Load and Prepare Data\n",
        "\n",
        "**We'll use MNIST digits, but only classify 0 vs 1 (binary)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "print('\ud83d\udcca Original Dataset:')\n",
        "print(f'   Shape: {X.shape}')\n",
        "print(f'   Classes: {np.unique(y)}')\n",
        "print(f'   Total samples: {len(X)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for binary classification (0 vs 1)\n",
        "mask = (y == 0) | (y == 1)\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "print('\ud83c\udfaf Binary Classification Dataset:')\n",
        "print(f'   Filtered to classes: 0 and 1')\n",
        "print(f'   Shape: {X.shape}')\n",
        "print(f'   Samples per class:')\n",
        "print(f'      Class 0: {np.sum(y == 0)}')\n",
        "print(f'      Class 1: {np.sum(y == 1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "fig.suptitle('Sample Images from Dataset', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X[i].reshape(8, 8), cmap='gray')\n",
        "    ax.set_title(f'Label: {y[i]}', fontsize=12)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n\ud83d\udca1 Each image is 8\u00d78 pixels = 64 features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print('\ud83d\udcca Data Split:')\n",
        "print(f'   Training: {X_train.shape[0]} samples')\n",
        "print(f'   Testing: {X_test.shape[0]} samples')\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print('\\n\u2705 Features scaled (mean=0, std=1)')\n",
        "\n",
        "# Reshape y for consistency\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "print(f'\\n\ud83d\udcd0 Final shapes:')\n",
        "print(f'   X_train: {X_train.shape}')\n",
        "print(f'   y_train: {y_train.shape}')\n",
        "print(f'   X_test: {X_test.shape}')\n",
        "print(f'   y_test: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Define Activation Functions\n",
        "\n",
        "**We need both the function and its derivative**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function\n",
        "    Output range: (0, 1)\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    \"\"\"\n",
        "    Derivative of sigmoid\n",
        "    Used in backpropagation\n",
        "    \"\"\"\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "print('\u2705 Sigmoid activation functions defined')\n",
        "print('   \u2022 sigmoid(z): Forward pass')\n",
        "print('   \u2022 sigmoid_derivative(z): Backward pass')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test activation functions\n",
        "test_values = np.array([-2, -1, 0, 1, 2])\n",
        "sig_output = sigmoid(test_values)\n",
        "sig_deriv = sigmoid_derivative(test_values)\n",
        "\n",
        "print('\ud83d\udcca Sigmoid Test:')\n",
        "print(f'   Input:      {test_values}')\n",
        "print(f'   Sigmoid:    {sig_output}')\n",
        "print(f'   Derivative: {sig_deriv}')\n",
        "print('\\n\ud83d\udca1 Notice: sigmoid(0) = 0.5, sigmoid(large) \u2192 1, sigmoid(small) \u2192 0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Initialize Network Parameters\n",
        "\n",
        "**Network Architecture: 64 \u2192 16 \u2192 8 \u2192 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define architecture\n",
        "input_size = 64      # 8\u00d78 pixels\n",
        "hidden1_size = 16    # First hidden layer\n",
        "hidden2_size = 8     # Second hidden layer\n",
        "output_size = 1      # Binary classification\n",
        "\n",
        "print('\ud83c\udfd7\ufe0f  Network Architecture:')\n",
        "print(f'   Input Layer:    {input_size} neurons')\n",
        "print(f'   Hidden Layer 1: {hidden1_size} neurons')\n",
        "print(f'   Hidden Layer 2: {hidden2_size} neurons')\n",
        "print(f'   Output Layer:   {output_size} neuron')\n",
        "print(f'\\n   Flow: {input_size} \u2192 {hidden1_size} \u2192 {hidden2_size} \u2192 {output_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    Initialize weights and biases with small random values\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "    \n",
        "    # Layer 1: Input \u2192 Hidden1\n",
        "    params['W1'] = np.random.randn(hidden1_size, input_size) * 0.01\n",
        "    params['b1'] = np.zeros((hidden1_size, 1))\n",
        "    \n",
        "    # Layer 2: Hidden1 \u2192 Hidden2\n",
        "    params['W2'] = np.random.randn(hidden2_size, hidden1_size) * 0.01\n",
        "    params['b2'] = np.zeros((hidden2_size, 1))\n",
        "    \n",
        "    # Layer 3: Hidden2 \u2192 Output\n",
        "    params['W3'] = np.random.randn(output_size, hidden2_size) * 0.01\n",
        "    params['b3'] = np.zeros((output_size, 1))\n",
        "    \n",
        "    return params\n",
        "\n",
        "# Initialize\n",
        "parameters = initialize_parameters()\n",
        "\n",
        "print('\u2705 Parameters initialized!')\n",
        "print('\\n\ud83d\udcd0 Parameter shapes:')\n",
        "for key, value in parameters.items():\n",
        "    print(f'   {key}: {value.shape}')\n",
        "\n",
        "total_params = sum(p.size for p in parameters.values())\n",
        "print(f'\\n\ud83d\udd22 Total parameters: {total_params:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Implement Forward Propagation\n",
        "\n",
        "**Pass data through the network layer by layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_propagation(X, params):\n",
        "    \"\"\"\n",
        "    Forward pass through the network\n",
        "    \n",
        "    Returns:\n",
        "        A3: Final output (predictions)\n",
        "        cache: Dictionary of intermediate values (for backprop)\n",
        "    \"\"\"\n",
        "    # Get parameters\n",
        "    W1, b1 = params['W1'], params['b1']\n",
        "    W2, b2 = params['W2'], params['b2']\n",
        "    W3, b3 = params['W3'], params['b3']\n",
        "    \n",
        "    # Layer 1\n",
        "    Z1 = np.dot(W1, X.T) + b1  # Weighted sum\n",
        "    A1 = sigmoid(Z1)            # Activation\n",
        "    \n",
        "    # Layer 2\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    \n",
        "    # Layer 3 (Output)\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "    \n",
        "    # Store for backpropagation\n",
        "    cache = {\n",
        "        'Z1': Z1, 'A1': A1,\n",
        "        'Z2': Z2, 'A2': A2,\n",
        "        'Z3': Z3, 'A3': A3\n",
        "    }\n",
        "    \n",
        "    return A3, cache\n",
        "\n",
        "print('\u2705 Forward propagation function implemented!')\n",
        "print('\\n\ud83d\udca1 It performs:')\n",
        "print('   1. Layer 1: Z1 = W1\u00b7X + b1, A1 = sigmoid(Z1)')\n",
        "print('   2. Layer 2: Z2 = W2\u00b7A1 + b2, A2 = sigmoid(Z2)')\n",
        "print('   3. Layer 3: Z3 = W3\u00b7A2 + b3, A3 = sigmoid(Z3)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test forward propagation\n",
        "test_output, test_cache = forward_propagation(X_train[:5], parameters)\n",
        "\n",
        "print('\ud83e\uddea Testing forward propagation on 5 samples:')\n",
        "print(f'\\n   Output shape: {test_output.shape}')\n",
        "print(f'   Output values: {test_output.T.flatten()}')\n",
        "print('\\n\ud83d\udca1 These are predicted probabilities for class 1')\n",
        "print('   Values close to 1 \u2192 Predict class 1')\n",
        "print('   Values close to 0 \u2192 Predict class 0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 5: Compute Loss Function\n",
        "\n",
        "**Binary Cross-Entropy Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss(A3, Y):\n",
        "    \"\"\"\n",
        "    Binary Cross-Entropy Loss\n",
        "    \n",
        "    Loss = -[y*log(\u0177) + (1-y)*log(1-\u0177)]\n",
        "    \"\"\"\n",
        "    m = Y.shape[0]  # Number of samples\n",
        "    \n",
        "    # Clip values to avoid log(0)\n",
        "    A3 = np.clip(A3, 1e-7, 1 - 1e-7)\n",
        "    \n",
        "    # Calculate loss\n",
        "    loss = -np.mean(Y.T * np.log(A3) + (1 - Y.T) * np.log(1 - A3))\n",
        "    \n",
        "    return loss\n",
        "\n",
        "print('\u2705 Loss function implemented!')\n",
        "print('\\n\ud83d\udcd0 Binary Cross-Entropy Loss:')\n",
        "print('   Measures how wrong predictions are')\n",
        "print('   Lower loss = Better predictions')\n",
        "print('   Perfect predictions = Loss close to 0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test loss computation\n",
        "test_loss = compute_loss(test_output, y_train[:5])\n",
        "\n",
        "print(f'\ud83e\uddea Test loss on 5 samples: {test_loss:.4f}')\n",
        "print('\\n\ud83d\udca1 Before training, loss is typically around 0.69 (random guessing)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 6: Implement Backpropagation\n",
        "\n",
        "**Calculate gradients to update weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward_propagation(X, Y, params, cache):\n",
        "    \"\"\"\n",
        "    Backward pass - calculate gradients\n",
        "    \n",
        "    Returns:\n",
        "        grads: Dictionary of gradients for each parameter\n",
        "    \"\"\"\n",
        "    m = X.shape[0]  # Number of samples\n",
        "    \n",
        "    # Get cached values\n",
        "    A1, A2, A3 = cache['A1'], cache['A2'], cache['A3']\n",
        "    Z1, Z2, Z3 = cache['Z1'], cache['Z2'], cache['Z3']\n",
        "    \n",
        "    # Get parameters\n",
        "    W1, W2, W3 = params['W1'], params['W2'], params['W3']\n",
        "    \n",
        "    # Output layer gradient\n",
        "    dZ3 = A3 - Y.T  # Derivative of loss w.r.t. Z3\n",
        "    dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
        "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "    \n",
        "    # Hidden layer 2 gradient\n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = dA2 * sigmoid_derivative(Z2)\n",
        "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    \n",
        "    # Hidden layer 1 gradient\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = dA1 * sigmoid_derivative(Z1)\n",
        "    dW1 = (1/m) * np.dot(dZ1, X)\n",
        "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    \n",
        "    # Store gradients\n",
        "    grads = {\n",
        "        'dW1': dW1, 'db1': db1,\n",
        "        'dW2': dW2, 'db2': db2,\n",
        "        'dW3': dW3, 'db3': db3\n",
        "    }\n",
        "    \n",
        "    return grads\n",
        "\n",
        "print('\u2705 Backpropagation function implemented!')\n",
        "print('\\n\ud83d\udca1 It calculates gradients for all weights and biases')\n",
        "print('   Using the chain rule, working backward from output to input')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 7: Update Parameters\n",
        "\n",
        "**Gradient Descent: W = W - learning_rate \u00d7 gradient**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \"\"\"\n",
        "    params['W1'] -= learning_rate * grads['dW1']\n",
        "    params['b1'] -= learning_rate * grads['db1']\n",
        "    params['W2'] -= learning_rate * grads['dW2']\n",
        "    params['b2'] -= learning_rate * grads['db2']\n",
        "    params['W3'] -= learning_rate * grads['dW3']\n",
        "    params['b3'] -= learning_rate * grads['db3']\n",
        "    \n",
        "    return params\n",
        "\n",
        "print('\u2705 Parameter update function implemented!')\n",
        "print('\\n\ud83d\udcd0 Formula: W_new = W_old - \u03b1 \u00d7 gradient')\n",
        "print('   \u03b1 (alpha) = learning rate (controls step size)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 8: Training Loop\n",
        "\n",
        "**Put it all together and train the network!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_network(X_train, y_train, X_test, y_test, \n",
        "                  learning_rate=0.5, epochs=1000, print_every=100):\n",
        "    \"\"\"\n",
        "    Complete training loop\n",
        "    \"\"\"\n",
        "    # Initialize parameters\n",
        "    params = initialize_parameters()\n",
        "    \n",
        "    # Store history\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "    \n",
        "    print('\ud83d\ude80 Starting training...')\n",
        "    print(f'   Learning rate: {learning_rate}')\n",
        "    print(f'   Epochs: {epochs}')\n",
        "    print('\\n' + '='*60)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Forward propagation\n",
        "        A3_train, cache_train = forward_propagation(X_train, params)\n",
        "        \n",
        "        # Compute loss\n",
        "        train_loss = compute_loss(A3_train, y_train)\n",
        "        \n",
        "        # Backward propagation\n",
        "        grads = backward_propagation(X_train, y_train, params, cache_train)\n",
        "        \n",
        "        # Update parameters\n",
        "        params = update_parameters(params, grads, learning_rate)\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        A3_test, _ = forward_propagation(X_test, params)\n",
        "        test_loss = compute_loss(A3_test, y_test)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        train_pred = (A3_train > 0.5).astype(int)\n",
        "        test_pred = (A3_test > 0.5).astype(int)\n",
        "        train_acc = np.mean(train_pred.T == y_train)\n",
        "        test_acc = np.mean(test_pred.T == y_test)\n",
        "        \n",
        "        # Store history\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_accuracies.append(test_acc)\n",
        "        \n",
        "        # Print progress\n",
        "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
        "            print(f'Epoch {epoch+1:4d}/{epochs} | '\n",
        "                  f'Train Loss: {train_loss:.4f} | '\n",
        "                  f'Test Loss: {test_loss:.4f} | '\n",
        "                  f'Train Acc: {train_acc:.4f} | '\n",
        "                  f'Test Acc: {test_acc:.4f}')\n",
        "    \n",
        "    print('='*60)\n",
        "    print('\\n\u2705 Training complete!')\n",
        "    \n",
        "    history = {\n",
        "        'train_losses': train_losses,\n",
        "        'test_losses': test_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'test_accuracies': test_accuracies\n",
        "    }\n",
        "    \n",
        "    return params, history\n",
        "\n",
        "print('\u2705 Training function ready!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the network!\n",
        "trained_params, history = train_network(\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    learning_rate=0.5,\n",
        "    epochs=1000,\n",
        "    print_every=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 9: Visualize Training Progress\n",
        "\n",
        "**Loss and accuracy curves**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot loss curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_losses'], label='Train Loss', linewidth=2)\n",
        "axes[0].plot(history['test_losses'], label='Test Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Loss Curve', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history['train_accuracies'], label='Train Accuracy', linewidth=2)\n",
        "axes[1].plot(history['test_accuracies'], label='Test Accuracy', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[1].set_title('Accuracy Curve', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n\ud83d\udca1 What to look for:')\n",
        "print('   \u2705 Loss should decrease over time')\n",
        "print('   \u2705 Accuracy should increase over time')\n",
        "print('   \u2705 Train and test curves should be close (not overfitting)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 10: Final Evaluation\n",
        "\n",
        "**How well did our network learn?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final predictions\n",
        "A3_final, _ = forward_propagation(X_test, trained_params)\n",
        "final_predictions = (A3_final > 0.5).astype(int).T\n",
        "\n",
        "# Calculate metrics\n",
        "final_accuracy = np.mean(final_predictions == y_test)\n",
        "final_loss = history['test_losses'][-1]\n",
        "\n",
        "print('\ud83c\udfaf Final Results on Test Set:')\n",
        "print('='*50)\n",
        "print(f'   Final Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)')\n",
        "print(f'   Final Loss: {final_loss:.4f}')\n",
        "print('='*50)\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(y_test, final_predictions)\n",
        "\n",
        "print('\\n\ud83d\udcca Confusion Matrix:')\n",
        "print(cm)\n",
        "print('\\n\ud83d\udccb Classification Report:')\n",
        "print(classification_report(y_test, final_predictions, \n",
        "                          target_names=['Digit 0', 'Digit 1']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "fig.suptitle('Sample Predictions on Test Set', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Get indices\n",
        "indices = np.random.choice(len(X_test), 10, replace=False)\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    idx = indices[i]\n",
        "    \n",
        "    # Get original image (before scaling)\n",
        "    image = scaler.inverse_transform(X_test[idx:idx+1]).reshape(8, 8)\n",
        "    \n",
        "    true_label = y_test[idx][0]\n",
        "    pred_label = final_predictions[idx][0]\n",
        "    prob = A3_final.T[idx][0]\n",
        "    \n",
        "    # Plot\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    \n",
        "    # Title with color\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label} | Pred: {pred_label}\\nProb: {prob:.2f}', \n",
        "                fontsize=10, color=color)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n\ud83d\udca1 Green = Correct, Red = Incorrect')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83c\udfaf Challenge: Improve the Network!\n",
        "\n",
        "**Try these modifications to improve performance:**\n",
        "\n",
        "### Easy Challenges:\n",
        "1. **Change learning rate**\n",
        "   - Try: 0.1, 0.3, 0.7, 1.0\n",
        "   - See which works best!\n",
        "\n",
        "2. **Train for more epochs**\n",
        "   - Try: 1500 or 2000 epochs\n",
        "   - Does it improve?\n",
        "\n",
        "3. **Modify network size**\n",
        "   - Try: 64 \u2192 32 \u2192 16 \u2192 1\n",
        "   - Or: 64 \u2192 20 \u2192 10 \u2192 1\n",
        "\n",
        "### Medium Challenges:\n",
        "4. **Add ReLU activation**\n",
        "   - Use ReLU for hidden layers\n",
        "   - Keep sigmoid for output\n",
        "\n",
        "5. **Try different initialization**\n",
        "   - Use larger initial weights (0.1 instead of 0.01)\n",
        "   - See the effect!\n",
        "\n",
        "### Hard Challenges:\n",
        "6. **Implement mini-batch gradient descent**\n",
        "   - Instead of using all data at once\n",
        "   - Use batches of 32 samples\n",
        "\n",
        "7. **Add learning rate decay**\n",
        "   - Start with large learning rate\n",
        "   - Gradually decrease it\n",
        "\n",
        "8. **Classify more digits**\n",
        "   - Try 0, 1, 2 (3 classes)\n",
        "   - Need to change output layer!\n",
        "\n",
        "---\n",
        "\n",
        "**Experiment below! Copy the training code and modify it:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your experiments here!\n",
        "# Try different hyperparameters and modifications\n",
        "\n",
        "# Example: Different learning rate\n",
        "# trained_params_exp, history_exp = train_network(\n",
        "#     X_train, y_train, X_test, y_test,\n",
        "#     learning_rate=0.3,  # Changed!\n",
        "#     epochs=1000,\n",
        "#     print_every=100\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udcda Summary\n",
        "\n",
        "### What We Built:\n",
        "\n",
        "**Complete Neural Network from Scratch!**\n",
        "- Using only NumPy (no deep learning frameworks)\n",
        "- 3-layer architecture (2 hidden + 1 output)\n",
        "- Binary classification on MNIST digits\n",
        "\n",
        "### Components Implemented:\n",
        "\n",
        "**1. Forward Propagation:**\n",
        "- Layer-by-layer computation\n",
        "- z = W\u00b7x + b\n",
        "- a = sigmoid(z)\n",
        "\n",
        "**2. Loss Function:**\n",
        "- Binary Cross-Entropy\n",
        "- Measures prediction error\n",
        "\n",
        "**3. Backpropagation:**\n",
        "- Calculate gradients using chain rule\n",
        "- Work backward through layers\n",
        "- Find how to improve each weight\n",
        "\n",
        "**4. Gradient Descent:**\n",
        "- Update weights: W = W - \u03b1 \u00d7 gradient\n",
        "- Iteratively improve the network\n",
        "\n",
        "**5. Training Loop:**\n",
        "- Forward \u2192 Loss \u2192 Backward \u2192 Update\n",
        "- Repeat until convergence\n",
        "\n",
        "### \ud83c\udfaf Key Takeaways:\n",
        "\n",
        "\u2705 **Neural networks learn through iteration**\n",
        "- Each epoch improves predictions slightly\n",
        "- Loss decreases, accuracy increases\n",
        "\n",
        "\u2705 **Hyperparameters matter**\n",
        "- Learning rate controls convergence\n",
        "- Network size affects capacity\n",
        "- Initialization impacts training\n",
        "\n",
        "\u2705 **Math becomes code**\n",
        "- Forward prop = matrix multiplication\n",
        "- Backprop = chain rule in action\n",
        "- Gradient descent = simple subtraction\n",
        "\n",
        "\u2705 **You now understand deep learning**\n",
        "- Frameworks like PyTorch do this automatically\n",
        "- But now you know what happens under the hood!\n",
        "\n",
        "### \ud83d\udca1 What's Next:\n",
        "\n",
        "- **Week 9:** Advanced optimizers (Adam, RMSprop)\n",
        "- **Week 9:** Introduction to PyTorch\n",
        "- **Week 10:** Convolutional Neural Networks\n",
        "- **Week 11:** Transfer Learning\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations! You built a neural network from scratch! \ud83c\udf89\ud83c\udf8a**\n",
        "\n",
        "**You are now a deep learning practitioner!** \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}