{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Week 9 - Day 3: Introduction to PyTorch\n",
    "\n",
    "## üìã Today's Learning Goals:\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "‚úÖ **Create and manipulate PyTorch tensors** (creation, indexing, reshaping)\n",
    "\n",
    "‚úÖ **Build a small neural network** in PyTorch\n",
    "\n",
    "‚úÖ **Understand the training loop** (forward, loss, backward, step)\n",
    "\n",
    "---\n",
    "\n",
    "## üî• What is PyTorch?\n",
    "\n",
    "PyTorch is a **deep learning framework** developed by Facebook AI Research that:\n",
    "- Makes building neural networks easy and intuitive\n",
    "- Automatically computes gradients (no manual calculus!)\n",
    "- Runs on GPUs for fast training\n",
    "- Has a Pythonic API that feels natural\n",
    "\n",
    "**Why PyTorch?**\n",
    "- Used by 95% of deep learning researchers\n",
    "- Powers products at Meta, Tesla, OpenAI, and more\n",
    "- Easier to debug than other frameworks\n",
    "- Huge community and ecosystem\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Notebook Structure:\n",
    "\n",
    "1. **Part 1:** Setup and Installation\n",
    "2. **Part 2:** Tensors in PyTorch (creation, indexing, reshaping)\n",
    "3. **Part 3:** Building a Small Neural Network\n",
    "4. **Part 4:** Understanding the Training Loop\n",
    "5. **Part 5:** Complete Training Example\n",
    "6. **Part 6:** Challenge Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üì¶ PART 1: Setup and Installation\n",
    "\n",
    "## üéØ What we'll do:\n",
    "- Import PyTorch and other necessary libraries\n",
    "- Check PyTorch version\n",
    "- Verify GPU availability (if applicable)\n",
    "- Set random seeds for reproducibility\n",
    "\n",
    "## ü§î Why set random seeds?\n",
    "Setting random seeds ensures that you get the **same results every time** you run the code. This is crucial for:\n",
    "- Debugging (if something goes wrong, you can reproduce it)\n",
    "- Comparing results with classmates\n",
    "- Scientific reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================\n",
    "\n",
    "import torch                    # Main PyTorch library\n",
    "import torch.nn as nn           # Neural network modules\n",
    "import torch.optim as optim     # Optimization algorithms (SGD, Adam, etc.)\n",
    "import numpy as np              # For numerical operations\n",
    "import matplotlib.pyplot as plt # For visualization\n",
    "\n",
    "# ============================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ============================================\n",
    "# Why? So everyone gets the same results!\n",
    "\n",
    "torch.manual_seed(42)    # PyTorch random seed\n",
    "np.random.seed(42)       # NumPy random seed\n",
    "\n",
    "# ============================================\n",
    "# CHECK PYTORCH VERSION AND GPU AVAILABILITY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üî• PYTORCH SETUP INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA Available (GPU): {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"üíª Running on CPU (no GPU detected)\")\n",
    "\n",
    "print(f\"‚úÖ Random seeds set to 42 for reproducibility\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüöÄ Ready to learn PyTorch!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üé≤ PART 2: Tensors in PyTorch\n",
    "\n",
    "## üéØ Learning Objectives:\n",
    "1. Understand what tensors are\n",
    "2. Create tensors in multiple ways\n",
    "3. Index and slice tensors\n",
    "4. Reshape tensors\n",
    "\n",
    "## ü§î What is a Tensor?\n",
    "\n",
    "A **tensor** is a multi-dimensional array. Think of it as:\n",
    "- **0D tensor** (scalar): Just a number ‚Üí `5`\n",
    "- **1D tensor** (vector): A list of numbers ‚Üí `[1, 2, 3]`\n",
    "- **2D tensor** (matrix): A table of numbers ‚Üí `[[1,2], [3,4]]`\n",
    "- **3D tensor**: A cube of numbers (like RGB images)\n",
    "- **4D tensor**: A batch of images\n",
    "\n",
    "Tensors are similar to NumPy arrays but:\n",
    "- Can run on GPUs (much faster!)\n",
    "- Have automatic differentiation (autograd)\n",
    "- Are optimized for deep learning\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Section 2.1: Creating Tensors\n",
    "\n",
    "Let's learn different ways to create tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 1: CREATE TENSOR FROM PYTHON LIST\n",
    "# ============================================\n",
    "# This is the most intuitive way to create a tensor\n",
    "\n",
    "# Create a 1D tensor (vector)\n",
    "tensor_1d = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"üìä 1D Tensor (Vector):\")\n",
    "print(tensor_1d)\n",
    "print(f\"Shape: {tensor_1d.shape}\")      # Shape tells us dimensions\n",
    "print(f\"Data type: {tensor_1d.dtype}\")  # Data type (int64, float32, etc.)\n",
    "print(f\"Device: {tensor_1d.device}\")    # CPU or GPU\n",
    "print()\n",
    "\n",
    "# Create a 2D tensor (matrix)\n",
    "tensor_2d = torch.tensor([[1, 2, 3],\n",
    "                          [4, 5, 6]])\n",
    "\n",
    "print(\"üìä 2D Tensor (Matrix):\")\n",
    "print(tensor_2d)\n",
    "print(f\"Shape: {tensor_2d.shape}\")      # (2, 3) = 2 rows, 3 columns\n",
    "print(f\"Data type: {tensor_2d.dtype}\")\n",
    "print()\n",
    "\n",
    "# Create a 3D tensor\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]],\n",
    "                          [[5, 6], [7, 8]]])\n",
    "\n",
    "print(\"üìä 3D Tensor:\")\n",
    "print(tensor_3d)\n",
    "print(f\"Shape: {tensor_3d.shape}\")      # (2, 2, 2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 2: CREATE TENSORS WITH SPECIFIC VALUES\n",
    "# ============================================\n",
    "\n",
    "print(\"üîπ Creating Special Tensors:\\n\")\n",
    "\n",
    "# Zeros tensor (all elements are 0)\n",
    "# Useful for: Initializing accumulators, padding, etc.\n",
    "zeros = torch.zeros(3, 4)  # 3 rows, 4 columns\n",
    "print(\"Zeros Tensor (3x4):\")\n",
    "print(zeros)\n",
    "print(f\"Shape: {zeros.shape}\\n\")\n",
    "\n",
    "# Ones tensor (all elements are 1)\n",
    "# Useful for: Masks, weights initialization, etc.\n",
    "ones = torch.ones(2, 3)\n",
    "print(\"Ones Tensor (2x3):\")\n",
    "print(ones)\n",
    "print(f\"Shape: {ones.shape}\\n\")\n",
    "\n",
    "# Random tensor (values between 0 and 1)\n",
    "# Useful for: Weight initialization\n",
    "random_uniform = torch.rand(2, 2)  # Uniform distribution [0, 1)\n",
    "print(\"Random Tensor - Uniform [0,1):\")\n",
    "print(random_uniform)\n",
    "print()\n",
    "\n",
    "# Random normal tensor (mean=0, std=1)\n",
    "# Useful for: Weight initialization in neural networks\n",
    "random_normal = torch.randn(2, 2)  # Normal distribution\n",
    "print(\"Random Tensor - Normal Distribution:\")\n",
    "print(random_normal)\n",
    "print()\n",
    "\n",
    "# Range tensor (similar to Python's range)\n",
    "# Useful for: Creating sequences, indexing\n",
    "range_tensor = torch.arange(0, 10, 2)  # Start=0, End=10, Step=2\n",
    "print(\"Range Tensor (0 to 10, step 2):\")\n",
    "print(range_tensor)\n",
    "print()\n",
    "\n",
    "# Identity matrix (diagonal of ones)\n",
    "# Useful for: Linear algebra operations\n",
    "identity = torch.eye(3)  # 3x3 identity matrix\n",
    "print(\"Identity Matrix (3x3):\")\n",
    "print(identity)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Section 2.2: Tensor Properties and Data Types\n",
    "\n",
    "Understanding tensor properties is crucial for debugging and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UNDERSTANDING TENSOR PROPERTIES\n",
    "# ============================================\n",
    "\n",
    "# Create a sample tensor\n",
    "sample = torch.randn(3, 4)  # 3 rows, 4 columns of random numbers\n",
    "\n",
    "print(\"üîç Tensor Properties:\\n\")\n",
    "print(f\"Tensor:\\n{sample}\\n\")\n",
    "\n",
    "# Shape: Dimensions of the tensor\n",
    "print(f\"üìê Shape: {sample.shape}\")           # torch.Size([3, 4])\n",
    "print(f\"   ‚Üí 3 rows, 4 columns\\n\")\n",
    "\n",
    "# Size: Same as shape (alternative method)\n",
    "print(f\"üìê Size: {sample.size()}\")           # Same as shape\n",
    "print()\n",
    "\n",
    "# Number of dimensions\n",
    "print(f\"üìä Number of dimensions (ndim): {sample.ndim}\")\n",
    "print(f\"   ‚Üí This is a 2D tensor\\n\")\n",
    "\n",
    "# Total number of elements\n",
    "print(f\"üî¢ Total elements (numel): {sample.numel()}\")\n",
    "print(f\"   ‚Üí 3 √ó 4 = 12 elements\\n\")\n",
    "\n",
    "# Data type\n",
    "print(f\"üéØ Data type (dtype): {sample.dtype}\")\n",
    "print(f\"   ‚Üí float32 is the default for neural networks\\n\")\n",
    "\n",
    "# Device (CPU or GPU)\n",
    "print(f\"üíª Device: {sample.device}\")\n",
    "print(f\"   ‚Üí Currently on CPU\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# WORKING WITH DIFFERENT DATA TYPES\n",
    "# ============================================\n",
    "# PyTorch supports many data types, but we mainly use:\n",
    "# - torch.float32 (default): For neural network weights\n",
    "# - torch.int64: For labels/indices\n",
    "# - torch.bool: For masks\n",
    "\n",
    "print(\"üé® Different Data Types:\\n\")\n",
    "\n",
    "# Float tensor (default)\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"Float: {float_tensor}\")\n",
    "print(f\"dtype: {float_tensor.dtype}\\n\")\n",
    "\n",
    "# Integer tensor\n",
    "int_tensor = torch.tensor([1, 2, 3])\n",
    "print(f\"Integer: {int_tensor}\")\n",
    "print(f\"dtype: {int_tensor.dtype}\\n\")\n",
    "\n",
    "# Convert between types using .to()\n",
    "# This is very common when preparing data!\n",
    "converted = int_tensor.to(torch.float32)\n",
    "print(f\"Converted to float: {converted}\")\n",
    "print(f\"dtype: {converted.dtype}\\n\")\n",
    "\n",
    "# Or use specific constructors\n",
    "float_tensor2 = torch.FloatTensor([1, 2, 3])\n",
    "long_tensor = torch.LongTensor([1, 2, 3])\n",
    "print(f\"FloatTensor: {float_tensor2}, dtype: {float_tensor2.dtype}\")\n",
    "print(f\"LongTensor: {long_tensor}, dtype: {long_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Section 2.3: Indexing and Slicing Tensors\n",
    "\n",
    "Indexing works just like NumPy arrays and Python lists!\n",
    "\n",
    "**Remember:**\n",
    "- Python uses **0-based indexing** (first element is at index 0)\n",
    "- Negative indices count from the end (-1 is the last element)\n",
    "- Slicing syntax: `[start:end:step]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INDEXING 1D TENSORS (VECTORS)\n",
    "# ============================================\n",
    "\n",
    "# Create a 1D tensor\n",
    "vector = torch.tensor([10, 20, 30, 40, 50])\n",
    "print(\"Original Vector:\")\n",
    "print(vector)\n",
    "print()\n",
    "\n",
    "# Access single elements\n",
    "print(\"üîπ Single Element Access:\")\n",
    "print(f\"First element (index 0): {vector[0]}\")\n",
    "print(f\"Third element (index 2): {vector[2]}\")\n",
    "print(f\"Last element (index -1): {vector[-1]}\")\n",
    "print(f\"Second to last (index -2): {vector[-2]}\")\n",
    "print()\n",
    "\n",
    "# Slicing: [start:end] - end is NOT included\n",
    "print(\"üîπ Slicing:\")\n",
    "print(f\"First 3 elements [0:3]: {vector[0:3]}\")      # [10, 20, 30]\n",
    "print(f\"From index 2 to end [2:]: {vector[2:]}\")     # [30, 40, 50]\n",
    "print(f\"Up to index 3 [:3]: {vector[:3]}\")           # [10, 20, 30]\n",
    "print(f\"Every other element [::2]: {vector[::2]}\")   # [10, 30, 50]\n",
    "print(f\"Reverse [::-1]: {vector[::-1]}\")             # [50, 40, 30, 20, 10]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INDEXING 2D TENSORS (MATRICES)\n",
    "# ============================================\n",
    "\n",
    "# Create a 2D tensor (3 rows, 4 columns)\n",
    "matrix = torch.tensor([[1,  2,  3,  4],\n",
    "                       [5,  6,  7,  8],\n",
    "                       [9, 10, 11, 12]])\n",
    "\n",
    "print(\"Original Matrix (3x4):\")\n",
    "print(matrix)\n",
    "print(f\"Shape: {matrix.shape}\\n\")\n",
    "\n",
    "# Access single element: [row, column]\n",
    "print(\"üîπ Single Element Access:\")\n",
    "print(f\"Element at row 0, col 0: {matrix[0, 0]}\")     # 1\n",
    "print(f\"Element at row 1, col 2: {matrix[1, 2]}\")     # 7\n",
    "print(f\"Last element: {matrix[-1, -1]}\")              # 12\n",
    "print()\n",
    "\n",
    "# Access entire rows\n",
    "print(\"üîπ Row Access:\")\n",
    "print(f\"First row [0, :]: {matrix[0, :]}\")            # [1, 2, 3, 4]\n",
    "print(f\"Second row [1]: {matrix[1]}\")                 # [5, 6, 7, 8]\n",
    "print(f\"Last row [-1]: {matrix[-1]}\\n\")               # [9, 10, 11, 12]\n",
    "\n",
    "# Access entire columns\n",
    "print(\"üîπ Column Access:\")\n",
    "print(f\"First column [:, 0]: {matrix[:, 0]}\")         # [1, 5, 9]\n",
    "print(f\"Third column [:, 2]: {matrix[:, 2]}\")         # [3, 7, 11]\n",
    "print(f\"Last column [:, -1]: {matrix[:, -1]}\\n\")      # [4, 8, 12]\n",
    "\n",
    "# Slicing: Get sub-matrices\n",
    "print(\"üîπ Sub-matrix Slicing:\")\n",
    "print(f\"Top-left 2x2 [:2, :2]:\\n{matrix[:2, :2]}\\n\")  # [[1,2], [5,6]]\n",
    "print(f\"Bottom-right 2x2 [1:, 2:]:\\n{matrix[1:, 2:]}\\n\")  # [[7,8], [11,12]]\n",
    "print(f\"Middle 2x2 [1:3, 1:3]:\\n{matrix[1:3, 1:3]}\\n\")    # [[6,7], [10,11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Section 2.4: Reshaping Tensors\n",
    "\n",
    "**Why reshape?**\n",
    "- Neural networks expect specific input shapes\n",
    "- Images need to be flattened for fully-connected layers\n",
    "- Batching data requires adding/removing dimensions\n",
    "\n",
    "**Key methods:**\n",
    "- `.view()`: Returns a new view of the same data (fast, memory-efficient)\n",
    "- `.reshape()`: More flexible, may copy data if needed\n",
    "- `.squeeze()`: Removes dimensions of size 1\n",
    "- `.unsqueeze()`: Adds a dimension of size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RESHAPING WITH .view() AND .reshape()\n",
    "# ============================================\n",
    "\n",
    "# Create a tensor with 12 elements\n",
    "original = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
    "print(\"Original Tensor:\")\n",
    "print(original)\n",
    "print(f\"Shape: {original.shape}\\n\")\n",
    "\n",
    "# Reshape to 3x4 (3 rows, 4 columns)\n",
    "reshaped_3x4 = original.view(3, 4)\n",
    "print(\"Reshaped to 3x4:\")\n",
    "print(reshaped_3x4)\n",
    "print(f\"Shape: {reshaped_3x4.shape}\\n\")\n",
    "\n",
    "# Reshape to 2x6\n",
    "reshaped_2x6 = original.view(2, 6)\n",
    "print(\"Reshaped to 2x6:\")\n",
    "print(reshaped_2x6)\n",
    "print(f\"Shape: {reshaped_2x6.shape}\\n\")\n",
    "\n",
    "# Use -1 to let PyTorch calculate the dimension automatically\n",
    "# Rule: Total elements must remain the same!\n",
    "reshaped_auto = original.view(4, -1)  # 4 rows, PyTorch calculates 3 columns\n",
    "print(\"Reshaped to 4x? (auto-calculated):\")\n",
    "print(reshaped_auto)\n",
    "print(f\"Shape: {reshaped_auto.shape}\")\n",
    "print(f\"PyTorch calculated: 12 √∑ 4 = 3 columns\\n\")\n",
    "\n",
    "# Flatten to 1D (common operation before fully-connected layers)\n",
    "flattened = original.view(-1)  # -1 means \"flatten everything\"\n",
    "print(\"Flattened to 1D:\")\n",
    "print(flattened)\n",
    "print(f\"Shape: {flattened.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ADDING AND REMOVING DIMENSIONS\n",
    "# ============================================\n",
    "\n",
    "# Create a 2D tensor\n",
    "tensor_2d = torch.tensor([[1, 2, 3],\n",
    "                          [4, 5, 6]])\n",
    "print(\"Original 2D Tensor:\")\n",
    "print(tensor_2d)\n",
    "print(f\"Shape: {tensor_2d.shape}\\n\")\n",
    "\n",
    "# Add dimension at position 0 (create a batch dimension)\n",
    "# This is very common when feeding single samples to a model!\n",
    "unsqueezed_0 = tensor_2d.unsqueeze(0)\n",
    "print(\"After unsqueeze(0) - Added batch dimension:\")\n",
    "print(unsqueezed_0)\n",
    "print(f\"Shape: {unsqueezed_0.shape}\")  # (1, 2, 3)\n",
    "print(f\"Interpretation: 1 batch, 2 rows, 3 columns\\n\")\n",
    "\n",
    "# Add dimension at position 1\n",
    "unsqueezed_1 = tensor_2d.unsqueeze(1)\n",
    "print(\"After unsqueeze(1) - Added dimension in middle:\")\n",
    "print(unsqueezed_1)\n",
    "print(f\"Shape: {unsqueezed_1.shape}\\n\")  # (2, 1, 3)\n",
    "\n",
    "# Remove dimensions of size 1\n",
    "tensor_with_ones = torch.randn(1, 3, 1, 4)  # Shape: (1, 3, 1, 4)\n",
    "print(f\"Tensor with extra dimensions: {tensor_with_ones.shape}\")\n",
    "\n",
    "squeezed = tensor_with_ones.squeeze()  # Remove all dimensions of size 1\n",
    "print(f\"After squeeze(): {squeezed.shape}\")  # (3, 4)\n",
    "print(\"Removed dimensions of size 1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRACTICAL EXAMPLE: IMAGE PREPROCESSING\n",
    "# ============================================\n",
    "# This is exactly what happens when you feed images to a neural network!\n",
    "\n",
    "print(\"üñºÔ∏è Practical Example: Preparing an Image for a Neural Network\\n\")\n",
    "\n",
    "# Simulate a grayscale image: 28x28 pixels (like MNIST digits)\n",
    "image = torch.randn(28, 28)\n",
    "print(f\"1. Original image shape: {image.shape}\")\n",
    "print(f\"   ‚Üí This is a 2D grayscale image\\n\")\n",
    "\n",
    "# Step 1: Flatten the image for a fully-connected layer\n",
    "flattened_image = image.view(-1)  # Convert to 1D\n",
    "print(f\"2. Flattened image: {flattened_image.shape}\")\n",
    "print(f\"   ‚Üí 28 √ó 28 = {flattened_image.shape[0]} pixels\\n\")\n",
    "\n",
    "# Step 2: Add batch dimension (neural networks expect batches)\n",
    "batched_image = flattened_image.unsqueeze(0)\n",
    "print(f\"3. Added batch dimension: {batched_image.shape}\")\n",
    "print(f\"   ‚Üí Shape is now (batch_size=1, features=784)\\n\")\n",
    "\n",
    "print(\"‚úÖ This image is now ready to be fed into a neural network!\")\n",
    "print(\"üí° Neural networks always expect batch dimension first: (batch, features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üß† PART 3: Building a Small Neural Network\n",
    "\n",
    "## üéØ What we'll learn:\n",
    "- How to define a neural network class in PyTorch\n",
    "- Understanding `nn.Module` (the base class for all networks)\n",
    "- Creating layers with `nn.Linear`\n",
    "- Implementing the forward pass\n",
    "\n",
    "## üèóÔ∏è Neural Network Architecture:\n",
    "\n",
    "We'll build a simple 2-layer network:\n",
    "\n",
    "```\n",
    "Input (4 features)\n",
    "      ‚Üì\n",
    "Linear Layer (4 ‚Üí 8 neurons)\n",
    "      ‚Üì\n",
    "ReLU Activation (non-linearity)\n",
    "      ‚Üì\n",
    "Linear Layer (8 ‚Üí 1 neuron)\n",
    "      ‚Üì\n",
    "Output (prediction)\n",
    "```\n",
    "\n",
    "This is a **regression network** (predicts a continuous value).\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Section 3.1: Understanding nn.Module\n",
    "\n",
    "**Every PyTorch neural network is a class that inherits from `nn.Module`.**\n",
    "\n",
    "You must implement two methods:\n",
    "1. `__init__()`: Define your layers here\n",
    "2. `forward()`: Define how data flows through layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEFINE A SIMPLE NEURAL NETWORK\n",
    "# ============================================\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 2-layer neural network for regression.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: 4 features\n",
    "    - Hidden layer: 8 neurons + ReLU activation\n",
    "    - Output layer: 1 neuron (for prediction)\n",
    "    \n",
    "    Why this architecture?\n",
    "    - Simple enough to understand\n",
    "    - Complex enough to learn patterns\n",
    "    - Similar to networks used in real projects\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=4, hidden_size=8, output_size=1):\n",
    "        \"\"\"\n",
    "        Initialize the network layers.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of input features\n",
    "            hidden_size (int): Number of neurons in hidden layer\n",
    "            output_size (int): Number of output values\n",
    "        \n",
    "        Why call super().__init__()?:\n",
    "        - This initializes the parent class (nn.Module)\n",
    "        - Required for PyTorch to track parameters\n",
    "        \"\"\"\n",
    "        # Initialize parent class - ALWAYS DO THIS FIRST!\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        # ============================================\n",
    "        # DEFINE LAYERS\n",
    "        # ============================================\n",
    "        \n",
    "        # Layer 1: Linear transformation (input ‚Üí hidden)\n",
    "        # nn.Linear(in_features, out_features)\n",
    "        # This creates: output = input @ weights + bias\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # fc1 has: 4√ó8 = 32 weights + 8 biases = 40 parameters\n",
    "        \n",
    "        # Activation function: ReLU (Rectified Linear Unit)\n",
    "        # ReLU(x) = max(0, x)\n",
    "        # Why ReLU? Introduces non-linearity, fast to compute\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Layer 2: Linear transformation (hidden ‚Üí output)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        # fc2 has: 8√ó1 = 8 weights + 1 bias = 9 parameters\n",
    "        \n",
    "        # Total parameters: 40 + 9 = 49 learnable parameters\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass (how data flows through the network).\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input data, shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Network output, shape (batch_size, output_size)\n",
    "        \n",
    "        What happens here:\n",
    "        1. Input (batch, 4) ‚Üí Linear ‚Üí (batch, 8)\n",
    "        2. (batch, 8) ‚Üí ReLU ‚Üí (batch, 8) [some values become 0]\n",
    "        3. (batch, 8) ‚Üí Linear ‚Üí (batch, 1) [final prediction]\n",
    "        \"\"\"\n",
    "        # Step 1: First linear layer\n",
    "        # x has shape: (batch_size, 4)\n",
    "        x = self.fc1(x)  # Output shape: (batch_size, 8)\n",
    "        \n",
    "        # Step 2: Apply ReLU activation\n",
    "        # This makes the network non-linear (can learn complex patterns)\n",
    "        # Without activation, the network is just linear regression!\n",
    "        x = self.relu(x)  # Shape stays: (batch_size, 8)\n",
    "        \n",
    "        # Step 3: Second linear layer (output)\n",
    "        x = self.fc2(x)  # Output shape: (batch_size, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CREATE AN INSTANCE OF THE NETWORK\n",
    "# ============================================\n",
    "\n",
    "# Create the model\n",
    "model = SimpleNet(input_size=4, hidden_size=8, output_size=1)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß† NEURAL NETWORK CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìä Architecture:\")\n",
    "print(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSPECT THE MODEL PARAMETERS\n",
    "# ============================================\n",
    "# Understanding parameters helps you:\n",
    "# - Debug model size issues\n",
    "# - Calculate memory requirements\n",
    "# - Understand what the model is learning\n",
    "\n",
    "print(\"üîç Model Parameters:\\n\")\n",
    "\n",
    "# Method 1: Print all parameters with names\n",
    "print(\"üìã All Parameters (with names):\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:20s} | Shape: {str(param.shape):15s} | Elements: {param.numel():4d}\")\n",
    "print()\n",
    "\n",
    "# Method 2: Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"üìä Parameter Summary:\")\n",
    "print(f\"  Total parameters: {total_params}\")\n",
    "print(f\"  Trainable parameters: {trainable_params}\")\n",
    "print()\n",
    "\n",
    "# Explanation of parameter counts:\n",
    "print(\"üí° Parameter Breakdown:\")\n",
    "print(f\"  Layer 1 (fc1): 4 inputs √ó 8 outputs = 32 weights + 8 biases = 40 parameters\")\n",
    "print(f\"  Layer 2 (fc2): 8 inputs √ó 1 output = 8 weights + 1 bias = 9 parameters\")\n",
    "print(f\"  Total: 40 + 9 = 49 parameters\")\n",
    "print()\n",
    "print(\"‚úÖ All parameters are trainable (requires_grad=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TEST THE FORWARD PASS\n",
    "# ============================================\n",
    "# Let's verify the model works by passing some data through it\n",
    "\n",
    "print(\"üß™ Testing Forward Pass:\\n\")\n",
    "\n",
    "# Create some random input data\n",
    "# Shape: (batch_size=3, features=4)\n",
    "# This represents 3 samples, each with 4 features\n",
    "test_input = torch.randn(3, 4)\n",
    "\n",
    "print(\"üì• Input:\")\n",
    "print(test_input)\n",
    "print(f\"Shape: {test_input.shape}\\n\")\n",
    "\n",
    "# Pass through the model (forward pass)\n",
    "# PyTorch automatically calls the forward() method\n",
    "test_output = model(test_input)\n",
    "\n",
    "print(\"üì§ Output:\")\n",
    "print(test_output)\n",
    "print(f\"Shape: {test_output.shape}\\n\")\n",
    "\n",
    "print(\"‚úÖ Success! The model processed the input correctly.\")\n",
    "print(\"üí° The output is random because the model hasn't been trained yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üîÑ PART 4: Understanding the Training Loop\n",
    "\n",
    "## üéØ What is the Training Loop?\n",
    "\n",
    "The training loop is where the **learning happens**. It consists of 4 key steps that repeat for many iterations:\n",
    "\n",
    "### **The 4 Steps of Training:**\n",
    "\n",
    "```python\n",
    "1. FORWARD PASS:\n",
    "   predictions = model(inputs)\n",
    "   # Pass data through network to get predictions\n",
    "\n",
    "2. COMPUTE LOSS:\n",
    "   loss = criterion(predictions, targets)\n",
    "   # Calculate how wrong the predictions are\n",
    "\n",
    "3. BACKWARD PASS:\n",
    "   optimizer.zero_grad()  # Clear old gradients\n",
    "   loss.backward()        # Compute new gradients\n",
    "   # Calculate how to adjust each weight\n",
    "\n",
    "4. UPDATE WEIGHTS:\n",
    "   optimizer.step()\n",
    "   # Adjust weights to reduce loss\n",
    "```\n",
    "\n",
    "**Analogy:** Think of it like learning to throw darts:\n",
    "1. **Forward:** Throw a dart (make a prediction)\n",
    "2. **Loss:** See how far from bullseye (measure error)\n",
    "3. **Backward:** Figure out how to adjust your throw (compute gradients)\n",
    "4. **Update:** Adjust your technique (update weights)\n",
    "\n",
    "Repeat thousands of times ‚Üí Get better at hitting bullseye!\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Section 4.1: Components of Training\n",
    "\n",
    "Before we can train, we need:\n",
    "1. **Loss Function** (criterion): Measures how wrong our predictions are\n",
    "2. **Optimizer**: Updates weights to reduce loss\n",
    "3. **Data**: Inputs and their correct outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: DEFINE LOSS FUNCTION\n",
    "# ============================================\n",
    "# Loss function (also called criterion) measures prediction error\n",
    "# Lower loss = better predictions\n",
    "\n",
    "# For regression problems, we use Mean Squared Error (MSE)\n",
    "# MSE = average of (prediction - target)¬≤\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"üìâ Loss Function: Mean Squared Error (MSE)\")\n",
    "print(\"   Formula: MSE = (1/n) * Œ£(prediction - target)¬≤\")\n",
    "print(\"   Why MSE? Penalizes large errors more than small ones\")\n",
    "print()\n",
    "\n",
    "# Example: How MSE works\n",
    "predictions = torch.tensor([2.0, 3.0, 4.0])\n",
    "targets = torch.tensor([2.5, 3.2, 3.8])\n",
    "example_loss = criterion(predictions, targets)\n",
    "\n",
    "print(\"üîç Example MSE Calculation:\")\n",
    "print(f\"  Predictions: {predictions.numpy()}\")\n",
    "print(f\"  Targets:     {targets.numpy()}\")\n",
    "print(f\"  MSE Loss:    {example_loss.item():.4f}\")\n",
    "print(f\"  Interpretation: Average squared difference is {example_loss.item():.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: DEFINE OPTIMIZER\n",
    "# ============================================\n",
    "# Optimizer updates weights to minimize loss\n",
    "# Think of it as the \"learning algorithm\"\n",
    "\n",
    "# We'll use Adam optimizer (most popular choice)\n",
    "# Adam = Adaptive Moment Estimation\n",
    "# - Adapts learning rate for each parameter\n",
    "# - Usually works well without tuning\n",
    "\n",
    "learning_rate = 0.01  # How big are the update steps?\n",
    "                      # Too high = unstable, too low = slow learning\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"‚öôÔ∏è Optimizer: Adam\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(\"   What it optimizes: All {49} parameters in our model\")\n",
    "print()\n",
    "print(\"üí° Why Adam?\")\n",
    "print(\"   - Adapts learning rate automatically\")\n",
    "print(\"   - Works well for most problems\")\n",
    "print(\"   - Industry standard (used in 90% of deep learning)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Section 4.2: Understanding Each Step in Detail\n",
    "\n",
    "Let's break down each step of the training loop with detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEMONSTRATION: ONE TRAINING ITERATION\n",
    "# ============================================\n",
    "# Let's manually go through one iteration step-by-step\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üéì DETAILED WALKTHROUGH: ONE TRAINING ITERATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Prepare some dummy data\n",
    "# In real training, this would come from your dataset\n",
    "dummy_input = torch.randn(5, 4)   # 5 samples, 4 features each\n",
    "dummy_target = torch.randn(5, 1)  # 5 target values\n",
    "\n",
    "print(\"üì¶ Input Data:\")\n",
    "print(f\"   Input shape: {dummy_input.shape}   (5 samples √ó 4 features)\")\n",
    "print(f\"   Target shape: {dummy_target.shape} (5 target values)\")\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: FORWARD PASS\n",
    "# ============================================\n",
    "print(\"üìä STEP 1: FORWARD PASS\")\n",
    "print(\"   ‚Üí Pass input through the network to get predictions\\n\")\n",
    "\n",
    "predictions = model(dummy_input)\n",
    "\n",
    "print(f\"   Predictions shape: {predictions.shape}\")\n",
    "print(f\"   First 3 predictions: {predictions[:3].detach().numpy().flatten()}\")\n",
    "print(f\"   First 3 targets:     {dummy_target[:3].numpy().flatten()}\")\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: COMPUTE LOSS\n",
    "# ============================================\n",
    "print(\"üìâ STEP 2: COMPUTE LOSS\")\n",
    "print(\"   ‚Üí Calculate how wrong our predictions are\\n\")\n",
    "\n",
    "loss = criterion(predictions, dummy_target)\n",
    "\n",
    "print(f\"   Loss value: {loss.item():.4f}\")\n",
    "print(f\"   What this means: Average squared error is {loss.item():.4f}\")\n",
    "print(f\"   Goal: Make this number smaller!\")\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: BACKWARD PASS (BACKPROPAGATION)\n",
    "# ============================================\n",
    "print(\"üîô STEP 3: BACKWARD PASS (Backpropagation)\")\n",
    "print(\"   ‚Üí Calculate gradients for all parameters\\n\")\n",
    "\n",
    "# CRITICAL: Always zero gradients first!\n",
    "# Why? Gradients accumulate by default in PyTorch\n",
    "# If we don't zero them, old gradients will be added to new ones\n",
    "optimizer.zero_grad()\n",
    "print(\"   ‚úì Cleared old gradients (optimizer.zero_grad())\")\n",
    "\n",
    "# Compute gradients via backpropagation\n",
    "# This is the magic of PyTorch - automatic differentiation!\n",
    "loss.backward()\n",
    "print(\"   ‚úì Computed gradients (loss.backward())\")\n",
    "\n",
    "# Let's inspect a gradient to see what happened\n",
    "first_param_grad = list(model.parameters())[0].grad\n",
    "print(f\"   Example gradient shape: {first_param_grad.shape}\")\n",
    "print(f\"   Gradient statistics: mean={first_param_grad.mean():.6f}, std={first_param_grad.std():.6f}\")\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: UPDATE WEIGHTS\n",
    "# ============================================\n",
    "print(\"‚ö° STEP 4: UPDATE WEIGHTS\")\n",
    "print(\"   ‚Üí Adjust parameters to reduce loss\\n\")\n",
    "\n",
    "# Get current weight before update\n",
    "first_param_before = list(model.parameters())[0].data.clone()\n",
    "\n",
    "# Update weights using computed gradients\n",
    "# The optimizer uses: new_weight = old_weight - learning_rate * gradient\n",
    "optimizer.step()\n",
    "print(\"   ‚úì Updated all weights (optimizer.step())\")\n",
    "\n",
    "# Get weight after update to see the change\n",
    "first_param_after = list(model.parameters())[0].data\n",
    "weight_change = (first_param_after - first_param_before).abs().mean()\n",
    "\n",
    "print(f\"   Average weight change: {weight_change:.6f}\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ ONE TRAINING ITERATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"üí° Key Takeaway:\")\n",
    "print(\"   Repeat these 4 steps thousands of times ‚Üí Model learns!\")\n",
    "print(\"   Each iteration: predictions get better, loss gets smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üéØ PART 5: Complete Training Example\n",
    "\n",
    "## üéØ What we'll do:\n",
    "Now let's put everything together and train a model on real data!\n",
    "\n",
    "We'll:\n",
    "1. Generate synthetic dataset\n",
    "2. Train the model for multiple epochs\n",
    "3. Visualize the training progress\n",
    "4. Evaluate the final model\n",
    "\n",
    "## üìä Dataset:\n",
    "We'll create a simple regression problem:\n",
    "- **Input:** 4 features (random numbers)\n",
    "- **Output:** Sum of all features (with some noise)\n",
    "- **Task:** Learn to predict the sum from the features\n",
    "\n",
    "This is a learnable pattern - the model should achieve low loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: GENERATE SYNTHETIC DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"üìä Generating Synthetic Dataset...\\n\")\n",
    "\n",
    "# Generate 1000 training samples\n",
    "n_samples = 1000\n",
    "n_features = 4\n",
    "\n",
    "# Random inputs: values between -1 and 1\n",
    "X_train = torch.randn(n_samples, n_features)\n",
    "\n",
    "# Target: sum of all features + small noise\n",
    "# This creates a learnable pattern!\n",
    "y_train = X_train.sum(dim=1, keepdim=True) + torch.randn(n_samples, 1) * 0.1\n",
    "\n",
    "print(f\"‚úÖ Training Data Created:\")\n",
    "print(f\"   Input shape: {X_train.shape}   (1000 samples √ó 4 features)\")\n",
    "print(f\"   Output shape: {y_train.shape} (1000 target values)\")\n",
    "print()\n",
    "print(f\"üìã Example Data:\")\n",
    "print(f\"   First sample input: {X_train[0].numpy()}\")\n",
    "print(f\"   First sample target: {y_train[0].item():.4f}\")\n",
    "print(f\"   Sum of inputs: {X_train[0].sum().item():.4f}\")\n",
    "print(f\"   (Target ‚âà Sum due to small noise)\")\n",
    "print()\n",
    "\n",
    "# Generate 200 test samples (for evaluation)\n",
    "n_test = 200\n",
    "X_test = torch.randn(n_test, n_features)\n",
    "y_test = X_test.sum(dim=1, keepdim=True) + torch.randn(n_test, 1) * 0.1\n",
    "\n",
    "print(f\"‚úÖ Test Data Created:\")\n",
    "print(f\"   Input shape: {X_test.shape}\")\n",
    "print(f\"   Output shape: {y_test.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: CREATE FRESH MODEL AND OPTIMIZER\n",
    "# ============================================\n",
    "# Create a new model for clean training\n",
    "\n",
    "model = SimpleNet(input_size=4, hidden_size=8, output_size=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"üß† Model Ready for Training\")\n",
    "print(f\"   Architecture: 4 ‚Üí 8 ‚Üí 1\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"   Optimizer: Adam (lr=0.01)\")\n",
    "print(f\"   Loss Function: MSE\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: TRAINING LOOP\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üèãÔ∏è TRAINING THE MODEL\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 100  # How many times to go through the entire dataset\n",
    "                  # More epochs = more learning (but risk overfitting)\n",
    "\n",
    "# List to store loss history for plotting\n",
    "train_losses = []\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ============================================\n",
    "    # THE 4 TRAINING STEPS (repeated each epoch)\n",
    "    # ============================================\n",
    "    \n",
    "    # 1. FORWARD PASS\n",
    "    predictions = model(X_train)\n",
    "    \n",
    "    # 2. COMPUTE LOSS\n",
    "    loss = criterion(predictions, y_train)\n",
    "    \n",
    "    # 3. BACKWARD PASS\n",
    "    optimizer.zero_grad()  # Clear old gradients\n",
    "    loss.backward()        # Compute new gradients\n",
    "    \n",
    "    # 4. UPDATE WEIGHTS\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save loss for plotting\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1:3d}/{num_epochs}] | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"üìä Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"üìà Loss Reduction: {train_losses[0]:.4f} ‚Üí {train_losses[-1]:.4f}\")\n",
    "print(f\"üìâ Improvement: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: VISUALIZE TRAINING PROGRESS\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Loss over time\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, linewidth=2, color='blue')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "plt.text(0, train_losses[0], f'Start: {train_losses[0]:.3f}', \n",
    "         fontsize=10, color='red', ha='left')\n",
    "plt.text(len(train_losses)-1, train_losses[-1], f'End: {train_losses[-1]:.3f}', \n",
    "         fontsize=10, color='green', ha='right')\n",
    "\n",
    "# Plot 2: Loss (log scale) - better for seeing later improvements\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, linewidth=2, color='green')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE) - Log Scale', fontsize=12)\n",
    "plt.title('Training Loss (Log Scale)', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä What the plots show:\")\n",
    "print(\"   ‚Üí Left: Loss decreases rapidly at first, then plateaus\")\n",
    "print(\"   ‚Üí Right: Log scale shows continued improvement throughout\")\n",
    "print(\"   ‚úÖ This is healthy training - loss consistently decreases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 5: EVALUATE ON TEST DATA\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüéØ EVALUATING TRAINED MODEL\\n\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "# This disables dropout, batch norm, etc. (not used in our simple model)\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation (saves memory and speeds up)\n",
    "with torch.no_grad():\n",
    "    # Make predictions on test set\n",
    "    test_predictions = model(X_test)\n",
    "    \n",
    "    # Compute test loss\n",
    "    test_loss = criterion(test_predictions, y_test)\n",
    "\n",
    "print(f\"üìä Test Set Performance:\")\n",
    "print(f\"   Test Loss (MSE): {test_loss.item():.4f}\")\n",
    "print(f\"   Training Loss: {train_losses[-1]:.4f}\")\n",
    "print()\n",
    "\n",
    "if test_loss.item() < train_losses[-1] * 1.2:\n",
    "    print(\"‚úÖ Good! Test loss is similar to training loss\")\n",
    "    print(\"   ‚Üí Model generalizes well to new data\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Test loss is much higher than training loss\")\n",
    "    print(\"   ‚Üí Model may be overfitting\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nüîç Sample Predictions vs Targets:\")\n",
    "print(\"\\n   Prediction  |  Target  |  Error\")\n",
    "print(\"   \" + \"=\"*40)\n",
    "for i in range(5):\n",
    "    pred = test_predictions[i].item()\n",
    "    target = y_test[i].item()\n",
    "    error = abs(pred - target)\n",
    "    print(f\"   {pred:10.4f}  |  {target:7.4f}  |  {error:6.4f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   Small errors ‚Üí Model learned the pattern well!\")\n",
    "print(\"   The model successfully learned to sum inputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üéØ PART 6: Challenge Exercise\n",
    "\n",
    "## üèÜ Your Turn to Code!\n",
    "\n",
    "Now that you understand PyTorch basics, try this challenge:\n",
    "\n",
    "### **Challenge: Build and Train a Classifier**\n",
    "\n",
    "**Task:** Create a neural network that classifies data into 2 categories (binary classification)\n",
    "\n",
    "### **Requirements:**\n",
    "\n",
    "1. **Network Architecture:**\n",
    "   - Input: 10 features\n",
    "   - Hidden layer 1: 16 neurons + ReLU\n",
    "   - Hidden layer 2: 8 neurons + ReLU\n",
    "   - Output: 1 neuron + Sigmoid (for binary classification)\n",
    "\n",
    "2. **Training Setup:**\n",
    "   - Use Binary Cross Entropy Loss (`nn.BCELoss()`)\n",
    "   - Use SGD optimizer (`optim.SGD()`)\n",
    "   - Train for 50 epochs\n",
    "   - Use the provided dataset below\n",
    "\n",
    "3. **Evaluation:**\n",
    "   - Calculate accuracy on test set\n",
    "   - Plot training loss\n",
    "   - Show confusion matrix (optional)\n",
    "\n",
    "### **Hints:**\n",
    "- For binary classification, use `nn.Sigmoid()` as final activation\n",
    "- BCELoss expects predictions in range [0, 1]\n",
    "- Accuracy = (correct predictions) / (total predictions)\n",
    "- Use `(predictions > 0.5).float()` to convert probabilities to binary predictions\n",
    "\n",
    "### **Bonus Challenges:**\n",
    "- Try different learning rates (0.001, 0.01, 0.1)\n",
    "- Add more hidden layers\n",
    "- Compare SGD vs Adam optimizer\n",
    "- Visualize decision boundary (advanced!)\n",
    "\n",
    "---\n",
    "\n",
    "### **Dataset Provided:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHALLENGE DATASET (Binary Classification)\n",
    "# ============================================\n",
    "\n",
    "# Generate synthetic binary classification data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Training data\n",
    "X_train_challenge = torch.randn(500, 10)  # 500 samples, 10 features\n",
    "# Target: 1 if sum > 0, else 0\n",
    "y_train_challenge = (X_train_challenge.sum(dim=1) > 0).float().unsqueeze(1)\n",
    "\n",
    "# Test data\n",
    "X_test_challenge = torch.randn(100, 10)\n",
    "y_test_challenge = (X_test_challenge.sum(dim=1) > 0).float().unsqueeze(1)\n",
    "\n",
    "print(\"üì¶ Challenge Dataset Ready!\")\n",
    "print(f\"   Training: {X_train_challenge.shape} inputs, {y_train_challenge.shape} labels\")\n",
    "print(f\"   Test: {X_test_challenge.shape} inputs, {y_test_challenge.shape} labels\")\n",
    "print(f\"\\n   Class distribution:\")\n",
    "print(f\"   Class 0: {(y_train_challenge == 0).sum().item()} samples\")\n",
    "print(f\"   Class 1: {(y_train_challenge == 1).sum().item()} samples\")\n",
    "print(\"\\nüí° Your task: Build a network that predicts the class!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE\n",
    "# ============================================\n",
    "\n",
    "# Step 1: Define your network class\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        # TODO: Define your layers\n",
    "        # Hint: Input ‚Üí Hidden(16) ‚Üí Hidden(8) ‚Üí Output(1)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # Remember: Use Sigmoid at the end!\n",
    "        pass\n",
    "\n",
    "# Step 2: Create model, loss, and optimizer\n",
    "# TODO: Your code here\n",
    "\n",
    "# Step 3: Training loop\n",
    "# TODO: Implement training for 50 epochs\n",
    "\n",
    "# Step 4: Evaluate and plot\n",
    "# TODO: Calculate accuracy and plot loss\n",
    "\n",
    "# Good luck! üçÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# üìö Summary: What We Learned Today\n",
    "\n",
    "## ‚úÖ Part 1: Tensors in PyTorch\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Tensors** are multi-dimensional arrays (like NumPy, but GPU-accelerated)\n",
    "- **Creation:** `torch.tensor()`, `torch.zeros()`, `torch.randn()`, etc.\n",
    "- **Indexing:** Works like NumPy - `tensor[0]`, `tensor[:, 1]`, etc.\n",
    "- **Reshaping:** Use `.view()` and `.reshape()` to change dimensions\n",
    "- **Important properties:** `.shape`, `.dtype`, `.device`, `.numel()`\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Tensors can run on GPU for massive speedup\n",
    "- Always check tensor shapes - most bugs come from shape mismatches!\n",
    "- Use `.view(-1)` to flatten, `.unsqueeze(0)` to add batch dimension\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Part 2: Building Neural Networks\n",
    "\n",
    "**Key Concepts:**\n",
    "- **nn.Module:** Base class for all neural networks\n",
    "- **Two required methods:**\n",
    "  - `__init__()`: Define layers\n",
    "  - `forward()`: Define data flow\n",
    "- **Common layers:**\n",
    "  - `nn.Linear()`: Fully-connected layer\n",
    "  - `nn.ReLU()`: Activation function\n",
    "  - `nn.Sigmoid()`: For binary classification\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Always call `super().__init__()` first!\n",
    "- Activation functions add non-linearity (essential for learning)\n",
    "- Use `.named_parameters()` to inspect learnable parameters\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Part 3: The Training Loop\n",
    "\n",
    "**The 4 Sacred Steps:**\n",
    "\n",
    "```python\n",
    "1. FORWARD:  predictions = model(inputs)\n",
    "2. LOSS:     loss = criterion(predictions, targets)\n",
    "3. BACKWARD: optimizer.zero_grad() + loss.backward()\n",
    "4. UPDATE:   optimizer.step()\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- **Loss Function:** Measures prediction error (MSE, CrossEntropy)\n",
    "- **Optimizer:** Updates weights to reduce loss (Adam, SGD)\n",
    "- **Epochs:** Number of times to go through entire dataset\n",
    "\n",
    "**Key Takeaways:**\n",
    "- **ALWAYS** call `optimizer.zero_grad()` before `loss.backward()`\n",
    "- Training is just repeating these 4 steps thousands of times!\n",
    "- Loss should decrease over time - if not, something is wrong\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Critical Things to Remember:\n",
    "\n",
    "### **The Training Loop Order:**\n",
    "```python\n",
    "# ‚úÖ CORRECT ORDER\n",
    "predictions = model(inputs)        # 1. Forward\n",
    "loss = criterion(predictions, targets)  # 2. Loss\n",
    "optimizer.zero_grad()              # 3. Clear gradients\n",
    "loss.backward()                    # 4. Compute gradients\n",
    "optimizer.step()                   # 5. Update weights\n",
    "\n",
    "# ‚ùå WRONG - Missing zero_grad()\n",
    "loss.backward()  \n",
    "optimizer.step()  # Gradients accumulate - model won't learn properly!\n",
    "```\n",
    "\n",
    "### **Common Beginner Mistakes:**\n",
    "\n",
    "1. **Forgetting `optimizer.zero_grad()`**\n",
    "   - Gradients accumulate ‚Üí wrong updates\n",
    "   \n",
    "2. **Wrong tensor shapes**\n",
    "   - Check shapes often: `print(tensor.shape)`\n",
    "   \n",
    "3. **Not setting model to eval mode**\n",
    "   - Use `model.eval()` before testing\n",
    "   \n",
    "4. **Using `loss.backward()` on non-scalar tensors**\n",
    "   - Loss must be a single number\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "**What's coming next:**\n",
    "- **Week 10:** Convolutional Neural Networks (CNNs) for image processing\n",
    "- **Week 11:** Transfer learning with pre-trained models\n",
    "- **Week 12:** Recurrent Neural Networks (RNNs) for sequences\n",
    "\n",
    "**To practice before next class:**\n",
    "1. Complete the challenge exercise above\n",
    "2. Try different network architectures\n",
    "3. Experiment with learning rates\n",
    "4. Read PyTorch tutorials: https://pytorch.org/tutorials/\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips:\n",
    "\n",
    "1. **Always check shapes:** `print(tensor.shape)` is your best friend\n",
    "2. **Start simple:** Get a small network working before making it complex\n",
    "3. **Monitor loss:** It should decrease consistently during training\n",
    "4. **Use GPU when possible:** Massive speedup for large models\n",
    "5. **Read documentation:** PyTorch docs are excellent!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Additional Resources:\n",
    "\n",
    "- **PyTorch Official Tutorials:** https://pytorch.org/tutorials/\n",
    "- **PyTorch Documentation:** https://pytorch.org/docs/\n",
    "- **PyTorch Forums:** https://discuss.pytorch.org/\n",
    "- **Deep Learning with PyTorch (Free Book):** https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf\n",
    "\n",
    "---\n",
    "\n",
    "# üéâ Congratulations!\n",
    "\n",
    "You now understand the fundamentals of PyTorch:\n",
    "- ‚úÖ How to create and manipulate tensors\n",
    "- ‚úÖ How to build neural networks\n",
    "- ‚úÖ How to train models using the training loop\n",
    "\n",
    "**You're ready to build real deep learning applications!** üöÄ\n",
    "\n",
    "Keep practicing, experiment with different architectures, and don't be afraid to make mistakes - that's how we learn!\n",
    "\n",
    "---\n",
    "\n",
    "*\"The only way to learn PyTorch is to code in PyTorch.\"*\n",
    "\n",
    "**Happy coding! üî•**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
